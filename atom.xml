<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rocky1ee的博客</title>
  <icon>https://www.gravatar.com/avatar/cdf2628d43f941c34796949e0857e3a5</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="//Rocky1ee.github.io/"/>
  <updated>2019-12-29T12:39:16.790Z</updated>
  <id>//Rocky1ee.github.io/</id>
  
  <author>
    <name>Rocky1ee</name>
    <email>666@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>差分进化算法</title>
    <link href="//Rocky1ee.github.io/2019/09/18/%E5%B7%AE%E5%88%86%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95DE/"/>
    <id>//Rocky1ee.github.io/2019/09/18/差分进化算法DE/</id>
    <published>2019-09-18T13:56:36.000Z</published>
    <updated>2019-12-29T12:39:16.790Z</updated>
    
    <content type="html"><![CDATA[<h1 id="references">References</h1><p>python DE：https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/</p><p>http://www.omegaxyz.com/2018/04/24/differential_evolution_intro/</p><p>https://vlight.me/2018/04/17/differential-evolution/</p><p>https://blog.csdn.net/hehainan_86/article/details/38685231</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;python DE：https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/&lt;
      
    
    </summary>
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Attention GAN Raindrop Removal</title>
    <link href="//Rocky1ee.github.io/2019/09/12/Attention%20GAN%20for%20raindrop%20removal/"/>
    <id>//Rocky1ee.github.io/2019/09/12/Attention GAN for raindrop removal/</id>
    <published>2019-09-12T13:56:36.000Z</published>
    <updated>2019-12-29T12:25:00.148Z</updated>
    
    <content type="html"><![CDATA[<p>Attentive Generative Adversarial Network for Raindrop Removal from A Single Image</p><h1 id="abstract">Abstract</h1><p>问题：1.雨点覆盖区域未给出。2.被覆盖区域下的真实信息缺失。</p><p>解决方法：在G和D中加入visual attention模块，用于学习雨滴区域及其背景。这样的话G可以更多关注于学习到的区域及其背景结构，D可以以该区域为判别真伪的重要依据。</p><h1 id="introduction">Introduction</h1><p>G由attentive-recurrent-network 和 auto-encoder构成：</p><div class="figure"><img src="/Assets/Img/attention%20gan%20rain%20drop.png" alt="the structure of AGAN"><p class="caption">the structure of AGAN</p></div><p>attentive-recurrent network：由ResNet构造的RNN，LSTM和Conv组成，用于生成attention mapping。</p><p>contextual auto-encoder：在decoder端使用了multi-scale loss，每个loss去评估相应尺寸的feature map和ground truth pairs，用此模块获取更多纹理信息。auto-encoder用perceptual loss评估最后结果与ground truth的整体相似度。</p><a id="more"></a><h1 id="raindrop-image-formation">Raindrop Image Formation</h1><p><span class="math display">\[I=(1-M)\bigodot B+R\]</span></p><p><span class="math inline">\(I\)</span>:输入的有雨点图，<span class="math inline">\(B\)</span>:无雨点图，<span class="math inline">\(R\)</span>:被雨点影响区域 <span class="math display">\[M(x)=\begin{cases}1 \qquad if \quad 被雨点覆盖\\0 \qquad if \quad 未雨点覆盖\end{cases}\]</span></p><h1 id="raindrop-removal-using-attentive-gan">Raindrop Removal using Attentive GAN</h1><p><span class="math display">\[\min_G\max_D \Epsilon_\R \sim_{P_{clean}}[log(D(\R))]+\Epsilon_{\Iota\sim{P_{raindrop}}}[log(1-D(G(\Iota)))]\]</span></p><p><span class="math inline">\(\Iota\)</span>:输入雨图，<span class="math inline">\(\R\)</span>：输入的真实无雨图</p><h2 id="generative-network">Generative Network</h2><h3 id="attentive-recurrent-network">Attentive-Recurrent Network</h3><p>用于寻找输入图像中需要关注的区域（雨点区域及其周边信息），结构如下：</p><div class="figure"><img src="/Assets/Img/agan2.png" alt="construction of ARN"><p class="caption">construction of ARN</p></div><p>每个block（time step）由5个ResNet（提取特征），LSTM和Conv（生成attention mapping）组成。</p><p>attention mapping是一个matrix，<span class="math inline">\(x in matrix,x \ in [0,1]\)</span>, 数值越大代表关注越强，从无雨点区域到有雨点区域取值不断变大。</p><p>LSTM部分？？？</p><p>产生attention mapping所采用的loss： <span class="math display">\[\mathcal{L}_{ATT}(\{A\}，M)=\sum_{t=1}^N\theta^{N-t}\mathcal{L}_{MSE}(A_t,M)\]</span> <span class="math inline">\(A_t\)</span>:attention map,其成分如下： <span class="math display">\[A_t=ATT_t(F_{t-1},H_{t-1},C_{t-1}),\\F_{t-1}:输入图像和上个time step产生attention map的组合.\\H_{t-1}:LSTM 上个time step产生的feature.\\C_{t-1}:Conv 上个time step产生的feature.\]</span> M:binary mask 由<span class="math inline">\(I-B\)</span>,然后进行Threshold得到。N:timestep总和，<span class="math inline">\(\theta\)</span>：超参数。</p><h3 id="contextual-autoencoder">Contextual Autoencoder</h3><p><img src="/Assets/Img/agan3.png" style="zoom:70%;"></p><p>Contextual Autoencoder 中包含multi-scale loss 和 perceptual loss。其中multi-scale loss是为了获取不同尺度的contextual information，其定义如下： <span class="math display">\[\mathcal{L}(\{S\},\{T\})=\sum_{i=1}^{M}\lambda_i\mathcal{L}_{MSE}(S_i,T_i)\]</span> <span class="math inline">\(S_i\)</span>：不同尺度的decoder端获取的feature map，<span class="math inline">\(T_i\)</span>是不同尺度feature map对应的ground truth，<span class="math inline">\(\lambda_i\)</span>为各尺度MSE权重。</p><p>perceptual loss 是为了获取autoencoder 的结果和相应的ground truth的全局差异。 <span class="math display">\[\mathcal{L}_P(O,T)=\mathcal{L}_{MSE}(VGG(O),VGG(T))\]</span> <span class="math inline">\(O=G(I)\)</span>：autoencoder的输出，T：ground truth</p><p>整个generative的loss为： <span class="math display">\[\mathcal{L}_G=10^{-2}\mathcal{L}_{GAN}(O)+\mathcal{L}_{ATT}({A},M)+\mathcal{L}_M(\{S\},\{T\}+\mathcal{L}_p(O,T))\\\mathcal{L}_{GAN}(O)=log(1-D(O))\]</span></p><h2 id="discriminative-network">Discriminative Network</h2><p><img src="/Assets/Img/agan4.png" alt="structure of discriminator" style="zoom:80%;"></p><p>特点是将D前面层的特征图a输入到CNN中得到结果b，将a和b相乘输入到下一层。基于b和attentive-recurrent network产生的attention map的loss定义如下： <span class="math display">\[\mathcal{L}_D(O,R,A_N)=-log(D(R))-log(1-D(O))+\gamma\mathcal{L}_{map}(O,R,A_N)\]</span> <span class="math inline">\(O\)</span>:G的输出的raindrop removal image，<span class="math inline">\(R\)</span>：ground truth image，<span class="math inline">\(A_N\)</span>:attentive-recurrent network 产生的attention map. <span class="math display">\[\mathcal{L}_{map}(O,R,A_N)=\mathcal{L}_{MSE}(D_{map}(O),A_N)+\mathcal{L}_{MSE}(D_{map}(R),0)\]</span> <span class="math inline">\(D_{map}()\)</span>:D前层提取的特征图，公式的意思是对于输入<span class="math inline">\(O\)</span>我们更关注attention 区域的 loss，对于<span class="math inline">\(R\)</span>我们没有需要特别关注的区域。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Attentive Generative Adversarial Network for Raindrop Removal from A Single Image&lt;/p&gt;
&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;问题：1.雨点覆盖区域未给出。2.被覆盖区域下的真实信息缺失。&lt;/p&gt;
&lt;p&gt;解决方法：在G和D中加入visual attention模块，用于学习雨滴区域及其背景。这样的话G可以更多关注于学习到的区域及其背景结构，D可以以该区域为判别真伪的重要依据。&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;G由attentive-recurrent-network 和 auto-encoder构成：&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/Img/attention%20gan%20rain%20drop.png&quot; alt=&quot;the structure of AGAN&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;the structure of AGAN&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;attentive-recurrent network：由ResNet构造的RNN，LSTM和Conv组成，用于生成attention mapping。&lt;/p&gt;
&lt;p&gt;contextual auto-encoder：在decoder端使用了multi-scale loss，每个loss去评估相应尺寸的feature map和ground truth pairs，用此模块获取更多纹理信息。auto-encoder用perceptual loss评估最后结果与ground truth的整体相似度。&lt;/p&gt;
    
    </summary>
    
      <category term="去雨方法" scheme="//Rocky1ee.github.io/categories/%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="Raindrop Removal" scheme="//Rocky1ee.github.io/tags/Raindrop-Removal/"/>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>蒙特卡罗算法</title>
    <link href="//Rocky1ee.github.io/2019/09/12/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E7%AE%97%E6%B3%95/"/>
    <id>//Rocky1ee.github.io/2019/09/12/蒙特卡罗算法/</id>
    <published>2019-09-12T13:56:36.000Z</published>
    <updated>2019-12-29T12:38:16.304Z</updated>
    
    <content type="html"><![CDATA[<p>蒙特卡罗算法：采样越多，越<strong>近似</strong>最优解；用有限随机数去计算估计值.</p><p>举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——<strong>尽量找好的，但不保证是最好的</strong>。</p><div class="figure"><img src="/Assets/BlogImg/圆周率估计.gif" alt="Monte Carlo Method"><p class="caption">Monte Carlo Method</p></div><p>使用蒙特卡洛方法估算π值. 放置30000个随机点后,π的估算值与真实值相差0.07%.</p><p>这张图其实很简单, 就像我们玩飞镖一样, 随机地在一个方形平面上投掷30000个飞镖, 事先我们并不知道圆周率π的值究竟是多少, 但是我们知道这里有1/4的圆, 于是我们把红色面积上的点数<span class="math inline">\(m\)</span>, 和蓝色面积上的点数<span class="math inline">\(n\)</span>, 以及圆周率<span class="math inline">\(π\)</span>的关系, 可以写出一个约等于的式子: <span class="math display">\[π ≈ 4m/(n+m)\]</span> 随着<span class="math inline">\(m+n\)</span>的投射点的逐渐增加, <span class="math inline">\(π\)</span>值的计算也越来越精确, 最后我们就估计出一个不错的比较精确的<span class="math inline">\(π\)</span>值啦</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;蒙特卡罗算法：采样越多，越&lt;strong&gt;近似&lt;/strong&gt;最优解；用有限随机数去计算估计值.&lt;/p&gt;
&lt;p&gt;举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少
      
    
    </summary>
    
      <category term="算法" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>RNN，LSTM</title>
    <link href="//Rocky1ee.github.io/2019/09/10/RNN/"/>
    <id>//Rocky1ee.github.io/2019/09/10/RNN/</id>
    <published>2019-09-10T11:56:36.000Z</published>
    <updated>2019-12-29T12:25:31.760Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rnn">RNN</h1><p>Recurrent Neural Networks:将神经网络中前层获得的信息传递到后层</p><div class="figure"><img src="/Assets/BlogImg/RNN.png" alt="RNN structure"><p class="caption">RNN structure</p></div><div class="figure"><img src="/Assets/BlogImg/standard_RNN.png" alt="standard RNN"><p class="caption">standard RNN</p></div><p>RNN的问题：随着层数的增长对于有用信息<span class="math inline">\(x_0,x_1\)</span>的记忆越弱</p><div class="figure"><img src="/Assets/BlogImg/RNN_problem.png" alt="drawback of RNN"><p class="caption">drawback of RNN</p></div><h1 id="lstm">LSTM</h1><h2 id="介绍">介绍</h2><p>Long Short Term Memory networks :可以记忆久远的的有用信息</p><p><img src="/Assets/BlogImg/structure_of_LSTM.png" alt="structure of LSTM"> <span class="math display">\[\begin{aligned}f_t&amp;=\sigma(W_f[h_{t-1},x_t]+b_f)\\i_t&amp;=\sigma(W_i[h_{t-1},x_t]+b_i)\\\widetilde{C_{t}}&amp;=tanh(W_c[h_{t-1},x_t]+b_c)\\C_t&amp;=f_t*C_{t-1}+i_t*\widetilde{C}\\o_t&amp;=\sigma(W_o[h_{t-1},x_t]+b_o)\\h_t&amp;=o_t*tanh(C_t)\end{aligned}\]</span> 主线信息贯穿整个网络</p><div class="figure"><img src="/Assets/BlogImg/main_line.png" alt="cell state"><p class="caption">cell state</p></div><a id="more"></a><h2 id="逐步分析">逐步分析</h2><p>forget gate layer：决定<span class="math inline">\(C_{t-1}\)</span>中的那些信息保留下来，那些信息丢掉。对输入<span class="math inline">\(x_t\)</span>和<span class="math inline">\(h_{t-1}\)</span>的组合进行sigmoid mapping为<span class="math inline">\(C_{t-1}\)</span>中的每个元素产生对应的output，output<span class="math inline">\(\in [0,1]\)</span>，0指丢弃当前信息，1指保留当前信息。</p><div class="figure"><img src="/Assets/BlogImg/forget_gate.png" alt="forget_gate"><p class="caption">forget_gate</p></div><p>input gate layer: 决定将<span class="math inline">\(\widetilde{C_t}\)</span>中的哪些信息加入到主线信息。</p><div class="figure"><img src="/Assets/BlogImg/input_gate.png" alt="input gate"><p class="caption">input gate</p></div><p>主线信息的更新：用<span class="math inline">\(f_t\)</span>决定主线中哪些信息保留，哪些信息丢弃。用<span class="math inline">\(i_t\)</span>决定添加哪些新信息到主线。</p><div class="figure"><img src="/Assets/BlogImg/update_cell_state.png" alt="update cell state"><p class="caption">update cell state</p></div><p>output gate layer：决定<span class="math inline">\(C_t\)</span>主线中输出的信息。</p><div class="figure"><img src="/Assets/BlogImg/output_gate_layer.png" alt="output gate layer"><p class="caption">output gate layer</p></div><h2 id="变体">变体</h2><p>将主线信息加入到门判断中</p><div class="figure"><img src="/Assets/BlogImg/cell_state_add_to_%20all_gate.png" alt="adding cell state to all gate"><p class="caption">adding cell state to all gate</p></div><p>用coupled forget 即一个门<span class="math inline">\(f_t\)</span>,<span class="math inline">\(1-f_t\)</span>判断主线信息的更新</p><div class="figure"><img src="/Assets/BlogImg/coupled_forget.png" alt="coupled forget"><p class="caption">coupled forget</p></div><p>Gated Recurrent Unit (GRU)</p><div class="figure"><img src="/Assets/BlogImg/GRU.png" alt="structure of GRU"><p class="caption">structure of GRU</p></div>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;rnn&quot;&gt;RNN&lt;/h1&gt;
&lt;p&gt;Recurrent Neural Networks:将神经网络中前层获得的信息传递到后层&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/RNN.png&quot; alt=&quot;RNN structure&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;RNN structure&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/standard_RNN.png&quot; alt=&quot;standard RNN&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;standard RNN&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;RNN的问题：随着层数的增长对于有用信息&lt;span class=&quot;math inline&quot;&gt;\(x_0,x_1\)&lt;/span&gt;的记忆越弱&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/RNN_problem.png&quot; alt=&quot;drawback of RNN&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;drawback of RNN&lt;/p&gt;
&lt;/div&gt;
&lt;h1 id=&quot;lstm&quot;&gt;LSTM&lt;/h1&gt;
&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;Long Short Term Memory networks :可以记忆久远的的有用信息&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/Assets/BlogImg/structure_of_LSTM.png&quot; alt=&quot;structure of LSTM&quot;&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{aligned}
f_t&amp;amp;=\sigma(W_f[h_{t-1},x_t]+b_f)\\
i_t&amp;amp;=\sigma(W_i[h_{t-1},x_t]+b_i)\\
\widetilde{C_{t}}&amp;amp;=tanh(W_c[h_{t-1},x_t]+b_c)\\
C_t&amp;amp;=f_t*C_{t-1}+i_t*\widetilde{C}\\
o_t&amp;amp;=\sigma(W_o[h_{t-1},x_t]+b_o)\\
h_t&amp;amp;=o_t*tanh(C_t)
\end{aligned}
\]&lt;/span&gt; 主线信息贯穿整个网络&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/main_line.png&quot; alt=&quot;cell state&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;cell state&lt;/p&gt;
&lt;/div&gt;
    
    </summary>
    
      <category term="神经网络算法学习" scheme="//Rocky1ee.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="//Rocky1ee.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="//Rocky1ee.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>反卷积(Transposed Conv)和空洞卷积(Dilated conv)</title>
    <link href="//Rocky1ee.github.io/2019/09/10/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E5%92%8C%E8%86%A8%E8%83%80%E5%8D%B7%E7%A7%AF/"/>
    <id>//Rocky1ee.github.io/2019/09/10/转置卷积和膨胀卷积/</id>
    <published>2019-09-10T08:56:36.000Z</published>
    <updated>2019-12-16T01:25:07.155Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积">卷积</h1>假设输入图像<span class="math inline">\(input,size = 4 x 4\)</span>,表示如下： $$<span class="math display">\[\begin{equation}input = \begin{bmatrix} x_1&amp; x_2 &amp; x_3 &amp;x_4 \\  x_5&amp; x_6 &amp; x_7 &amp;x_8 \\  x_9&amp; x_{10} &amp; x_{11} &amp; x_{12}\\  x_{13}&amp; x_{14} &amp; x_{15} &amp; x_{16}\end{bmatrix}\end{equation}\]</span><span class="math display">\[$kernel, size =3 x 3$,表示如下：\]</span><span class="math display">\[\begin{bmatrix} w_{0,0}&amp; w_{0,1}  &amp; w_{0,2} \\  w_{1,0}&amp; w_{1,1}  &amp; w_{1,2} \\  w_{2,0}&amp; w_{2,1}  &amp; w_{2,2}\end{bmatrix}\]</span><p><span class="math inline">\($ 按照卷积公式\)</span>o=+1<span class="math inline">\(得出输出特征图\)</span>output size = 2 x 2$</p><p>卷积过程用矩阵乘法描述为<span class="math inline">\(Y=CX\)</span> <span class="math display">\[X = [x_1,x_2,x_3,x_4, x_5,x_6,x_7,x_8,  x_9,x_{10},x_{11},x_{12}, x_{13},x_{14},x_{15},x_{16}]\]</span></p><p><span class="math display">\[C = \begin{bmatrix} w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}  &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0 \\ 0 &amp;w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}  &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\ 0 &amp;0 &amp;0 &amp;0 &amp;w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}  &amp; 0   \\  0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}   \end{bmatrix}\]</span></p><div class="figure"><img src="/Assets/BlogImg/conv.gif" alt="convelution"><p class="caption">convelution</p></div><a id="more"></a><h1 id="反卷积">反卷积</h1><p>反卷积的操作就是要对这个矩阵运算过程进行逆运算，即通过 <span class="math inline">\(C\)</span>和<span class="math inline">\(Y\)</span>得到<span class="math inline">\(X\)</span>,反卷积操作可以表示为：</p><p>$X=C^TY&amp; <span class="math display">\[Y=\begin{bmatrix}y_1\\ y_2\\ y_3\\ y_4\end{bmatrix}\]</span></p><p><span class="math display">\[C^T=\begin{bmatrix} w_{0,0}&amp;0  &amp; 0 &amp; 0\\  w_{0,1}&amp;w_{0,0}  &amp;0  &amp; 0\\  w_{0,2}&amp;w_{0,1}  &amp; 0 &amp; 0\\  0&amp; w_{0,2} &amp; 0 &amp; 0\\  w_{1,0}&amp; 0 &amp;w_{0,0}  &amp; 0\\  w_{1,1}&amp; w_{1,0} &amp;w_{0,1}  &amp; w_{0,0}\\  w_{1,2}&amp;w_{1,1}  &amp; w_{0,2} &amp;w_{0,0} \\  0&amp;w_{1,2}  &amp;0  &amp;w_{2,1} \\  w_{2,0}&amp; 0 &amp; w_{1,0} &amp; 0\\  w_{2,1}&amp; w_{2,0} &amp;w_{1,1}  &amp;w_{1,0} \\  w_{2,2}&amp; w_{2,1} &amp; w_{1,2} &amp;w_{1,1} \\ 0&amp; w_{2,2} &amp; 0 &amp; w_{1,2}\\  0&amp; 0 &amp; w_{2,0} &amp; 0\\  0&amp;0  &amp;w_{2,1}  &amp; w_{2,0}\\  0&amp;0  &amp;w_{2,2}  &amp;w_{2,1} \\   0&amp; 0 &amp;0  &amp; w_{2,2}\end{bmatrix}\]</span></p>反卷积公式为： $$ o=<span class="math display">\[\begin{cases}s(i-1)-2p+k,\quad &amp;if \quad(o+2p-k)\%s=0 \\s(i-1)-2p+k+(o+2p-k)\%s,\quad &amp;if \quad (o+2p-k)\%s\neq0\end{cases}\]</span><p>$$ <img src="/Assets/BlogImg/Tconv.gif" alt="Transform Convelution"></p><h1 id="空洞卷积">空洞卷积</h1><div class="figure"><img src="/Assets/BlogImg/dilated.gif" alt="dilated conv"><p class="caption">dilated conv</p></div><div class="figure"><img src="/Assets/BlogImg/dilated%20conv.jpg" alt="论文实例"><p class="caption">论文实例</p></div><p>(a):3x3,1-dilated conv,其效果和普通卷积一样。</p><p>(b):3x3,2dilated conv,其效果为非红点pixel取0的7x7普通卷积。</p><p>(c):3x3,4dilated conv,其效果为非红点pixel取0的15x15普通卷积</p><p>优点：在不做pooling损失信息和相同的计算条件下的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。空洞卷积经常用在实时图像分割中。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑空洞卷积。</p><h1 id="references">references</h1><p>https://www.cnblogs.com/hellcat/p/9687624.html</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;卷积&quot;&gt;卷积&lt;/h1&gt;
假设输入图像&lt;span class=&quot;math inline&quot;&gt;\(input,size = 4 x 4\)&lt;/span&gt;,表示如下： $$
&lt;span class=&quot;math display&quot;&gt;\[\begin{equation}
input = 
\begin{bmatrix}
 x_1&amp;amp; x_2 &amp;amp; x_3 &amp;amp;x_4 \\ 
 x_5&amp;amp; x_6 &amp;amp; x_7 &amp;amp;x_8 \\ 
 x_9&amp;amp; x_{10} &amp;amp; x_{11} &amp;amp; x_{12}\\ 
 x_{13}&amp;amp; x_{14} &amp;amp; x_{15} &amp;amp; x_{16}
\end{bmatrix}

\end{equation}\]&lt;/span&gt;
&lt;span class=&quot;math display&quot;&gt;\[
$kernel, size =3 x 3$,表示如下：
\]&lt;/span&gt;
&lt;span class=&quot;math display&quot;&gt;\[\begin{bmatrix}
 w_{0,0}&amp;amp; w_{0,1}  &amp;amp; w_{0,2} \\ 
 w_{1,0}&amp;amp; w_{1,1}  &amp;amp; w_{1,2} \\ 
 w_{2,0}&amp;amp; w_{2,1}  &amp;amp; w_{2,2}
\end{bmatrix}\]&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\($ 按照卷积公式\)&lt;/span&gt;o=+1&lt;span class=&quot;math inline&quot;&gt;\(得出输出特征图\)&lt;/span&gt;output size = 2 x 2$&lt;/p&gt;
&lt;p&gt;卷积过程用矩阵乘法描述为&lt;span class=&quot;math inline&quot;&gt;\(Y=CX\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
X = [x_1,x_2,x_3,x_4,
 x_5,x_6,x_7,x_8, 
 x_9,x_{10},x_{11},x_{12},
 x_{13},x_{14},x_{15},x_{16}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
C = \begin{bmatrix}
 w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp;0 \\ 
0 &amp;amp;w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  \\ 
0 &amp;amp;0 &amp;amp;0 &amp;amp;0 &amp;amp;w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}  &amp;amp; 0   \\ 
 0 &amp;amp;0 &amp;amp;0 &amp;amp;0 &amp;amp;0 &amp;amp;w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}   
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/conv.gif&quot; alt=&quot;convelution&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;convelution&lt;/p&gt;
&lt;/div&gt;
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="upsampling，卷积" scheme="//Rocky1ee.github.io/tags/upsampling%EF%BC%8C%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>模拟退火算法</title>
    <link href="//Rocky1ee.github.io/2019/09/06/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95SA/"/>
    <id>//Rocky1ee.github.io/2019/09/06/模拟退火算法SA/</id>
    <published>2019-09-06T11:56:36.000Z</published>
    <updated>2019-12-10T07:43:48.033Z</updated>
    
    <content type="html"><![CDATA[<p>模拟退火算法(Simulated Annealing)：采用类似于物理退火的过程，先在一个高温状态下（相当于算法随机搜索），然后逐渐退火，在每个温度下（相当于算法的每一次状态转移）徐徐冷却（相当于算法局部搜索），最终达到物理基态（相当于算法找到最优解）。</p><h1 id="算法思想">算法思想</h1><h2 id="爬山算法">爬山算法</h2><p><strong>爬山算法</strong>是一种简单的<strong>贪心搜索算法</strong>，它每次都<strong>鼠目寸光</strong>地从当前解临近的解空间中选择一个更优解代替当前解，直到得到一个局部最优解。</p><p>如图1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解后就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。因此，爬山算法只能搜索到局部最优值，而不一定能搜索到全局最优解。</p><div class="figure"><img src="/Assets/BlogImg/SA.jpg" alt="图 1"><p class="caption">图 1</p></div><h2 id="模拟退火算法">模拟退火算法</h2><p>模拟退火算法其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法<strong>以一定的概率</strong>来接受一个比当前解要差的解，因此<strong>有可能</strong>会跳出这个局部的最优解，达到全局的最优解。</p><p>以图1为例，模拟退火算法在搜索到局部最优解A后，会<strong>以一定的概率</strong>向E的移动。可能经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。模拟退火算法是一种随机算法，并不一定能找到全局的最优解，但可以比较快的找到问题的近似最优解。</p><a id="more"></a><h1 id="算法流程">算法流程</h1><p><img src="/Assets/BlogImg/SA2.png" alt="The process of SA" style="zoom:80%;"></p><h2 id="初始化">初始化</h2><p>模拟退火时需设定三个参数：一个比较大初始温度<span class="math inline">\(T_0\)</span> ，降温系数<span class="math inline">\(d\)</span> 非常接近但小于<span class="math inline">\(1\)</span>，终止温度 <span class="math inline">\(T_k\)</span>是一个接近<span class="math inline">\(0\)</span>的正数。</p><h2 id="metropolis准则">Metropolis准则</h2>解的替换按照Metropolis准则进行，假设前一状态为 <span class="math inline">\(f(x_n)\)</span>，系统受到一定扰动，状态变为 <span class="math inline">\(f(x_{n+1})\)</span>，相应地，系统能量由 <span class="math inline">\(E(x_n)\)</span> 变为 <span class="math inline">\(E(x_{n+1})\)</span>。 定义系统由 <span class="math inline">\(f(x_n)\)</span> 变为$f(x_{n+1}) $的接收概率为 <span class="math inline">\(p\)</span>（probability of acceptance）： $$<span class="math display">\[\begin{equation}p=\begin{cases}1, \quad &amp;if \quad E(x_{n+1})&lt;E(x_n)\\e^{-\frac{E(x_{n+1})-E(x_n)}{T}} \quad &amp;if \quad E(x_{n+1})\geq E(x_n)\end{cases}\end{equation}\]</span><p>$$</p><p>通俗解释为：</p><ol style="list-style-type: decimal"><li>在高温下可接受与当前状态能量差较大的新状态；</li><li>在低温下基本只接受与当前能量差较小的新状态；</li><li>当温度趋于零时，就不能接受比当前状态能量高的新状态。</li></ol><p>随着温度的降低，对随机性的接受度降低。</p><h2 id="迭代更新">迭代更新</h2><p>首先让温度 <span class="math inline">\(T=T_0\)</span>，然后按照上述Metroplis准则对解进行更新，再让<span class="math inline">\(T=d \times T\)</span>。当<span class="math inline">\(T&lt;T_k\)</span>时模拟退火过程结束，当前最优解即为最终的最优解。</p><p>直观表示如下图：随着温度的降低，跳跃越来越不随机，最优解也越来越稳定。</p><div class="figure"><img src="/Assets/BlogImg/SA3.gif" alt="退火算法求解过程"><p class="caption">退火算法求解过程</p></div><h1 id="算法实例">算法实例</h1><h1 id="references">references</h1><p>OI Wiki 习题https://oi-wiki.org/misc/simulated-annealing/</p><p>科学网 http://blog.sciencenet.cn/blog-1813407-893984.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;模拟退火算法(Simulated Annealing)：采用类似于物理退火的过程，先在一个高温状态下（相当于算法随机搜索），然后逐渐退火，在每个温度下（相当于算法的每一次状态转移）徐徐冷却（相当于算法局部搜索），最终达到物理基态（相当于算法找到最优解）。&lt;/p&gt;
&lt;h1 id=&quot;算法思想&quot;&gt;算法思想&lt;/h1&gt;
&lt;h2 id=&quot;爬山算法&quot;&gt;爬山算法&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;爬山算法&lt;/strong&gt;是一种简单的&lt;strong&gt;贪心搜索算法&lt;/strong&gt;，它每次都&lt;strong&gt;鼠目寸光&lt;/strong&gt;地从当前解临近的解空间中选择一个更优解代替当前解，直到得到一个局部最优解。&lt;/p&gt;
&lt;p&gt;如图1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解后就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。因此，爬山算法只能搜索到局部最优值，而不一定能搜索到全局最优解。&lt;/p&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/SA.jpg&quot; alt=&quot;图 1&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;图 1&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&quot;模拟退火算法&quot;&gt;模拟退火算法&lt;/h2&gt;
&lt;p&gt;模拟退火算法其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法&lt;strong&gt;以一定的概率&lt;/strong&gt;来接受一个比当前解要差的解，因此&lt;strong&gt;有可能&lt;/strong&gt;会跳出这个局部的最优解，达到全局的最优解。&lt;/p&gt;
&lt;p&gt;以图1为例，模拟退火算法在搜索到局部最优解A后，会&lt;strong&gt;以一定的概率&lt;/strong&gt;向E的移动。可能经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。模拟退火算法是一种随机算法，并不一定能找到全局的最优解，但可以比较快的找到问题的近似最优解。&lt;/p&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="模拟退火算法" scheme="//Rocky1ee.github.io/tags/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>遗传算法</title>
    <link href="//Rocky1ee.github.io/2019/09/04/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95GA/"/>
    <id>//Rocky1ee.github.io/2019/09/04/遗传算法GA/</id>
    <published>2019-09-04T09:56:36.000Z</published>
    <updated>2019-12-10T07:45:30.478Z</updated>
    
    <content type="html"><![CDATA[<p>遗传算法（Genetic Algorithm）</p><h1 id="算法背景">算法背景</h1><p><strong>种群(Population)</strong>：生物的进化以群体的形式进行，这样的一个群体称为种群。</p><p><strong>个体</strong>：组成种群的单个生物。</p><p><strong>染色体 ( Chromosome )</strong> ：包含一组的基因。</p><p><strong>基因 ( Gene )</strong> ：一个遗传因子。</p><p><strong>生存竞争，适者生存</strong>：对环境适应度高的、优良的个体参与繁殖的机会比较多，后代就会越来越多。适应度低的个体参与繁殖的机会比较少，后代就会越来越少。</p><p><strong>选择</strong>：根据相应策略选择父母染色体对进行繁殖。</p><p><strong>交叉</strong>：将父母双方各一部分的基因进行组合形成子代染色体。</p><p><strong>变异</strong>：子代染色体有一定的概率发生基因变异。</p><p>简单说来就是：选择优良父代进行繁殖，期间会发生基因交叉( Crossover ) ，基因突变 ( Mutation ) ，适应度( Fitness )低的个体会被逐步淘汰，而适应度高的个体会越来越多。那么经过N代的自然选择后，保存下来的个体都是适应度很高的，其中很可能包含史上产生的适应度最高的个体。</p><h1 id="算法流程">算法流程</h1><h2 id="总体流程">总体流程</h2><div class="figure"><img src="/Assets/BlogImg/GA1.jpg" alt="The process of GA"><p class="caption">The process of GA</p></div><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">begin</span><br><span class="line">       initialize P(0);      </span><br><span class="line">       t = 0;             //t是进化的代数，一代、二代、三代...</span><br><span class="line"></span><br><span class="line">       while(t &lt;= T) do</span><br><span class="line">              for i = 1 to M  do     //M是初始种群的个体数</span><br><span class="line">                     Evaluate fitness of P(t);  //计算P（t）中各个个体的适应度</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M  do</span><br><span class="line">                     Select operation to P(t);  //将选择算子作用于群体</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M/2  do</span><br><span class="line">                     Crossover operation to P(t); //将交叉算子作用于群体</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M  do</span><br><span class="line">                     Mutation operation to P(t);  //将变异算子作用于群体</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M  do</span><br><span class="line">                     P(t+1) = P(t);      //得到下一代群体P（t + 1）</span><br><span class="line">              end for</span><br><span class="line">              </span><br><span class="line">              t = t + 1;      //终止条件判断  t≦T：t← t+1 转到步骤2</span><br><span class="line"></span><br><span class="line">       end while</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="编码">编码</h2><p>遗传算法需要将问题的解(个体)编码成字符串的形式。最简单的一种编码方式是二进制编码，</p><p>即<span class="math inline">\(24=11000\)</span>。</p><h2 id="适应度计算">适应度计算</h2><p>适应度函数 ( Fitness Function )：用于评价染色体的适应度，用<span class="math inline">\(f(x)\)</span>表示。<strong>适应度函数</strong>与<strong>目标函数</strong>是正相关的，可对目标函数作一些变形来得到适应度函数。有时需要区分染色体的适应度函数与问题的目标函数。例如：0-1背包问题的目标函数是所取得物品价值，但将物品价值作为染色体的适应度函数可能并不一定适合。</p><h2 id="选择">选择</h2><p>使用选择运算子对个体进行优胜劣汰操作。适应度高的个体被遗传到下一代群体中的概率大；适应度低的个体，被遗传到下一代群体中的概率小。目标就是从父代群体中选取优个体，遗传到下一代群体。</p><p>常用算法为轮盘赌法： <span class="math display">\[P(x_i)=\frac{f(x_i)}{\sum\limits_{j=1}^Nf(x_j)}\qquad(1)\]</span> <span class="math inline">\(x_i\)</span>:个体<span class="math inline">\(i，i \in[1,N]\)</span>,<span class="math inline">\(f()\)</span>:适应度函数,<span class="math inline">\(P(x_i)\)</span>:选择个体<span class="math inline">\(i\)</span>的概率。 <span class="math display">\[q_i = \sum\limits_{j=1}^iP(x_j)\qquad (2)\]</span> <span class="math inline">\(q_i\)</span>为累积概率。</p><div class="figure"><img src="/Assets/BlogImg/轮盘赌选择法.jpeg" alt="轮盘赌法"><p class="caption">轮盘赌法</p></div><p>生成随机数<span class="math inline">\(r_k \in[0,1],k =1,2,3...N\)</span>,<span class="math inline">\(N\)</span>为选择个体总数。假设<span class="math inline">\(r_1=0.4\)</span>,则选择<span class="math inline">\(个体3\)</span>,即选择<span class="math inline">\(q_i \geq r_1\)</span>的第首个个体。</p><h2 id="交叉">交叉</h2><p>染色体交叉是以一定的概率发生的，这个概率记为$P_c $。交叉的方法有很多种，这里用的是单点交叉</p><p>交叉前：</p><p>00000|<strong>011100000000</strong>|10000</p><p>11100|<strong>000001111110</strong>|00101</p><p>交叉后：</p><p>00000|<strong>000001111110</strong>|10000</p><p>11100|<strong>011100000000</strong>|00101</p><h2 id="变异">变异</h2><p>在繁殖过程，新产生的染色体中的基因会以一定的概率出错，称为变异。变异发生的概率记为&amp;P_m&amp;</p><p>变异前：</p><p>000001110000<strong>0</strong>00010000</p><p>变异后：</p><p>000001110000<strong>1</strong>00010000</p><h1 id="算法实例">算法实例</h1><p>已知<span class="math inline">\(x\)</span>为整数，利用遗传算法求函数<span class="math inline">\(y=x^2\)</span>区间<span class="math inline">\([0, 31]\)</span>上的的最大值。</p><h2 id="初始化">初始化</h2><p>将种群规模设定为<span class="math inline">\(M=4\)</span>；用5位二进制数编码染色体；取下列个体组成初始种群<span class="math inline">\(S_1\)</span> <span class="math display">\[s_1= 13 (01101),  s_2= 24 (11000)\\s_3= 8 (01000),    s_4= 19 (10011）\]</span></p><h2 id="适应度计算-1">适应度计算</h2><p>适应度函数定义为<span class="math inline">\(f(x)=x^2\)</span>,由此计算各染色体的适应度： <span class="math display">\[\begin{split}&amp;f (s_1) = f(13) = 13^2 = 169\\&amp;f (s_2) = f(24) = 24^2 = 576\\&amp;f (s_3) = f(8) = 8^2 = 64\\&amp;f (s_4) = f(19) = 19^2 = 361\end{split}\]</span> ## 选择</p><p>再由公式(1)计算<span class="math inline">\(S_1\)</span>中各个体的选择概率： <span class="math display">\[\begin{split}&amp;P(s_1) = P(13) = 0.14\\&amp;P(s_2) = P(24) = 0.49\\&amp;P(s_3) = P(8) = 0.06\\&amp;P(s_4) = P(19) = 0.31\end{split}\]</span> 再由公式(2)计算<span class="math inline">\(S_1\)</span>中各个体的累计概率： <span class="math display">\[\begin{split}&amp;q(s_1) = q(13) = 0.14\\&amp;q(s_2) = q(24) = 0.63\\&amp;q(s_3) = q(8) = 0.69\\&amp;q(s_4) = q(19) = 1\end{split}\]</span> 假设从<span class="math inline">\([0,1]\)</span>中产生的<span class="math inline">\(N=4\)</span>个随机数如下： <span class="math display">\[\begin{split}&amp;r1 = 0.450126,\\     &amp;r2 = 0.110347\\&amp;r3 = 0.572496, \\    &amp;r4 = 0.98503\end{split}\]</span> 选择结果为：</p><table><thead><tr class="header"><th>个体</th><th>染色体</th><th>适应度</th><th>选择概率</th><th>累积概率</th><th>选择次数</th></tr></thead><tbody><tr class="odd"><td>13</td><td><span class="math inline">\(s_1=01101\)</span></td><td>169</td><td>0.14</td><td>0.14</td><td>1</td></tr><tr class="even"><td>24</td><td><span class="math inline">\(s_2=11000\)</span></td><td>576</td><td>0.49</td><td>0.63</td><td>2</td></tr><tr class="odd"><td>8</td><td><span class="math inline">\(s_3=01000\)</span></td><td>64</td><td>0.06</td><td>0.69</td><td>0</td></tr><tr class="even"><td>19</td><td><span class="math inline">\(s_4=10011\)</span></td><td>361</td><td>0.31</td><td>1</td><td>1</td></tr></tbody></table><h2 id="交叉-1">交叉</h2><p>设交叉率<span class="math inline">\(p_c\)</span>=100%，即<span class="math inline">\(S_1\)</span>中的全体染色体都参加交叉运算,交叉点位: | 。 设<span class="math inline">\(s_1’与s_2’\)</span>配对，<span class="math inline">\(s_3’与s_4’\)</span>配对。 <span class="math display">\[s_1’ =110|00（24）,  s_2’ =011|01（13）s_3’ =110|00（24）,  s_4’ =100|11（19）\]</span> 分别交换后两位基因，得新染色体： <span class="math display">\[s_1’’=11001（25）,  s_2’’=01100（12）s_3’’=11011（27）,  s_4’’=10000（16）\]</span></p><h2 id="变异-1">变异</h2><p>设变异率<span class="math inline">\(p_m=0.001\)</span>。这样，群体<span class="math inline">\(S1\)</span>中共有<span class="math inline">\(5×4×0.001=0.02\)</span>位基因可以变异。<span class="math inline">\(0.02\)</span>位显然不足1位，所以本轮遗传操作不做变异。</p><p>于是，得到第二代种群S2： <span class="math display">\[s_1=11001（25）,  s_2=01100（12）s_3=11011（27）,  s_4=10000（16）\]</span></p><h2 id="迭代更替">迭代更替</h2><table><thead><tr class="header"><th>个体</th><th>染色体</th><th>适应度</th><th>选择概率</th><th>累积概率</th><th>选择次数</th></tr></thead><tbody><tr class="odd"><td>25</td><td><span class="math inline">\(s_1=11001\)</span></td><td>625</td><td>0.36</td><td>0.36</td><td>1</td></tr><tr class="even"><td>12</td><td><span class="math inline">\(s_2=01100\)</span></td><td>144</td><td>0.07</td><td>0.44</td><td>0</td></tr><tr class="odd"><td>27</td><td><span class="math inline">\(s_3=11011\)</span></td><td>729</td><td>0.41</td><td>0.85</td><td>2</td></tr><tr class="even"><td>256</td><td><span class="math inline">\(s_4=10000\)</span></td><td>256</td><td>0.15</td><td>1.00</td><td>1</td></tr></tbody></table><p>假设这一轮遗传操作中，种群S2中的4个染色体都被选中，则得到群体： <span class="math display">\[ s_1’=11|001（25）,  s_2’=11|011（27）， s_3’=11|011（27）,  s_4’= 10|000（16）\]</span> 做交叉运算，让s1’与s2’，s3’与s4’ 分别交叉，得 <span class="math display">\[s1’’=11011（27）,    s2’’ = 11001（25）,  s3’’ =11000（24）,   s4’’ = 10011（19）\]</span> 这一轮仍然不会发生变异。 于是，得第三代种群S3： <span class="math display">\[s1=11011（27）,    s2 = 11001（25）,  s3 =11000（24）,   s4= 10011（19）\]</span> 迭代到第5代可以得到最优解<span class="math inline">\(s^*=11111(31),f(s^*)=961\)</span>。</p><h2 id="结束">结束</h2><p>按照以上流程不断迭代更替直到迭代次数超出预设值或目标函数的最优值不再变化。</p><h1 id="references">references</h1><p>JULY大神 https://blog.csdn.net/v_JULY_v/article/details/6132775</p><p>Michael翔 https://michael728.github.io/2015/12/24/algorithm-GA-basic/</p><!--more-->]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遗传算法（Genetic Algorithm）&lt;/p&gt;
&lt;h1 id=&quot;算法背景&quot;&gt;算法背景&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;种群(Population)&lt;/strong&gt;：生物的进化以群体的形式进行，这样的一个群体称为种群。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;个体&lt;/strong&gt;：组成种群的单个生物。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;染色体 ( Chromosome )&lt;/strong&gt; ：包含一组的基因。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基因 ( Gene )&lt;/strong&gt; ：一个遗传因子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;生存竞争，适者生存&lt;/strong&gt;：对环境适应度高的、优良的个体参与繁殖的机会比较多，后代就会越来越多。适应度低的个体参与繁殖的机会比较少，后代就会越来越少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;选择&lt;/strong&gt;：根据相应策略选择父母染色体对进行繁殖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;交叉&lt;/strong&gt;：将父母双方各一部分的基因进行组合形成子代染色体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;变异&lt;/strong&gt;：子代染色体有一定的概率发生基因变异。&lt;/p&gt;
&lt;p&gt;简单说来就是：选择优良父代进行繁殖，期间会发生基因交叉( Crossover ) ，基因突变 ( Mutation ) ，适应度( Fitness )低的个体会被逐步淘汰，而适应度高的个体会越来越多。那么经过N代的自然选择后，保存下来的个体都是适应度很高的，其中很可能包含史上产生的适应度最高的个体。&lt;/p&gt;
&lt;h1 id=&quot;算法流程&quot;&gt;算法流程&lt;/h1&gt;
&lt;h2 id=&quot;总体流程&quot;&gt;总体流程&lt;/h2&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/GA1.jpg&quot; alt=&quot;The process of GA&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;The process of GA&lt;/p&gt;
&lt;/div&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="进化算法" scheme="//Rocky1ee.github.io/tags/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>简化粒子群</title>
    <link href="//Rocky1ee.github.io/2019/09/03/%E7%AE%80%E5%8C%96%E7%BE%A4%E4%BC%98%E5%8C%96SSO/"/>
    <id>//Rocky1ee.github.io/2019/09/03/简化群优化SSO/</id>
    <published>2019-09-03T07:56:36.000Z</published>
    <updated>2019-12-10T05:54:19.335Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>Swarm-based optimization tend to suffer from premature convergence in the high dimensional problem space. thus，proposing simplified swarm optimization (SSO) algorithm to overcome the above convergence problem by incorporating it with the new <strong>local search strategy</strong> named ELS(Exchange Local Search).</p><h1 id="introduction">Introduction</h1><p>The ELS strategy is introduced to find a better solution from the neighbourhood of the current solution which is produced by SSO. It allows the particles to better explore the search space, and preserves swarm diversity which is important in preventing premature convergence of the particles.</p><a id="more"></a><h1 id="particle-swarm-optimization-and-simplified-swarm-optimization.">Particle Swarm Optimization and Simplified Swarm Optimization.</h1><h2 id="pso">PSO</h2><p><span class="math display">\[v_{id}^t = \omega\cdot v_{id}^{t-1}+c_1\cdot rand_1\cdot (p_{id}-x_{id}^{t-1})+c_2\cdot rand_2\cdot(p_{gd}-x_{id}^{t-1}) \quad (1)\]</span></p><p><span class="math display">\[x_{id}^t = x_{id}^{t-1}+v_{id}^t \quad (2)\]</span></p><p>The algorithm of the standard PSO is presented as follows:</p><ol style="list-style-type: decimal"><li>Initialize a population of particles with random positions and velocities.</li><li>Evaluate the fitness value of each particle in the population.</li><li>Get the <span class="math inline">\(pbest\)</span> value. If the fitness value of the particle <span class="math inline">\(i\)</span> is better than its <span class="math inline">\(pbest\)</span> fitness value, and then set the fitness value as a new <span class="math inline">\(pbest\)</span> of particle <span class="math inline">\(i\)</span>.</li><li>Get the <span class="math inline">\(gbest\)</span> value. If any <span class="math inline">\(pbest\)</span> is updated and it is better than the current <span class="math inline">\(gbest\)</span>, and then set <span class="math inline">\(gbest\)</span> to the current <span class="math inline">\(pbest\)</span> value of particle <span class="math inline">\(i\)</span>.</li><li>Update particle’s velocity and position according to (1) and (2).</li><li>Stop iteration if the best fitness value or the maximum generation is met; otherwise go back to step 2.</li></ol><h2 id="sso">SSO</h2><p>In every generation, the particle’s position value in each dimension will be kept or be updated by its <span class="math inline">\(pbest\)</span> value or by the <span class="math inline">\(gbest\)</span> value or be replaced by new random value according to this procedure. <span class="math display">\[x_{id} = \begin{cases}x_{id}^{t-1},\quad if\quad rand() \in[0,C_\omega]&amp;\\p_{id}^{t-1},\quad if\quad rand() \in[C_\omega,C_p]&amp;\\g_{id}^{t-1},\quad if\quad rand() \in[C_p,C_g]&amp;\\x ,\qquad if\quad rand()\in[C_g,1]&amp;\\\end{cases}\quad (3)\]</span> <span class="math inline">\(C_w\)</span> , <span class="math inline">\(C_p\)</span> and <span class="math inline">\(C_g\)</span> are three predetermined positive constants with <span class="math inline">\(C_w &lt; C_p &lt; C_g\)</span> .</p><div class="figure"><img src="/Assets/BlogImg/SSO1.png" alt="Flowchart of SSO algorithm"><p class="caption">Flowchart of SSO algorithm</p></div><p><strong>根据随机概率对粒子的位置进行更新。从而增强模型的随机性，防止提前收敛。</strong></p><h1 id="the-proposed-sso-els-data-mining-algorithm">The Proposed SSO-ELS Data Mining Algorithm</h1><p>SSO-ELS:Simplified Swarm Optimization (SSO) with Exchange Local Search scheme.</p><h2 id="the-rule-mining-encoding">The rule mining encoding</h2><p><span class="math display">\[LowerBound=x-rand()*(max(X_i)-min(X_i))\quad (4)\]</span></p><p><span class="math display">\[UpperBound=x+rand()*(max(X_i)-min(X_i))\quad (5)\]</span></p><p><strong>$ (max(X_i ) − min(X_i ))$ is the range value of the data source in each attribute.???</strong></p><h2 id="rule-evaluation">Rule evaluation</h2><p>P,N: What's your judgement about the sample?</p><p>T,F: Is your judgement right(True) or not(False)?</p><div class="figure"><img src="/Assets/BlogImg/ROC1.png" alt="Confusion matrix"><p class="caption">Confusion matrix</p></div><p>Evaluation index:</p><p><img src="/Assets/BlogImg/ROC2.png" alt="evaluation index-w60"> <span class="math display">\[The \quad rule\quad quality = sensitivety\times specifigity=\frac{TP}{TP+FN}\times \frac{TN}{TN+FP}\]</span></p><h2 id="rule-pruning">Rule pruning</h2><p><span class="math display">\[Prediction\quad value +=a*rule\quad quality +\beta*percentage\quad of\quad the \quad rule \quad covered\]</span></p><h2 id="the-proposed-exchange-local-search-strategy">The proposed exchange local search strategy</h2><p>The principle of the ELS applied in SSO and PSO rule mining is to exchange the lower bound and upper bound value for one selected attribute from the neighbour particle, re-evaluate the fitness value of the target particle, and then try to find out the new <span class="math inline">\(pbest\)</span> of the target particle or new <span class="math inline">\(gbest\)</span> in the swarm.</p><p>Step 1 Pre-determine local search time <span class="math inline">\((T)\)</span> which will only be used for <span class="math inline">\(gbest\)</span>.</p><p>Step 2 Choose a target particle$ (P_t )$. In this phase, <span class="math inline">\(gbest\)</span> will be the first target particle to be run in <span class="math inline">\(T\)</span> times’ of local search. Later, the other <span class="math inline">\(pbest\)</span> will be sequentially selected as target particles and they will only be run once in local search.</p><p>Step 3 Randomly select one attribute from the rule set in the dataset. This process is called exchangeAttribute.</p><p>Step 4 Randomly select two different neighbour particles, <span class="math inline">\(P_x\)</span> and <span class="math inline">\(P_y\)</span> from the population.</p><p>Step 5 Get a <span class="math inline">\(LowerBound(x)\)</span> of the selected attribute (selected from Step 3) of $P_x $, and get an <span class="math inline">\(UpperBound(y)\)</span> of the selected attribute of <span class="math inline">\(P_y\)</span> .</p><p>Step 6 Temporarily replace the corresponding <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> of the tar- get particle with <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p><p>Step 7 Re-evaluate the fitness value of the target particle.</p><p>Step 8 Check whether the fitness value is better than the current <span class="math inline">\(pbest\)</span> of the target particle or better than <span class="math inline">\(gbest\)</span>. If it is better, <span class="math inline">\(pbest\)</span> and <span class="math inline">\(gbest\)</span> will be updated,and the exchange value for the target particle will be kept; otherwise, the original <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> values will be positioned back to the target particle.</p><div class="figure"><img src="/Assets/BlogImg/ELS.png" alt="The process of the ELS"><p class="caption">The process of the ELS</p></div><div class="figure"><img src="/Assets/BlogImg/exchange%20Attribute.png" alt="The UpperBound and LowerBound exchange strategy for one selected attribute in each generation (note: UB = UpperBound, LB = LowerBound)"><p class="caption">The UpperBound and LowerBound exchange strategy for one selected attribute in each generation (note: UB = UpperBound, LB = LowerBound)</p></div><h1 id="conclusions">Conclusions</h1><p>The main idea of exchange local search is to improve and refine the searching process by exchanging the <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> for one selected attribute into the <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> of the corresponding target particle.</p><p>reference</p><p>precision and recall,ROC,AUC</p><p>https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c</p><p>A new simplified swarm optimization (SSO) using exchange local search scheme</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Swarm-based optimization tend to suffer from premature convergence in the high dimensional problem space. thus，proposing simplified swarm optimization (SSO) algorithm to overcome the above convergence problem by incorporating it with the new &lt;strong&gt;local search strategy&lt;/strong&gt; named ELS(Exchange Local Search).&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The ELS strategy is introduced to find a better solution from the neighbourhood of the current solution which is produced by SSO. It allows the particles to better explore the search space, and preserves swarm diversity which is important in preventing premature convergence of the particles.&lt;/p&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="粒子群算法" scheme="//Rocky1ee.github.io/tags/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>粒子群算法</title>
    <link href="//Rocky1ee.github.io/2019/09/01/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95PSO/"/>
    <id>//Rocky1ee.github.io/2019/09/01/粒子群算法PSO/</id>
    <published>2019-09-01T08:56:36.000Z</published>
    <updated>2019-12-10T07:45:55.605Z</updated>
    
    <content type="html"><![CDATA[<h1 id="算法思路">算法思路</h1><h2 id="基本思想">基本思想</h2><p>粒子群算法-particle swarm optimization。粒子群优化算法的基本思想是：通过群体中个体之间的协作和信息共享来寻找最优解。</p><p>我们可以设想这样的一个场景，一群鸟在随机搜寻食物。这个区域里只有一块食物。所有的鸟都不知道食物再哪里，但他们知道目前距离食物还有多远，那么找到食物的最佳策略是什么？</p><p><strong>1.找寻距离食物最近的鸟之周围区域</strong></p><p><strong>2.根据自己本身飞行的经验判断食物的所在</strong></p><p>在PSO中，每个优化问题的潜在解都可以想象成<span class="math inline">\(N\)</span>维搜索空间上的一个点，我们称之为“粒子”（Particle），由目标函数决定每个粒子的适应值，每个粒子还有一个速度决定他们运动的方向和距离，然后粒子们就追随当前的最优粒子在解空间中搜索。</p><h2 id="数学定义">数学定义</h2><p>粒子<span class="math inline">\(i\)</span>在<span class="math inline">\(N\)</span>维空间的位置为： <span class="math display">\[X_i = (x_1,x_2,...,x_N)\]</span> 运动速度表示为： <span class="math display">\[V_i=(v_1,v_2,...,v_N)\]</span> 每个粒子都有一个由目标函数决定的<strong>适应值</strong>，并且知道自己到目前为止的最好位置<span class="math inline">\((pbest_i)\)</span>和现在所在的位置<span class="math inline">\(X_i\)</span>．这个可以看作是粒子自己的运动经验。同时，每个粒子还知道到目前整个群体中位置最好的粒子<span class="math inline">\((gbest) (gbest是pbest_i中的最好值)\)</span>．这个可以看作是粒子同伴的经验。粒子就是通过自己的经验和同伴中最好的经验来决定下一步的运动。</p><a id="more"></a><h1 id="算法基本内容">算法基本内容</h1><h2 id="初始化">初始化</h2><p>对于每个粒子的位置<span class="math inline">\(X_i\)</span>和运动速度<span class="math inline">\(V_i\)</span>进行随机初始化</p><h2 id="更新">更新</h2><p>粒子速度的更新： <span class="math display">\[V_i = V_i+c_1\times rand()\times(pbest_i-X_i) + c_2\times rand()\times(gbest-X_i)\qquad公式(1)\]</span> 位置的更新： <span class="math display">\[X_i = X_i+V_i\qquad 公式(2)\]</span> 假设种群中粒子的总数为<span class="math inline">\(M\)</span>。<span class="math inline">\(i \in [1,M]\)</span>。</p><p>随机数<span class="math inline">\(rand() \in (0,1)\)</span>。</p><p>学习因子：<span class="math inline">\(c_1和c_2\)</span></p><p>假设空间的维度为<span class="math inline">\(N\)</span>,在每一维度上，<span class="math inline">\(v_d的上界为v_{dmax},d \in [1,N]\)</span></p><p>公式（1）的第一部分称为<strong>记忆项</strong>，表示上次速度大小和方向的影响；</p><p>第二部分称为<strong>自身认知项</strong>，是从当前点指向粒子自身最好点的一个矢量，粒子自身经验所决定的运动；</p><p>第三部分称为<strong>群体认知项</strong>，是从当前点指向种群最好点的矢量，反映了粒子间的协同合作。</p><p>粒子通过自身的经验和同伴中最好的经验来决定下一步的运动。</p><div class="figure"><img src="/Assets/BlogImg/粒子群算法1.png" alt="粒子群算法1"><p class="caption">粒子群算法1</p></div><h2 id="引入惯性权重因子">引入惯性权重因子</h2><p>为了完善算法引入惯性权重因子:<span class="math inline">\(\omega \geq 0\)</span>,公式(2)变为： <span class="math display">\[V_i = \omega \times V_i+c_1\times rand()\times(pbest_i-X_i) + c_2\times rand()\times(gbest-X_i)\qquad公式(3)\]</span> 当<span class="math inline">\(\omega\)</span>越大，越倾向于寻找全局最优。当<span class="math inline">\(\omega\)</span>越小越倾向于寻找局部最优。</p><h2 id="动态权重因子">动态权重因子</h2><p>试验发现动态权重因子比固定权重因子更有效。目前经常采用线性递减权值策略（linearly decreasing weight，LDW） <span class="math display">\[\omega=(\omega_{ini}-\omega_{end})(G_k-g)/G_k+\omega_{end}\]</span> <span class="math display">\[\omega_{ini}:初始惯性权值， \omega_{end}:迭代至最大数时惯性权值，G_k:最大迭代数， g:当前迭代数，\]</span></p><p>公式(3)被称为标准PSO。</p><h2 id="全局最优法">全局最优法</h2><p>当<span class="math inline">\(c_1=0\)</span>时，粒子没有了认知能力，变为<strong>只有社会的模型(social-only)</strong>： <span class="math display">\[V_i = \omega \times V_i + c_2\times rand()\times(gbest-X_i)\qquad公式(4)\]</span> 被称为<strong>全局PSO算法</strong>。粒子有扩展搜索空间能力，具有较快收敛速度，由于缺少局部搜索，对于复杂问题比标准PSO 更易陷入局部最优。</p><h2 id="局部最优法">局部最优法</h2><p>当<span class="math inline">\(c_2=0\)</span>时，则粒子之间没有社会信息，模型变为<strong>只有自我(cognition-only)模型</strong>： <span class="math display">\[V_i = \omega \times V_i+c_1\times rand()\times(pbest_i-X_i) \qquad公式(5)\]</span> 被称为<strong>局部PSO算法</strong>。由于个体之间没有信息的交流，整个群体相当于多个粒子进行盲目的随机搜索，收敛速度慢，因而得到最优解的可能性小。</p><h2 id="算法流程">算法流程</h2><div class="figure"><img src="/Assets/BlogImg/粒子群算法2.png" alt="粒子群算法2"><p class="caption">粒子群算法2</p></div><h1 id="实例应用">实例应用</h1><p>用粒子群算法求解<span class="math inline">\(y=f(x_1,x_2)=x_1^2+x_2^2,-10&lt;x_1,x_2&lt;10\)</span>的最小值。已知当<span class="math inline">\(x_1=x_2=0\)</span>时<span class="math inline">\(y\)</span>取最小值。现在用粒子群算法求解。</p><h2 id="初始化-1">初始化</h2><p>设种群大小<span class="math inline">\(M=3\)</span>;惯性权重<span class="math inline">\(\omega=0.5\)</span>;<span class="math inline">\(c_1=c_2=2\)</span>;<span class="math inline">\(r_1,r_2是[0,1]\)</span>内随机数。</p><p>假设个体的初始位置和速度分别为： <span class="math display">\[p_1=\begin{cases}v_1=(3,2)&amp;\\x_1=(8,-5)\end{cases}p_2=\begin{cases}v_1=(-3,-2)&amp;\\x_1=(-5,9)\end{cases}p_3=\begin{cases}v_1=(5,2)&amp;\\x_1=(-7,-8)\end{cases}\]</span> 计算适应函数值，并且得到粒子的历史最优位置和群体的全局最优位置 <span class="math display">\[\begin{cases}f_1=8^2+(-5)^2=89&amp;\\pbest=x_1=(8,-5)\end{cases}\begin{cases}f_2=(-5)^2+9^2=106&amp;\\pbest=x_2=(-5,9)\end{cases}\begin{cases}f_3=(-7)^2+(-8)^2=113&amp;\\pbest=x_3=(-7,-8)\end{cases}\]</span></p><p><span class="math display">\[gbest=pbest=(8,-5)\]</span></p><h2 id="粒子速度和位置更新">粒子速度和位置更新</h2><p><strong>根据自身的历史最优位置和全局最优位置，更新每个粒子的速度和位置：</strong> <span class="math display">\[p1=\begin{cases}V_1 = \omega \times V_1+c_1\times r_1\times(pbest_1-X_1) + c_2\times r_2\times(gbest-X_1)&amp;\\\Rightarrow v_1=\begin{cases}0.5\times3+0+0=1.5 &amp;\\0.5\times2+0+0=1\end{cases}&amp;\\x_1=x_1+v_1=(8,-5)+(1.5,1)=(9.5,-4)\end{cases}\]</span></p><p><span class="math display">\[p2=\begin{cases}V_2 = \omega \times V_2+c_1\times r_1\times(pbest_2-X_2) + c_2\times r_2\times(gbest-X_2)&amp;\\\Rightarrow v_2=\begin{cases}0.5\times-3+0+2\times0.3\times(8-(-5))=6.1 &amp;\\0.5\times2+0+2\times0.3\times(-5-9)=1.8\end{cases}&amp;\\x_2=x_2+v_2=(-5,9)+(6.1,1.8)=(1.1,10)\end{cases}\]</span></p><p><span class="math display">\[p3=\begin{cases}V_3 = \omega \times V_3+c_1\times r_1\times(pbest_3-X_3) + c_2\times r_2\times(gbest-X_3)&amp;\\\Rightarrow v_2=\begin{cases}0.5\times5+0+2\times0.3\times(8-(-7))=11.5 &amp;\\0.5\times3+0+2\times0.3\times(-5-(-8))=3.3\end{cases}&amp;\\x_3=x_3+v_3=(-7,-8)+(11.5,3.3)=(4.5,-4.7)\end{cases}\]</span></p><h2 id="更新粒子的历史最优位置和全局最优位置">更新粒子的历史最优位置和全局最优位置：</h2><p><span class="math display">\[f_1^*=9.5^2+(-4)^2=106.25&gt;f_1=89\\\begin{cases}f_1=89&amp;\\pbest_1=(8,-5)\end{cases}\]</span></p><p><span class="math display">\[f_2^*=1.1^2+10^2=101.21&lt;f_2=106\\\begin{cases}f_2=f_2^*=101.32&amp;\\pbest_2=(1.1,10)\end{cases}\]</span></p><p><span class="math display">\[f_3^*=4.5^2+(-4.7)^2=42.34&lt;f_3=113\\\begin{cases}f_3=f_3^*=42.34&amp;\\pbest_3=(4.5,-4.7)\end{cases}\]</span></p><p><span class="math display">\[gbest=pbest_3=(4.5,-4.7)\]</span></p><h2 id="迭代">迭代</h2><p>如果满足结束条件，则输出全局最优结果并结束程序，否则，转向第二步继续执行。</p><!-- more -->]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;算法思路&quot;&gt;算法思路&lt;/h1&gt;
&lt;h2 id=&quot;基本思想&quot;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;粒子群算法-particle swarm optimization。粒子群优化算法的基本思想是：通过群体中个体之间的协作和信息共享来寻找最优解。&lt;/p&gt;
&lt;p&gt;我们可以设想这样的一个场景，一群鸟在随机搜寻食物。这个区域里只有一块食物。所有的鸟都不知道食物再哪里，但他们知道目前距离食物还有多远，那么找到食物的最佳策略是什么？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.找寻距离食物最近的鸟之周围区域&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.根据自己本身飞行的经验判断食物的所在&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在PSO中，每个优化问题的潜在解都可以想象成&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;维搜索空间上的一个点，我们称之为“粒子”（Particle），由目标函数决定每个粒子的适应值，每个粒子还有一个速度决定他们运动的方向和距离，然后粒子们就追随当前的最优粒子在解空间中搜索。&lt;/p&gt;
&lt;h2 id=&quot;数学定义&quot;&gt;数学定义&lt;/h2&gt;
&lt;p&gt;粒子&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;在&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;维空间的位置为： &lt;span class=&quot;math display&quot;&gt;\[
X_i = (x_1,x_2,...,x_N)
\]&lt;/span&gt; 运动速度表示为： &lt;span class=&quot;math display&quot;&gt;\[
V_i=(v_1,v_2,...,v_N)
\]&lt;/span&gt; 每个粒子都有一个由目标函数决定的&lt;strong&gt;适应值&lt;/strong&gt;，并且知道自己到目前为止的最好位置&lt;span class=&quot;math inline&quot;&gt;\((pbest_i)\)&lt;/span&gt;和现在所在的位置&lt;span class=&quot;math inline&quot;&gt;\(X_i\)&lt;/span&gt;．这个可以看作是粒子自己的运动经验。同时，每个粒子还知道到目前整个群体中位置最好的粒子&lt;span class=&quot;math inline&quot;&gt;\((gbest) (gbest是pbest_i中的最好值)\)&lt;/span&gt;．这个可以看作是粒子同伴的经验。粒子就是通过自己的经验和同伴中最好的经验来决定下一步的运动。&lt;/p&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="粒子群算法" scheme="//Rocky1ee.github.io/tags/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>蚁群算法</title>
    <link href="//Rocky1ee.github.io/2019/08/31/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95(ACO)/"/>
    <id>//Rocky1ee.github.io/2019/08/31/蚁群算法(ACO)/</id>
    <published>2019-08-31T08:56:36.000Z</published>
    <updated>2019-12-10T07:45:43.146Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本原理">基本原理</h1><div class="figure"><img src="/Assets/BlogImg/蚁群算法.png" alt="蚁群算法1"><p class="caption">蚁群算法1</p></div><div class="figure"><img src="/Assets/BlogImg/蚁群算法1.png" alt="蚁群算法2"><p class="caption">蚁群算法2</p></div><div class="figure"><img src="/Assets/BlogImg/蚁群算法2.png" alt="蚁群算法3"><p class="caption">蚁群算法3</p></div><a id="more"></a><h1 id="基本流程">基本流程</h1><h2 id="路径构建">路径构建</h2><p><span class="math display">\[\begin{equation}    P_{ij}(t) =   \begin{cases}   \frac{[\tau_{ij}(t)]^\alpha \times [\eta_{ij}(t)]^\beta}{\sum\limits_{k\in allowed_k }[\tau_{ik}(t)]^\alpha \times [\eta_{ik}(t)]^\beta}&amp;   {if \quad j \in allowed_k}\\   \qquad 0&amp;   others   \end{cases}  \end{equation}\]</span></p><hr><ul><li><span class="math inline">\(i,j\)</span>分别为起点和终点</li><li><span class="math inline">\(\eta_{ij} = \frac{1}{d_{ij}}\)</span>为能见度，是两点<span class="math inline">\(i、j\)</span>间的导数</li><li><span class="math inline">\(\tau_{ij}(t)\)</span>为时间<span class="math inline">\(t\)</span>时由<span class="math inline">\(i\)</span>到<span class="math inline">\(j\)</span>的信息素强度</li><li><span class="math inline">\(allowed_k\)</span>为从<span class="math inline">\(i\)</span>出发可访问到节点的集合</li><li><span class="math inline">\(\alpha,\beta\)</span>为两常数，分别是信息素和能见度的加权值</li></ul><p>结果表示<strong>当前点到每个可能的下个节点的概率</strong></p><h2 id="信息素更新">信息素更新</h2><p>蚂蚁的信息素释放量<span class="math inline">\(C(0)\)</span>,如果太小则容易导致局部最优。为什么？</p><p>如果太大，则对搜索方向的导向作用降低。一般用贪婪算法获取一个路径值<span class="math inline">\(Cnn\)</span>，然后根据蚂蚁个数来计算<span class="math inline">\(C(0) = m/Cnn\)</span>,<span class="math inline">\(m\)</span>为蚂蚁个数。</p><p>信息素更新如下：</p><p><span class="math display">\[\tau(t)=(1-p)\tau_{ij}+\sum_{k=1}^m\Delta\tau_{ij}^k\]</span> <span class="math inline">\(m\)</span>:蚂蚁个数，<span class="math inline">\(0&lt;\rho&lt;=1\)</span>:信息素蒸发率,<span class="math inline">\(\Delta\tau_{ij}^k\)</span>:第<span class="math inline">\(k\)</span>只蚂蚁在路径<span class="math inline">\(i\)</span>到 <span class="math inline">\(j\)</span> 所留下的信息素 <span class="math display">\[\begin{equation}    \Delta \tau_{ij}^k =   \begin{cases}   (c_k)^{-1}&amp;\quad 第k^{th}只蚂蚁经过路径(i，j)\\\\   0&amp;\quad others   \end{cases}  \end{equation}\]</span> 信息素挥发(evaporation):避免算法过快地向局部最优区域集中，有助于搜索区域的扩展。</p><p>信息素增强(reinforcement):指引最优路径的指南。</p><h2 id="迭代与停止">迭代与停止</h2><p>算法每次迭代：<strong>每次迭代的m只蚂蚁都完成了自己的路径过程，回到原点后的整个过程。</strong></p><p>停止：指定迭代次数或达成指定的最优解条件</p><h1 id="蚁群算法的实例">蚁群算法的实例</h1><div class="figure"><img src="/Assets/BlogImg/蚁群算法4.png" alt="蚁群算法4"><p class="caption">蚁群算法4</p></div><p>假设共<span class="math inline">\(m=3\)</span>只蚂蚁，参数<span class="math inline">\(\alpha=1,\beta=2,\rho=0.5\)</span></p><h2 id="初始化">初始化</h2><p>首先使用贪婪算法得到路径的<strong>(ACDBA)</strong>, 则<span class="math inline">\(C_{nn}=1+2+4+3=10\)</span>,求得<span class="math inline">\(\tau_0=m\div C_{nn}=0.3\)</span> <span class="math display">\[\tau(0)=\begin{bmatrix}0&amp;0.3&amp;0.3&amp;0.3 \\0.3&amp;0&amp;0.3&amp;0.3 \\0.3&amp;0.3&amp;0&amp;0.3 \\0.3&amp;0.3&amp;0.3&amp;0\end{bmatrix}\]</span></p><h2 id="出发地">出发地</h2><p>为每个蚂蚁随机选择出发城市，假设蚂蚁1选择城市A，蚂蚁2选择城市B，蚂蚁3选择城市D。</p><h2 id="访问地">访问地</h2><p>以蚂蚁1为例，当前城市<span class="math inline">\(i=A\)</span>,可访问城市集合<span class="math inline">\(J_1 (i)={B,C,D}\)</span></p><p>计算蚂蚁1访问各个城市的概率 <span class="math display">\[A \Rightarrow \begin{cases}B:\tau_{AB}^a\times\eta_{AB}^\beta=0.3^1\times\frac{1}{3}^2=0.033\\\\C:\tau_{AC}^a\times\eta_{AC}^\beta=0.3^1\times\frac{1}{1}^2=0.300\\\\D:\tau_{AD}^a\times\eta_{AD}^\beta=0.3^1\times\frac{1}{2}^2=0.075\\\\\end{cases}\]</span></p><p><span class="math display">\[P(B)=\frac{0.033}{0.033+0.3+0.075}=0.08 \\\\P(C)=\frac{0.03}{0.033+0.3+0.075}=0.74 \\\\P(D)=\frac{0.075}{0.033+0.3+0.075}=0.1\]</span></p><div class="figure"><img src="/Assets/BlogImg/轮盘赌选择法.jpeg" alt="轮盘赌选择法"><p class="caption">轮盘赌选择法</p></div><p>轮盘赌选择法：选择累积概率超过随机值的第一个个体。</p><p>用轮盘赌法选择下一个访问城市。假设产生的随机值<span class="math inline">\(r=0.05\)</span>，则蚂蚁<strong>1</strong>会选择城市<strong>B</strong> 同样，假设蚂蚁<strong>2</strong>选择城市<strong>D</strong>，蚂蚁<strong>3</strong>选择城市<strong>A</strong>。</p><p>现在蚂蚁<strong>1</strong>已经来到<strong>B</strong>了，路径记忆向量<span class="math inline">\(R^l=(AB)\)</span>,可访问城市集合<span class="math inline">\(J_i(i)={C,D}\)</span></p><p>蚂蚁<strong>1</strong>访问<strong>C，D</strong>城市的概率： <span class="math display">\[B\Rightarrow\begin{cases}C:\tau_{BC}^\alpha\times\eta_{BC}^\beta=0.3^1\times\frac{1}{5}^2=0.012\\\\D:\tau_{BD}^\alpha\times\eta_{BD}^\beta=0.3^1\times\frac{1}{4}^2=0.019\end{cases}\\\\P(C)=\frac{0.012}{0.012+0.019}=0.39\\\\P(D)=\frac{0.019}{0.012+0.019}=0.61\]</span> 用轮盘赌法选择下一个访问城市。假设产生的随机数<span class="math inline">\(r=0.67\)</span>，则蚂蚁<strong>1</strong>会选择城市<strong>D</strong> 同样，假设蚂蚁<strong>2</strong>选择城市<strong>C</strong>，蚂蚁<strong>3</strong>选择城市<strong>D</strong>。</p><p>最终，所有蚂蚁选择的路径为：</p><p>蚂蚁1：ABDCA</p><p>蚂蚁2：BDCAB</p><p>蚂蚁3：DACBD</p><h2 id="信息素更新-1">信息素更新</h2><p>每只蚂蚁走过的路径长度：<span class="math inline">\(C_1=3+4+2+1=10;C_2=4+2+1+3=10;C_3=2+1+5+4=12。\)</span></p><p>每条边信息素的更新如下: <span class="math display">\[\tau_{AB}=（1-\rho)\times\tau_{AB} + \sum_{k=1}^3\Delta\tau_{AB}^k=0.5\times0.3+(\frac{1}{10}+\frac{1}{10})=0.35\\\\\tau_{AC}=（1-\rho)\times\tau_{AC} + \sum_{k=1}^3\Delta\tau_{AC}^k=0.5\times0.3+(\frac{1}{12})=0.16\\\\\tau_{AD},\tau_{BC}...\]</span> 在<span class="math inline">\(\tau_{AB}\)</span>的计算中蚂蚁3并未走过路径<strong>AB</strong>，所以<span class="math inline">\(\tau_{AB}^3=0\)</span>。</p><h2 id="结束">结束</h2><p>如果满足结束条件，则输出全局最优结果并结束程序，否则，则转向步骤2继续执行。</p><p>references</p><p>https://www.cnblogs.com/asxinyu/p/Path_Optimization_Tsp_Problem_Ant_System_CSharp.html#opennewwindow</p><!-- more -->]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;基本原理&quot;&gt;基本原理&lt;/h1&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/蚁群算法.png&quot; alt=&quot;蚁群算法1&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;蚁群算法1&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/蚁群算法1.png&quot; alt=&quot;蚁群算法2&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;蚁群算法2&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/Assets/BlogImg/蚁群算法2.png&quot; alt=&quot;蚁群算法3&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;蚁群算法3&lt;/p&gt;
&lt;/div&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="蚁群算法" scheme="//Rocky1ee.github.io/tags/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>conditional GAN De-raining</title>
    <link href="//Rocky1ee.github.io/2019/07/31/%E6%9D%A1%E4%BB%B6GAN%E5%8E%BB%E9%9B%A8/"/>
    <id>//Rocky1ee.github.io/2019/07/31/条件GAN去雨/</id>
    <published>2019-07-31T08:56:36.000Z</published>
    <updated>2019-12-13T03:19:16.364Z</updated>
    
    <content type="html"><![CDATA[<p>Image De-raining Using a Conditional Generative Adversarial Network</p><h1 id="abstract">Abstract</h1><p>在C-GAN中执行限制条件使去雨后的图像与相应的真实图像一致。其中的adversarial loss 有助于获取更好结果。此外在G和D中提出新的refined loss function（降低创造能力使图像更加真实）和新的网络结构提升效果。G使用densely connected networks），D可利用局部和全局信息进行真伪判断。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>一张有雨的图像可以被分为2部分</p><div class="figure"><img src="/Assets/Img/De-rain.png" alt="X = y + w"><p class="caption">X = y + w</p></div><p>数学上可以定义为： <span class="math display">\[X = y + w\]</span> <span class="math inline">\(X:(a),y:(b),w:(c)\)</span></p><p>C-GAN由densely-connected G和multi-scale D组成，为了防止GAN生成新的图像（与无雨原图不一致）引入refined perceptual loss，同时提出multi-scale D充分利用不同尺寸的信息对de-rained图像进行真伪判断。</p><h1 id="proposed-method">Proposed Method</h1><div class="figure"><img src="/Assets/Img/De-rain3.png" alt="overview of the ID-CGAN"><p class="caption">overview of the ID-CGAN</p></div><p>G 是个对称且带有skip connections的densely-connected network用于生成de-rained image（输入为rainy image）。</p><p>D 是个multi-scale network 用于判断G生成去雨图像的真伪。真对应:ground truth image，伪对应：G生成的de-rained image。</p><p>定义refined perceptual loss functions解决G的 image synthesized 问题。</p><h2 id="gan-objective-function">GAN Objective Function</h2><p><span class="math display">\[\min_{G}\max_{D} \Epsilon_{x\sim p_{data(x)}},[log(1-D(x,G(x)))]+\Epsilon_{x\sim p_{data(x,y)}}[logD(x,y)]\]</span></p><p><span class="math inline">\(x\)</span>:rainy image,<span class="math inline">\(y\)</span>:synthesized de-rained image</p><h2 id="generator-with-symmetric-structure">Generator with Symmetric Structure</h2><div class="figure"><img src="/Assets/Img/De-rain4.png" alt="Structure of G"><p class="caption">Structure of G</p></div><p>G采用了对称跳跃连接的形式，其结构如下： <span class="math display">\[\begin{equation}\begin{aligned}&amp;CBLP(64)-\\&amp;D(256)-Td(128)-D(512)-Td(256)-D(1024)-Tn(512)-D(768)-Tn(128)-\\&amp;D(640)-Tu(120)-D(384)-Tu(64)\quad-D(192)-Tu(64)-\quad D(32)-Tn(16)-\\&amp;C(3)-Tanh\end{aligned}\end{equation}\]</span> C：Convolution，B：BN，L：ReLU，P:Pooling，Td：down-sampling，Tu：up-sampling，</p><p>Tn：no-sampling。</p><div class="figure"><img src="/Assets/Img/De-raind-t2.png" alt="architecture of G"><p class="caption">architecture of G</p></div><h2 id="multi-scale-discriminator">Multi-scale Discriminator</h2><div class="figure"><img src="/Assets/Img/De-rain5.png" alt="structure of D"><p class="caption">structure of D</p></div><p>输入G生成的de-raind image 和 对应datasets 中的ground truth，经过Conv+BN+PRelU得到特征图，对特征图进行多尺度的down-sampling的多尺度的特征图，然后up-samping成统一尺寸，之后用1x1 Conv进行融合。最后用sigmoid函数进行评分。</p><div class="figure"><img src="/Assets/Img/De-raind-t3.png" alt="architecture for the D"><p class="caption">architecture for the D</p></div><h2 id="refined-perceptual-loss">Refined Perceptual Loss</h2><p>引入 perceptual loss 解决G的合成问题。loss 由 pixel-to-pixel欧几里德loss，perceptual loss，adversarial loss 以不同权重组合而成 <span class="math display">\[L_{RP}=L_E+\lambda_a L_A+\lambda_pL_p\]</span> <span class="math inline">\(L_A\)</span>:adversarial loss,<span class="math inline">\(L_E\)</span>:输出图和ground truth的欧几里德loss ，<span class="math inline">\(L_P\)</span>:perceptual loss, 如果<span class="math inline">\(\lambda_a和\lambda_b\)</span>同时为0，则network变为CNN。若只是<span class="math inline">\(\lambda_p=0\)</span>，则network变为标准 的GAN。</p><p>其中<span class="math inline">\(L_E\)</span>为： <span class="math display">\[L_E=\frac{1}{CWH}\sum_{c=1}^C\sum_{x=1}^W\sum_{y=1}^H \left\| \phi_E(x)^{c,w,h} -(y_b)^{c,w,h}\right\|^2_2\]</span> C:channels,W:width,H:height。x:input image, <span class="math inline">\(y_b\)</span>:corresponding ground truth ,</p><p><span class="math inline">\(\phi_E(x)\)</span>:mapping G <span class="math display">\[L_P=\frac{1}{C_iW_iH_i}\sum_{c=1}^{C_i}\sum_{x=1}^{W_i}\sum_{y=1}^{H_i} \left\| V(\phi_E(x)^{c,w,h}) -V((y_b)^{c,w,h})\right\|^2_2\]</span> V:non-linear CNN transformation. <span class="math display">\[L_A=-\frac{1}{N}\sum_{i=1}^Nlog(D(\phi_E(x)))\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Image De-raining Using a Conditional Generative Adversarial Network&lt;/p&gt;
&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;在C-GAN中执行限制条件使去雨后的图像与相应的真实图像一致。其中的adversarial loss 有助于获取更好结果。此外在G和D中提出新的refined loss function（降低创造能力使图像更加真实）和新的网络结构提升效果。G使用densely connected networks），D可利用局部和全局信息进行真伪判断。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
</feed>
