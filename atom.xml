<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rocky1ee的博客</title>
  <icon>https://www.gravatar.com/avatar/cdf2628d43f941c34796949e0857e3a5</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="//Rocky1ee.github.io/"/>
  <updated>2020-04-18T09:14:50.218Z</updated>
  <id>//Rocky1ee.github.io/</id>
  
  <author>
    <name>Rocky1ee</name>
    <email>666@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>人生的智慧</title>
    <link href="//Rocky1ee.github.io/2020/04/05/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%99%BA%E6%85%A7-%E5%8F%94%E6%9C%AC%E5%8D%8E/"/>
    <id>//Rocky1ee.github.io/2020/04/05/人生的智慧-叔本华/</id>
    <published>2020-04-05T08:56:36.000Z</published>
    <updated>2020-04-18T09:14:50.218Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主题">主题</h1><p>如何尽量幸福，愉快地度过一生？</p><h1 id="基本的划分">基本的划分</h1><p>决定人们命运的根本差别在于三项内容：人的自身，即在最广泛范围内属于人的东西，包括健康、力量、外貌、气质、道德品格、精神智力及其潜在发展；人所拥有的身外之物，包括财产及其它占有物；人向其他人所显示的样子，即他人的看法，包括名誉、地位和名声。</p><p>保留自己的个性，人应该选择与其个性相匹配的地位、职业和生活方式，遵循个性的发展。</p><p>保持身体健康和发挥个人才能远比挣钱更为明智。</p><p>一个人的才能和财产，决定了这个人在他人眼中的价值，他人眼中的高价值又能帮助这个人获得更多财产和进一步提升才能，二者相互影响，共同促进。</p><h1 id="人的自身">人的自身</h1><p>人的自身是决定其人生幸福的关键因素。其中健康具有压倒性的优势。</p><p>人们在生活中所遭遇的事情，其重要性远远不及他们对这些事情的感受方式。无论身在何处，我们只能从自身寻找或获得幸福。</p><p>理性的人追求的是免于痛苦，而非寻欢作乐。即生活之中不要将注意力放在享乐和安逸之上，而是要致力于免于痛苦。幸福的生活应该被理解为少了很多不幸的生活。只有如此我们才会去追求更为自律，更为健康的生活态度，以此去避免不幸的来临。</p><p>痛苦和无聊是人类幸福的两个天敌，每当远离一个敌人也就意味着接近另外一个敌人，生活就是在这两者当中或强或弱地摇摆。</p><h1 id="人所拥有的财产">人所拥有的财产</h1><p>财产可以让我们避免痛苦，并为我们追求幸福提供基础。但是拥有财产不一定会幸福。</p><h1 id="人的表象">人的表象</h1><p>我们通常都会过分看重他人对我们的看法，但是他人的看法就其本身来说对我们的幸福并非至关重要，不要过分看重他人的看法。学会正确评估自身的价值和看待他人对我们的看法，首先尽量减低我们对待别人意见评价的敏感程度，恰如其分的评价它的价值。这有助于人生的幸福。</p><h1 id="建议">建议</h1><p>理性的人寻求的不是快乐，而是没有痛苦。</p><h2 id="认识自我">认识自我</h2><p>“认识自己”的第一步，即清楚自己首要的意愿并对于第二和第三位置的意愿也能做到心中有数。</p><p>我们应该大致明白自己能够从事何种职业，需要扮演何种角色以及自己与这一世界的关系。</p><p>我们在晚上睡觉前，应该详细地逐一检查自己在当天的所作所为，对那些值得回味时刻的记忆和记录，应该小心保存下来。</p><p>人应该学会在人群中保持一定程度的孤独：不要把自己的想法马上告诉别人，不要对别人所说的话太过当真，不要对别人有太多的期待，不要去跟别人比较。</p><p>当我们在做某件事情时，我们应该深思熟虑地计划一番，而当我们已经付诸行动后，我们就不要再回头考虑和担忧了，而是投入到当前的局势中。</p><p>当我们看到某样东西时，我们不妨想象下如果失去它会怎么样，因为在大多数情况下，只有在失去某物之后，我们才会知道它的价值。</p><h2 id="对待他人">对待他人</h2><p>人们只是表现出他们真实的样子，很难通过外力改变，我们应该包容每一个人每一种真实的个性，包括我们不喜欢的，而不是去生他们的气，去谴责他们，甚至去试图改变他们。</p><p>每个人都只能根据自己的思想智力经历去判断他人，很难看到自身之外的东西。所以不要去评判他人。</p><p>人为了保护自己很可能会带上面具，我们要学会理解。太真实的人格他人也不一定能接受。学会适当的展示自我。</p><h2 id="对待环境">对待环境</h2><p>人生确实需要运气，但我们要尽量不受其摆布。</p><p>我们不仅要具备才能，同时还要具备为自己创造机会的能力，要习惯在准备不充分时积极的行事。</p><p>我们应该谨慎细心地预见和避开各种可能的不幸。面对环境的变化调整自我的行为。</p><p>环境对自身的形成具有巨大的影响，良好的环境会促进自身的发展，同时自身能力越强也更容易融入良好的环境。所以既要注重自身能力的积累，也要了解外界信息把握机遇。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;主题&quot;&gt;主题&lt;/h1&gt;
&lt;p&gt;如何尽量幸福，愉快地度过一生？&lt;/p&gt;
&lt;h1 id=&quot;基本的划分&quot;&gt;基本的划分&lt;/h1&gt;
&lt;p&gt;决定人们命运的根本差别在于三项内容：人的自身，即在最广泛范围内属于人的东西，包括健康、力量、外貌、气质、道德品格、精神智力及其潜在
      
    
    </summary>
    
      <category term="读书" scheme="//Rocky1ee.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="人生" scheme="//Rocky1ee.github.io/tags/%E4%BA%BA%E7%94%9F/"/>
    
      <category term="方法论" scheme="//Rocky1ee.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>conditional GAN De-raining</title>
    <link href="//Rocky1ee.github.io/2020/03/31/%E6%9D%A1%E4%BB%B6GAN%E5%8E%BB%E9%9B%A8/"/>
    <id>//Rocky1ee.github.io/2020/03/31/条件GAN去雨/</id>
    <published>2020-03-31T08:56:36.000Z</published>
    <updated>2020-04-18T07:32:41.307Z</updated>
    
    <content type="html"><![CDATA[<p>Image De-raining Using a Conditional Generative Adversarial Network</p><h1 id="abstract">Abstract</h1><p>在C-GAN中执行限制条件使去雨后的图像与相应的真实图像一致。其中的adversarial loss 有助于获取更好结果。此外在G和D中提出新的refined loss function（降低创造能力使图像更加真实）和新的网络结构提升效果。G使用densely connected networks，D的多尺度结构可利用局部和全局信息进行生成图像真伪判断。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>一张有雨的图像可以被分为2部分</p><figure><img src="/Assets/Img/De-rain.png" alt><figcaption>X = y + w</figcaption></figure><p>数学上可以定义为： <span class="math display">\[X = y + w\]</span> <span class="math inline">\(X:(a),y:(b),w:(c)\)</span></p><p>C-GAN由densely-connected G和multi-scale D组成，为了防止GAN生成新的图像（与无雨原图不一致）引入refined perceptual loss，同时提出multi-scale D充分利用不同尺寸的信息对de-rained图像进行真伪判断。</p><h1 id="proposed-method">Proposed Method</h1><figure><img src="/Assets/Img/De-rain3.png" alt><figcaption>overview of the ID-CGAN</figcaption></figure><p>G 是个对称且带有skip connections的densely-connected network用于生成de-rained image（输入为rainy image）。</p><p>D 是个multi-scale network 用于判断G生成去雨图像的真伪。真对应:ground truth image，伪对应：G生成的de-rained image。</p><p>定义refined perceptual loss functions解决G的 image synthesized 问题。</p><h2 id="gan-objective-function">GAN Objective Function</h2><p><span class="math display">\[\min_{G}\max_{D} \Epsilon_{x\sim p_{data(x)}},[log(1-D(x,G(x)))]+\Epsilon_{x\sim p_{data(x,y)}}[logD(x,y)]\]</span></p><p><span class="math inline">\(x\)</span>:rainy image,<span class="math inline">\(y\)</span>:synthesized de-rained image，以<span class="math inline">\(x\)</span>作为条件。</p><h2 id="generator-with-symmetric-structure">Generator with Symmetric Structure</h2><figure><img src="/Assets/Img/De-rain4.png" alt><figcaption>Structure of G</figcaption></figure><p>G采用了对称跳跃连接的形式，其结构如下： <span class="math display">\[\begin{equation}\begin{aligned}&amp;CBLP(64)-\\&amp;D(256)-Td(128)-D(512)-Td(256)-D(1024)-Tn(512)-D(768)-Tn(128)-\\&amp;D(640)-Tu(120)-D(384)-Tu(64)\quad-D(192)-Tu(64)-\quad D(32)-Tn(16)-\\&amp;C(3)-Tanh\end{aligned}\end{equation}\]</span> C：Convolution，B：BN，L：ReLU，P:Pooling，Td：down-sampling，Tu：up-sampling，</p><p>Tn：no-sampling。</p><figure><img src="/Assets/Img/De-raind-t2.png" alt><figcaption>architecture of G</figcaption></figure><h2 id="multi-scale-discriminator">Multi-scale Discriminator</h2><figure><img src="/Assets/Img/De-rain5.png" alt><figcaption>structure of D</figcaption></figure><p>输入G生成的de-raind image 和 对应datasets 中的ground truth，经过Conv+BN+PRelU得到特征图，对特征图进行多尺度的down-sampling的多尺度的特征图，然后up-samping成统一尺寸，之后用1x1 Conv进行融合。最后用sigmoid函数进行评分。</p><figure><img src="/Assets/Img/De-raind-t3.png" alt><figcaption>architecture for the D</figcaption></figure><h2 id="refined-perceptual-loss">Refined Perceptual Loss</h2><p>引入 perceptual loss 解决G的合成问题。loss 由 pixel-to-pixel欧几里德loss，perceptual loss，adversarial loss 以不同权重组合而成 <span class="math display">\[L_{RP}=L_E+\lambda_a L_A+\lambda_pL_p\]</span> <span class="math inline">\(L_A\)</span>:adversarial loss,<span class="math inline">\(L_E\)</span>:输出图和ground truth的欧几里德loss ，<span class="math inline">\(L_P\)</span>:perceptual loss, 如果<span class="math inline">\(\lambda_a和\lambda_b\)</span>同时为0，则network变为CNN。若只是<span class="math inline">\(\lambda_p=0\)</span>，则network变为标准 的GAN。</p><p>其中<span class="math inline">\(L_E\)</span>为： <span class="math display">\[L_E=\frac{1}{CWH}\sum_{c=1}^C\sum_{x=1}^W\sum_{y=1}^H \left\| \phi_E(x)^{c,w,h} -(y_b)^{c,w,h}\right\|^2_2\]</span> C:channels,W:width,H:height。x:input image, <span class="math inline">\(y_b\)</span>:corresponding ground truth ,</p><p><span class="math inline">\(\phi_E(x)\)</span>:mapping G <span class="math display">\[L_P=\frac{1}{C_iW_iH_i}\sum_{c=1}^{C_i}\sum_{x=1}^{W_i}\sum_{y=1}^{H_i} \left\| V(\phi_E(x)^{c,w,h}) -V((y_b)^{c,w,h})\right\|^2_2\]</span> V:non-linear CNN transformation. <span class="math display">\[L_A=-\frac{1}{N}\sum_{i=1}^Nlog(D(\phi_E(x)))\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Image De-raining Using a Conditional Generative Adversarial Network&lt;/p&gt;
&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;在C-GAN中执行限制条件使去雨后的图像与相应的真实图像一致。其中的adversarial loss 有助于获取更好结果。此外在G和D中提出新的refined loss function（降低创造能力使图像更加真实）和新的网络结构提升效果。G使用densely connected networks，D的多尺度结构可利用局部和全局信息进行生成图像真伪判断。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Unsupervised Single Image Deraining with Self-supervised Constraints</title>
    <link href="//Rocky1ee.github.io/2020/03/25/Unsupervised%20Single%20Image%20Deraining%20with%20Self-supervised%20Constraints/"/>
    <id>//Rocky1ee.github.io/2020/03/25/Unsupervised Single Image Deraining with Self-supervised Constraints/</id>
    <published>2020-03-25T08:56:36.000Z</published>
    <updated>2020-04-18T07:37:45.290Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>UD-GAN : Unsupervised Deraining Generative Adversarial Network ,基于unpaired rainy 和 clean images的本质特征引入self-supervised constrains。首先网络由Rain Guidance Module（RGM）和Background Guidance Module（BGM）组成。RGM的G判别real rainy images和D生产的fake rainy images。BGM确保rainy input和de-rained output background的一致性。其次提出luminance-adjusting adversarial loss进行real clean image和de-rained images间的亮度调节。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>问题：真实的雨跟合成的雨有差别，所以模型在去除未知real rainy时有所不足。同时难以用一个fully-supervised deraining model 应对所有雨的类型。</p><p>方法：基于unpaired rainy 和 clean image的intrinsic statistics建立self-supervised constraints。RGM通过限制rainy输入和de-rained输出间的不同来间接缩小生成de-rained images的解空间。BGM通过rainy输入和de-rained output间的分级Gaussian-Blur gradient error确保background的一致。引入luminance-adjusting adversarial loss调节de-raind images和real images间的亮度差，以得到更真实的de-rained images。</p><!--more--><h1 id="related-work">Related Work</h1><p>传统方法：基于先验的方法如稀疏编码，低秩表达，GMM，易出现over-smooth或under-smooth问题。</p><p>Self-supervised Learning：利用输入数据的内在特性（如人脸中眼镜是在上方，嘴巴是在下方这样的位置信息）作为label。</p><h1 id="unsupervised-deraining-generative-adversarial-network">Unsupervised Deraining Generative Adversarial Network</h1><p><img src="/Assets/Img/UD-GAN.png"></p><p><span class="math inline">\(G_c\)</span>生成的de-rained images中一些区域性存在visual artifact，但我们希望de-rained images与clean image相同。于是提出self-supervision。</p><h2 id="self-supervision-by-rainy-image">Self-supervision by Rainy Image</h2><p>用RGM模型学习雨的特性作为de-rain任务的self-supervision。首先rain streaks(<span class="math inline">\(\widetilde{S}\)</span>)与real clean images（C)组成的fake rainy images(<span class="math inline">\(\widetilde{S}+C\)</span>)与real rainy images<span class="math inline">\(R\)</span>越相似，de-rained images（<span class="math inline">\(\widetilde{C}\)</span>)越清晰。 <span class="math display">\[\widetilde{s}=r-\widetilde{c},D_s(r,\widetilde{s}+c)?=real/fake\]</span> 只要<span class="math inline">\(G_c\)</span>训练的足够好，任意C与<span class="math inline">\(\widetilde{S}\)</span>组成的fake rainy image都可以糊弄<span class="math inline">\(D_s\)</span>,所以RGM间接指导<span class="math inline">\(G_C\)</span>得到更好的de-rained output。其loss为： <span class="math display">\[\mathcal{L}^R_{guid}(G_c,D_s) = E_{r∼p_{data(r)}}[logD_s(r)] +E_{r,c∼p_{data(r,c)}}[log(1 −D_s((r −G_c(r) + c)))] \quad(3)\]</span></p><p>用BGM模型确保<span class="math inline">\(\widetilde{C}\)</span>与<span class="math inline">\(C\)</span>的结构，细节一致。首先用高斯blur kernel或导向滤波对<span class="math inline">\(R,\widetilde{C}\)</span>分高低频。</p><p>两者低频部分的background features随着高斯核的scales$$的增大而越发相似，同时gradient error也不断减小。于是提出background guidance loss，对rainy input和de-rained output间使用不同 scalers的高斯滤波guidance，并以不同scales上的guidance指导detial的修复。</p><p><img src="/Assets/Img/UD-GAN_BGM_loss.png" style="zoom:50%;"> <span class="math display">\[\mathcal{L}_{guid}^B(G_c)=\sum_{\sigma=3,5,9}\lambda_{\sigma}|∇B_σ(r) −∇B_σ(G_c(r))|\]</span> 其中<span class="math inline">\(\lambda_{\sigma}\)</span>为不同scales对应的权重，文中所取参数为<span class="math inline">\(λ__σ as [0.01, 0.1, 1] for σ = 3, 5, 9,\)</span></p><h2 id="self-supervision-by-clean-image">Self-supervision by Clean Image</h2><p>真实的雨图是在阴雨天拍摄的，所以会比较暗。而real clean image通常背景明亮（晴天拍摄）。同时合成的雨图中的雨线也比较亮，会比较容易残留在de-rained images中。所以以此为目标会导致de-rained iamges亮度过高。</p><p><img src="/Assets/Img/lumunate_loss.png"></p><p>通过提升<span class="math inline">\(C\)</span>的亮度得到数据<span class="math inline">\(E\)</span>,将<span class="math inline">\(C,S\)</span>输入<span class="math inline">\(D_c\)</span>对<span class="math inline">\(G_c\)</span>进行指导，以生成更真实的de-rained images</p><p>亮度调节GAN的loss如下： <span class="math display">\[\mathcal{L}_{lum-adv}(G_c,D_c) = E_{c∼p_{data(c)}}[logD_c(c)] + E_{e∼p_{data(e)}}[log(1 −D_c(e))] + E_{r∼p_{data(r)}}[log(1 −D_c(G_c(r)))]\]</span></p><h2 id="loss-function">Loss Function</h2><p>提出cycle consistency loss ，将de-rained images输入rain-added generator<span class="math inline">\(G_r\)</span>生成雨图re-rainy images，以保持rainy input和de-rained output间风格，颜色的一致性。 <span class="math display">\[\mathcal{L}_{cyc}(G_c, G_r) = E_{r∼p_{data(r)}}[||G_r(G_c(r)) − r||_1] \]</span> UD-GAN的总Loss为： <span class="math display">\[\mathcal{L}_{total} = w_1\mathcal{L}^R_{guid} +w_2\mathcal{L}^B_{guid} +w_3\mathcal{L}_{lum-adv} +w_4\mathcal{L}_{cyc}\]</span> 其中<span class="math inline">\([w_1,w_2,w_3,w_4]=[1,5,1,0.5]\)</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;UD-GAN : Unsupervised Deraining Generative Adversarial Network ,基于unpaired rainy 和 clean images的本质特征引入self-supervised constrains。首先网络由Rain Guidance Module（RGM）和Background Guidance Module（BGM）组成。RGM的G判别real rainy images和D生产的fake rainy images。BGM确保rainy input和de-rained output background的一致性。其次提出luminance-adjusting adversarial loss进行real clean image和de-rained images间的亮度调节。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Lightweight Pyramid Networks for Image Deraining</title>
    <link href="//Rocky1ee.github.io/2020/03/23/Lightweight%20Pyramid%20Networks%20for%20Image%20Deraining/"/>
    <id>//Rocky1ee.github.io/2020/03/23/Lightweight Pyramid Networks for Image Deraining/</id>
    <published>2020-03-23T08:56:36.000Z</published>
    <updated>2020-04-18T07:36:35.056Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>lightweight pyramid of networks (LPNet),使用domain-specific knowledge 简化学习过程。在NN中引入GaussianLaplacian image pyramid 分解技术，同时采用recursive 和 residual构建轻量模型。</p><h1 id="introduction">Introduction</h1><p>首先采用Laplacian pyramids decompose rainy image成不同的level，然后对不同level进行recursive和residual 网络建立Guassian pyramid进行De-raining，不同level采用不同loss，最后的的bottom level得到de-rained image。</p><p><img src="/Assets/Img/LPN_structure.png"></p><a id="more"></a><h1 id="lightweight-pyramid-network-for-deraining">LIGHTWEIGHT PYRAMID NETWORK FOR DERAINING</h1><h2 id="the-laplacian-pyramid">The Laplacian pyramid</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;lightweight pyramid of networks (LPNet),使用domain-specific knowledge 简化学习过程。在NN中引入GaussianLaplacian image pyramid 分解技术，同时采用recursive 和 residual构建轻量模型。&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;首先采用Laplacian pyramids decompose rainy image成不同的level，然后对不同level进行recursive和residual 网络建立Guassian pyramid进行De-raining，不同level采用不同loss，最后的的bottom level得到de-rained image。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/Assets/Img/LPN_structure.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Non-locally Enhanced Encoder-Decoder Network for Single</title>
    <link href="//Rocky1ee.github.io/2020/02/25/Non-locally%20Enhanced%20Encoder-Decoder%20Network%20for%20Single/"/>
    <id>//Rocky1ee.github.io/2020/02/25/Non-locally Enhanced Encoder-Decoder Network for Single/</id>
    <published>2020-02-25T08:56:36.000Z</published>
    <updated>2020-04-18T07:38:23.457Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>NLEDN：Non-locally enhanced encoder-decoder network专注于特征表达和long-range dependencies， 累进学习rain streaks的特征，同时完美保留图像的细节。non-locally enhanced dense blocks充分利用所有Conv layers的分级特征，同时获取long-distance dependencies 和结构信息。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>问题：现有NN receptive filed有限，pixel value的复原只依赖于周边的局部区域。同时容易忽视long-distance spatial context会导致over smooth。</p><p>受到adaptive nonlocal means filter启发，将non-local操作加入NN</p><p>方法：设计对称的encoder和decoder：各由3个non-locally enhanced dense block(NEDB)组成。NEDB用4个densely 连接的conv获取non-local feature map weight，还有一个conv用于residual inference。encoder中引入pooling striding mechanism用于学习increasingly abstract feature representation，同时记录polling的index，在decoder对位置采用upsampling以保留structure和detail。</p><h1 id="related-works">Related works</h1><p>Non-local Networks:</p><h1 id="method">Method</h1><p>non-locally enhanced encoder-decoder network(NLEDN):FC encoder-decoder学习输入输出间的复杂pixel-wise mapping。non-locally enchanced dense block(NEDB):利用rian streak maps中的大量结构细节和free nature 中的self-similarities。</p><p><img src="/Assets/Img/NLEDN.png"></p><h2 id="entrance-and-exit-layers">Entrance and Exit Layers</h2><p>将合成雨图<span class="math inline">\(I_0\)</span>输入Conv<span class="math inline">\(H_0\)</span>得到特征<span class="math inline">\(F_0\)</span>，然后将<span class="math inline">\(F_0\)</span>输入层Conv<span class="math inline">\(H_1\)</span>得到<span class="math inline">\(F_1\)</span>。并对<span class="math inline">\(I_0,F_0\)</span>进行skip-connection。<span class="math inline">\(\hat{Y} = I_0+R\)</span>,final rain-free image:<span class="math inline">\(\hat{Y}\)</span>。</p><h2 id="non-locally-enhanced-encoding-and-decodeing">Non-locally Enhanced Encoding and Decodeing</h2><p><img src="/Assets/Img/NEDB.png"></p><p>NEDB表示为<span class="math inline">\(F_n\)</span>，<span class="math inline">\(f()\)</span>为计算pair-wise relationship的函数，表示如下： <span class="math display">\[f(F_{n,i},F_{n,j}) = \theta(F_{n,i})^T\phi(F_{n,j})\]</span> <span class="math inline">\(F_{n,i}\)</span>:<span class="math inline">\(F_n\)</span> at position i. <span class="math display">\[\begin{equation}\begin{aligned}y_{n,i} &amp;= \frac{1}{C(F)}\sum_{\forall j}f(F_{n,i},F_{n,j})g(F_n,j)\\C(f)&amp;=\sum_{\forall j}f(F_{n,i},F_{n,j})\end{aligned}\end{equation}\]</span> 然后进行densely connect的consecutive conv <span class="math display">\[D_l = H_l([D_0,...,D_{l-1}])\]</span> 为了避免梯度消失采用residual learning <span class="math display">\[F_m = \mathcal{F}(F_{m-1},W_m)+F_{m-1}\\\mathcal{F}(F_{m-1},W_m) = Conv_{1\times1}([D_0,...,D_L])\]</span> 使用Pooling indeces指导decoder进行upsample，在与downsample对称处进行upsample。</p><h2 id="loss-function">Loss Function</h2><p><span class="math display">\[\mathcal{L} = \frac{1}{HWC}\sum_i\sum_j\sum_k||\hat{Y}_{i,j,k}-Y_{i,j,k}||_1\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;NLEDN：Non-locally enhanced encoder-decoder network专注于特征表达和long-range dependencies， 累进学习rain streaks的特征，同时完美保留图像的细节。non-locally enhanced dense blocks充分利用所有Conv layers的分级特征，同时获取long-distance dependencies 和结构信息。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Physics-Based Generative Adversarial Models for Image Restoration and Beyond</title>
    <link href="//Rocky1ee.github.io/2020/02/14/Physics-Based%20Generative%20Adversarial%20Models%20for%20Image%20Restoration%20and%20Beyond/"/>
    <id>//Rocky1ee.github.io/2020/02/14/Physics-Based Generative Adversarial Models for Image Restoration and Beyond/</id>
    <published>2020-02-14T08:56:36.000Z</published>
    <updated>2020-04-18T07:38:10.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>由于GAN具有一定生成能力，原始GAN生成的图像与ground truth有些区域会不一致。所以提出物理model对学习方法进行限制，以此引导GAN适应特定的任务。以此应对各种image restoration问题。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>原始GAN的Loss，使输出和GT不完全一致。为了满足特定的任务，利用物理模型对生成图像进行限制，保持生成图像和ground truth的一致性（physically correct）。</p><h1 id="related-work">Related Work</h1><p>Image deblurring：CNN+基于统计的有效先验信息,多尺度感知方法,基于SVD的CNN(non-blind image deblurring)。CNN评估blur kernel+CNN deblur，end-to-end CNN(blind image deblurring)。</p><p>Image dehazing,Image deraining,Image super-resolution,image filtering,image denoising。</p><h1 id="image-restoration-from-gan">Image Restoration from GAN</h1><p>image restoration问题可以表示为： <span class="math display">\[x^∗ = arg\underset{x}minρ(x, y) + \psi(x)\]</span> <span class="math inline">\(ρ(x, y)\)</span>为恢复图像，<span class="math inline">\(\psi(x)\)</span>为对x的限制。</p><h1 id="proposed-algorithm">Proposed Algorithm</h1><p>使用fundamental constraint 指导GAN的训练，确保结果physically correct。在GAN基础上，提出额外的D</p><p><img src="/Assets/Img/Physically_based_gan.png"></p><p>Physically based GAN 由1个G和2个D组成。为了保持deblurred images和blur images的物理一致性，以图像deblur为例，采用模型: <span class="math display">\[\widetilde{y_i} = k_i ⊗ \mathcal{G}(y_i),\]</span> 表示blur kernel（<span class="math inline">\(k_i\)</span>)和G生成的deblurred image($ (y_i)<span class="math inline">\()进行conv形成regenerated blur image(\)</span><span class="math inline">\()。将\)</span>y_i<span class="math inline">\(和\)</span><span class="math inline">\(输入\)</span>D_h<span class="math inline">\(以确保\)</span> (y_i)$的物理正确。</p><p>将<span class="math inline">\(x_i\)</span>和$ (y_i)<span class="math inline">\(输入\)</span>D_g<span class="math inline">\(以确保\)</span> (y_i)$变清晰。</p><p>在不同的任务中物理模型不同即<span class="math inline">\(\widetilde{y_i}\)</span>的生成方式不同，如dehazing中采用:<span class="math inline">\(\widetilde{y_i} =k_i ⊗ \mathcal{G}(y_i)t_i+A_i(1-t_i)\)</span>。image super-resolution采用<span class="math inline">\(\widetilde{y_i} =\mathcal{F} (\mathcal{G}(y_i))\)</span>。</p><h2 id="network-architecture">Network Architecture</h2><p>G和D的参数如下表，D通常采用Patch-GANs</p><p><img src="/Assets/Img/Physically_GAN_parameters_table.png"></p><h2 id="loss-function">Loss Function</h2><p>为了保证G生成的结果接近GT，并且结果与input满足物理模型，采用了2个pixel-wise loss <span class="math display">\[\mathcal{L}_p = \sum_i||\widetilde{y_i}-y_i||_1\\\mathcal{L}_g = \sum_i||\mathcal{G}(y_i)-x_i||_1\\\]</span> 为了使G训练更加稳定，进一步提出loss <span class="math display">\[\widetilde{\mathcal{L}_g} = \sum_i||\mathcal{G}(\widetilde{y_i})-x_i||_1\]</span> 结合GAN的loss 变为 <span class="math display">\[\mathcal{L}_a = \sum_i[log(D_g(x_i))] + [log(1-D_g(\mathcal{G}(y_i)))] +[log(D_h(y_i))] + [log(1-D_h(\widetilde{y_i}))]\]</span> 最终loss为： <span class="math display">\[(\mathcal{G^*},\mathcal{D_g^*},\mathcal{D_h^*})=\underset{\mathcal{G}}{max}\underset{\mathcal{D}}{min}\frac{1}{N}\begin{Bmatrix}\mathcal{L}_a+\lambda(\mathcal{L}_p,\mathcal{L}_g,\mathcal{\widetilde L}_g)\end{Bmatrix}\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;由于GAN具有一定生成能力，原始GAN生成的图像与ground truth有些区域会不一致。所以提出物理model对学习方法进行限制，以此引导GAN适应特定的任务。以此应对各种image restoration问题。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Rain streak removal for single image via kernel guided cnn</title>
    <link href="//Rocky1ee.github.io/2020/01/22/Rain%20streak%20removal%20for%20single%20image%20via%20kernel%20guided%20cnn/"/>
    <id>//Rocky1ee.github.io/2020/01/22/Rain streak removal for single image via kernel guided cnn/</id>
    <published>2020-01-22T08:56:36.000Z</published>
    <updated>2020-04-18T07:31:42.808Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>KGCNN：kernel guided convolutional neural network利用rain streak的motion blur mechanism对rain streak的干扰进行建模。</p><p>框架首先用一个简单的NN根据图中texture component的角度和长度学习motion blur kernel，然后对motion blur kernel进行Dimensionality stretching 方法作为rainy patch 嵌入到degradation map。最后将degradation map和 texture patch输入ResNet,在motion blur kernel的指引下输出rain streaks。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>问题：大量的深度学习方法难以区分rain streaks和line pattern textures</p><p>方法：将rain streaks 对ground truth的degradation模拟为motion blur，利用rain steaks的反复性和方向性区分rain streaks 和 line pattern textures。这样问题转变为如何从数据中获取mention kernel和deraning时如何使用mention kernel。</p><p>首先将rainy image patch输入简单的6层NN推断motion blur kernel的角度和长度，然后用dimensionality stretching方法stretching motion blur kernel to degradation maps得到detial patchs，之后将detail patch和degradation map一起输入26层的ResNet得到rain streaks，最后用rainy image 减去rain streaks 得到derain results。</p><p><img src="/Assets/Img/KGCNN.png"></p><h1 id="the-kernel-guided-convolutional-neural-network">The Kernel Guided Convolutional Neural Network</h1><h2 id="observation-model">Observation Model</h2><p>一般模型为： <span class="math display">\[O=B+R\quad (1)\]</span> 考虑到motion blur，模型由（1）变为（2） <span class="math display">\[O = B + K(\theta,l)\otimes M \quad(2)\]</span> 其中<span class="math inline">\(\theta\)</span>，<span class="math inline">\(l\)</span>为分别为motion blur kernel（K）的角度和长度，M为raindrops mask，<span class="math inline">\(otimes\)</span>为卷积操作，表示Raindrop 进行motion blur kernel运动变成rain streaks。</p><p>rainy patch decomposed 成texture component(<span class="math inline">\(O_T\)</span>)和structure component(<span class="math inline">\(O_S\)</span>)两个部分，模型： <span class="math display">\[O = O_S+O_T\]</span> 进一步进行稀疏化和减少mapping range，模型由（2）变为（3） <span class="math display">\[O_T = B_T + K(\theta,l)\otimes M \quad(2)\]</span></p><h2 id="the-parameter-sub-network">The Parameter Sub-Network</h2><p>一般NN将输入的texture patch mapping 到 motion blur kernel，其loss如下： <span class="math display">\[L_K(Θ_K) =\frac1n \sum_{i=1}^n||F_K(O^i_T; Θ_K) −K^i||^2_F \quad(4)\]</span> 其中<span class="math inline">\(Θ_K\)</span>表示NN的参数，i为image patches的index</p><p>之所以叫Parameter Sub-Network(<span class="math inline">\(F_P()\)</span>)是因为，网络将输入的 texture patch mapping为只含角度和长度2个参数的向量。只要角度和长度确定motion blur kernel便是唯一确定的。于是将loss（4）变为（5）： <span class="math display">\[L_P(Θ_P) =\frac1n \sum_{i=1}^n||F_P(O^i_T; Θ_P) −P^i||^2_F \quad(5)\\P=[\theta,l]^T\]</span></p><h2 id="dimensionality-stretching">Dimensionality Stretching</h2><p>为了使motion blur kernel 与 texture pitch充分结合，对motion blur kernel进行Dimensionality stretching操作。</p><p>首先将motion blur kernel 向量化得到$  R<sup>{P</sup>2}<span class="math inline">\(,其次通过PCA将\)</span> <span class="math inline">\(project到t维线性空间得到\)</span> <span class="math inline">\(,然后构造成一个m×n×t维的degradation maps，第t个m×n平面的所有pixel值用\)</span> $的第t个值填充，具体如下图：</p><p><img src="/Assets/Img/KGCNN_Stretching.png" style="zoom:33%;"></p><p>最后将degradation maps和texture image 一起作为Derain Net的输入</p><h2 id="derain-net">Derain Net</h2><p>Derain Net采用26层的ResNet，其loss为 <span class="math display">\[L_D(Θ_D) =\frac1 n\sum^n_{i=1}||f_d(O^i_{texture},K^i −R^i||^2_F\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;KGCNN：kernel guided convolutional neural network利用rain streak的motion blur mechanism对rain streak的干扰进行建模。&lt;/p&gt;
&lt;p&gt;框架首先用一个简单的NN根据图中texture component的角度和长度学习motion blur kernel，然后对motion blur kernel进行Dimensionality stretching 方法作为rainy patch 嵌入到degradation map。最后将degradation map和 texture patch输入ResNet,在motion blur kernel的指引下输出rain streaks。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Residual-Guide Network for Single Image Deraining</title>
    <link href="//Rocky1ee.github.io/2020/01/17/Residual-Guide%20Network%20for%20Single%20Image%20Deraining/"/>
    <id>//Rocky1ee.github.io/2020/01/17/Residual-Guide Network for Single Image Deraining/</id>
    <published>2020-01-17T08:56:36.000Z</published>
    <updated>2020-04-18T07:31:22.901Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>ResGuideNet：Residual-guide feature fusion network，采用级联式的网络，用浅层blocks生成的residuals引导深层blocks。随着blocks的传播得到更好的negative residual，将不同blocks的outputs合并得到最终结果（类似集成学习）。同时采用recursive+shortcut构建每个block，监督所有中间outputs。</p><h1 id="introduction">Introduction</h1><p>问题：</p><p>1.欧氏距离衡量的de-rain image 与相应 ground-truth image间的per-pixel loss 与视觉感知有差别</p><p>2.难以找到适合所有rain streak removal tasks的统一模型</p><p>提出ResGuideNet：</p><p>其中每个block含都有global shortcut用于预测residual，不断的从浅层到深层的将predicted residuals合并用于之后block的预测。并监督所有中间outputs，随着blocks go deeper获得coarse-to-fine residual。</p><p>同时采用recursive+shortcut构建每个block，以防止梯度消失和减少参数数量。</p><a id="more"></a><h1 id="method">METHOD</h1><p><img src="/Assets/Img/ResGuideNet.png"></p><h2 id="motivation">motivation</h2><p>现存模型基于negative residual或大量参数的固定模型，容易over-smoothed。同时不够灵活和太笨重，难以应用。ResGuideNet思想：少blocks对应light rainy，多blocks对应heavy rainy。</p><p><img src="/Assets/Img/ResGuideNet_blocks.png"></p><h2 id="residual-feature-reuse">Residual Feature Reuse</h2><p>使用residual mapping，从浅层block到深层block concatenate feature maps以解决梯度消失和模型参数量。</p><p>在每个block中使用一个long shortcut实现global residual learning，block由若干Leaky Rectified Linear Units构成（Baseline），逐步合并前层信息（residual reuse）输入后层（Baseline-RR）</p><h2 id="loss-function">Loss Function</h2><p>l2 loss 不足以分清object edges 和 rain streaks ，于是用L2 loss + SSIM loss： <span class="math display">\[\begin{equation}\begin{aligned}L_{MSE_k} &amp;= \frac{1}{N} \sum_{i=1}^N(||f_k(X_i,W,b)-Y||^2_2\\L_{SSIM_k} &amp;= log(1.0/g(f_k(X_i,W,b),Y)+1e^{-4}) \\L_{B_k} &amp;= L_{MSE_k} + \lambda*L_{SSIM_k}\end{aligned}\end{equation}\]</span> 其中，N：training rainy patches的数量，k：block的编号，X：rainy patches，<span class="math inline">\(X^i\)</span>：第i个block的输入，W，b：weight，bias，<span class="math inline">\(f\)</span>: mapping of each block, <span class="math inline">\(g\)</span>: SSIM 函数。</p><p>ResGuideNet的综合loss： <span class="math display">\[\begin{equation}\begin{aligned}L &amp;= \frac{1}{M+1}(\sum_{k=1}^ML_{B_k}+L_{Merge})\\L_{Merge} &amp;= ||f(X_{Merge},W,b)-Y||^2_2\end{aligned}\end{equation}\]</span> 其中，<span class="math inline">\(X_Merge\)</span>:每个block outputs的合并</p><h2 id="recursive-computation">Recursive Computation</h2><p>每个block中采用nonlinear mapping进行recursive strategy。</p><p>每个recursive unit中有2个conv，操作如下 <span class="math display">\[x^t = g(x^{t-1}),x^{t+1} = g(x^{t}) \qquad(3)\]</span> 为了防止梯度消失，将第一个用Conv + LReLU获取特征多次送入后面的recursive unit</p><p><img src="/Assets/Img/ResGuideNet_blocks_Recursive_unit.png"></p><p>公式（3）变为（4）： <span class="math display">\[x^t = g(x^{t-1})+x^0,x^{t+1} = g(x^{t})+x^0 \qquad(3)\]</span></p><h2 id="inter-block-ensemble">Inter-block Ensemble</h2><p>通过 1x1的conv将所有block的结果合并，类似于ensemble</p><p><img src="/Assets/Img/ResGuideNet_blocks_Merge.png"></p><p>公式表示如下： <span class="math display">\[\begin{equation}\begin{aligned}F_1(X) &amp;=\hat{r_1},\hat{y_1}=X+\hat{r_1}\\F_2(\hat{y_1};\hat{r_1}) &amp;=\hat{r_2},\hat{y_2}=X+\hat{r_2}\\F_2(\hat{y_2};\hat{r_2};\hat{r_1}) &amp;=\hat{r_3},\hat{y_3}=X+\hat{r_3}\\……\\F_2(\hat{y_4};\hat{r_4};\hat{r_3};\hat{r_2};\hat{r_1}) &amp;=\hat{r_5},\hat{y_5}=X+\hat{r_5}\end{aligned}\end{equation}\]</span> <img src="/Assets/Img/ResGuideNet_structure.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;ResGuideNet：Residual-guide feature fusion network，采用级联式的网络，用浅层blocks生成的residuals引导深层blocks。随着blocks的传播得到更好的negative residual，将不同blocks的outputs合并得到最终结果（类似集成学习）。同时采用recursive+shortcut构建每个block，监督所有中间outputs。&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;问题：&lt;/p&gt;
&lt;p&gt;1.欧氏距离衡量的de-rain image 与相应 ground-truth image间的per-pixel loss 与视觉感知有差别&lt;/p&gt;
&lt;p&gt;2.难以找到适合所有rain streak removal tasks的统一模型&lt;/p&gt;
&lt;p&gt;提出ResGuideNet：&lt;/p&gt;
&lt;p&gt;其中每个block含都有global shortcut用于预测residual，不断的从浅层到深层的将predicted residuals合并用于之后block的预测。并监督所有中间outputs，随着blocks go deeper获得coarse-to-fine residual。&lt;/p&gt;
&lt;p&gt;同时采用recursive+shortcut构建每个block，以防止梯度消失和减少参数数量。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Fast Single Image Rain Removal via a Deep Decomposition-Composition Network</title>
    <link href="//Rocky1ee.github.io/2019/12/31/Fast%20Single%20Image%20Rain%20Removal%20via%20a%20Deep%20Decomposition-Composition%20Network/"/>
    <id>//Rocky1ee.github.io/2019/12/31/Fast Single Image Rain Removal via a Deep Decomposition-Composition Network/</id>
    <published>2019-12-31T08:56:36.000Z</published>
    <updated>2020-04-18T07:38:59.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>提出deep decomposition-recomposition network（DDC-Net） ，由decomposition和composition组成。其中decomposition用于分离rain images成为clean background和rain layers。decomposition的部分A代表clean image，部分B代表rain layer。composition用clean background 和 rain layers重建input rain images。其目标是进一步提升decomposition的效果。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>问题2个：如何有效率的提高准确性和实时应用性？</p><p>贡献：</p><p>DDC-Net：训练时需要D和C两部分，而测试时只需要D部分。</p><p>pre-trained model采用了unpaired 微调，从而对real case有更好的适应新。</p><p>不仅考虑clean background的恢复精度，也考虑了rain layers的恢复精度。</p><h1 id="deep-decomposition-composition-network">DEEP DECOMPOSITION-COMPOSITION NETWORK</h1><p><img src="/Assets/Img/DCCNet.png"></p><h2 id="decomposition-net">Decomposition Net</h2><p>1.enconder的前两层卷积采用dilated convolution。</p><p>2.decoder部分由2个branch组成，clean background branch（恢复clean background）和auxiliary rain branch（恢复rain layer）。</p><p>3.将上述2branch的获取的特征结合起来再进行conv+BN+ReLU以获取更好的rain information。</p><h3 id="pre-train-on-synthetic-images">Pre-train on synthetic images</h3><p>将clean images和相应的synthesized rainy images作为输入和label，使用欧氏距离作为loss function <span class="math display">\[\mathfrak{L}_B = \frac{1}{N} \sum_{i=1}^N||f_b(O^i)-B^i||^2_F \quad (1)\]</span></p><p><span class="math display">\[\mathfrak{L}_R = \frac{1}{N} \sum_{i=1}^N||f_r(O^i)-R^i||^2_F \quad (1)\]</span></p><p>其中<span class="math inline">\(O^i\)</span>为synthesized rainy images，<span class="math inline">\(B^i\)</span>为 clean images，<span class="math inline">\(R^i\)</span>为rain part,怎么产生的？？，i表示bunch size N 中的第i张图片。</p><h3 id="fine-tune-on-real-images">Fine-tune on real images</h3><p>这部分没怎么看懂</p><h2 id="composition-net">Composition Net</h2><p>将 clean background 输出的 rain-free image <span class="math inline">\(B\)</span> 与 auxiliary rain branch 输出的 rain layer <span class="math inline">\(R\)</span>结合，输入 Composition Net，其loss function 为： <span class="math display">\[\mathfrak{L}_O = \frac{1}{N} \sum_{i=1}^N||f(B^i+R^i)-O^i||^2_F \quad (1)\]</span></p><h2 id="training-dataset">Training Dataset</h2><p>为了使synthesized rainy images 更加真实，而采用screen blending的方式： <span class="math display">\[O = 1- (1-B)\circ(1-R)=B+R-B\circ R\]</span> 其中<span class="math inline">\(O\)</span>为合成雨图，<span class="math inline">\(B\)</span>为back ground，<span class="math inline">\(R\)</span>为rain layer。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;提出deep decomposition-recomposition network（DDC-Net） ，由decomposition和composition组成。其中decomposition用于分离rain images成为clean background和rain layers。decomposition的部分A代表clean image，部分B代表rain layer。composition用clean background 和 rain layers重建input rain images。其目标是进一步提升decomposition的效果。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>深度学习的优化方法</title>
    <link href="//Rocky1ee.github.io/2019/12/11/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>//Rocky1ee.github.io/2019/12/11/优化方法总结/</id>
    <published>2019-12-11T08:56:36.000Z</published>
    <updated>2020-04-18T07:36:53.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="学习优化方法的基本框架">学习优化方法的基本框架</h1><p>首先定义：待优化参数：<span class="math inline">\(w\)</span>,目标函数：<span class="math inline">\(f(w)\)</span>,学习率<span class="math inline">\(\alpha\)</span>。</p><p>然后进行迭代优化，每个epoch <span class="math inline">\(t\)</span> 的操作如下:</p><p>1.目标函数与当前参数的梯度：<span class="math inline">\(g_t=\nabla f(w_t)\)</span></p><p>2.由历史梯度按特定方法计算得出的一阶动量：<span class="math inline">\(m_t = \phi(g_1, g_2, \cdots, g_t)\)</span>和二阶动量:<span class="math inline">\(V_t = \psi(g_1, g_2, \cdots, g_t)\)</span></p><p>3.计算当前参数的下降梯度：<span class="math inline">\(\eta_t = \alpha \cdot \frac{m_t }{ \sqrt{V_t}}\)</span></p><p>4.对当前参数进行更新：<span class="math inline">\(w_{t+1} = w_t - \eta_t\)</span></p><p>关键在于第2步。第二步的变化，产生了不同的算法</p><a id="more"></a><h1 id="sgd">SGD</h1><p>Stochastic Gradient Decent：值考虑当前梯度，不考虑历史梯度，没有动量设置： <span class="math display">\[w_{t+1} = w_t - \eta_t = w_t - \alpha \cdot  g_t\]</span> 特点是收敛速度慢，容易进入局部最优可能会出现震荡，需要精心调参，适用性强。</p><h1 id="sgd-with-momentum">SGD with Momentum</h1><p>为了抑制震荡和加速收敛，从而加入对历史梯度的考虑，引入一阶动量： <span class="math display">\[m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t\]</span> 一阶动量是各个时刻梯度方向的<strong>指数移动平均值</strong>，通俗讲就是t时刻的下降方向，由历史梯度累积和当前梯度加权决定。<span class="math inline">\(\beta_1\)</span>越大，历史梯度决定性越强。 <span class="math display">\[w_{t+1} = w_t - \eta_t = w_t - \alpha \cdot  m_t=w_t-(\beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t)\]</span></p><h1 id="sgd-with-nesterov-acceleration">SGD with Nesterov Acceleration</h1><p>NAG中引入了一种试探步的想法，即计算当前位置按一阶动量走一步后位置的梯度。<span class="math inline">\(g_t\)</span>不是t时刻的参数<span class="math inline">\(w_t\)</span>的梯度，而是t时刻参数<span class="math inline">\(w_t\)</span>按动量一更新一次后的参数的梯度： <span class="math display">\[g_t=\nabla f(w_t-\alpha \cdot \frac{m_{t-1} } {\sqrt{V_{t-1}}})\]</span></p><p><span class="math display">\[w_{t+1} = w_t - \eta_t = w_t - \alpha \cdot  m_t=w_t-(\beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot \nabla f(w_t-\alpha \cdot \frac{m_{t-1} } {\sqrt{V_{t-1}}}))\]</span></p><h1 id="adagrad">AdaGrad</h1><p>引入二阶动量来衡量参数的更新频率，以此来调整学习率。对于经常更新的参数（对应梯度的绝对值大，单个样本依赖强）学习率小一点，而偶尔更新的参数（对应梯度的绝对值小，单个样本依赖弱）学习率大一点，以此实现自适应学习率，对样本进行均衡化的依赖。</p><p>二阶动量为：参数对应维度上，所有梯度值的平方和： <span class="math display">\[V_t = \sum_{\tau=1}^{t} g_\tau^2\]</span></p><p><span class="math display">\[w_{t+1} = w_t - \eta_t = w_t - \frac{\alpha}{\sqrt{V_{t}}} \cdot  m_t\]</span></p><p>用上式可知学习率实际上是<span class="math inline">\(\frac{\alpha}{\sqrt{V_{t}}}\)</span>。为了避免分母为0，分母会加上一个小的平滑项。<strong>参数更新越频繁，二阶动量越大，学习率就越小</strong>。同时分母是单调递增的，会使的学习率单调递减至0，导致训练提前结束，训练不够充分。</p><h1 id="adadelta-rmsprop">AdaDelta / RMSProp</h1><p>AdaGrad二阶动量的计算过于激进，于是改变为：只关注过去一段时间窗口的下降梯度，而不是累积全部历史梯度，从而避免训练过早结束。改用指数平均的方式计算： <span class="math display">\[V_t = \beta_2 * V_{t-1} + (1-\beta_2) g_t^2\]</span></p><p><span class="math display">\[w_{t+1} = w_t - \eta_t = w_t - \frac{\alpha}{\sqrt{V_{t}}} \cdot  m_t\]</span></p><h1 id="adam">Adam</h1><p><strong>把一阶动量和二阶动量都用起来，就是 Adam了——Adaptive + Momentum</strong> <span class="math display">\[\begin{equation}\begin{aligned}g_t &amp;= \nabla f(w_t) \\m_1 &amp;= g_1,V_1 = g_1^2 \\m_t &amp;= \beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t \\V_t &amp;= \beta_2 \cdot V_{t-1} + (1-\beta_2) \cdot g_t^2\\w_{t+1} &amp;= w_t - \eta_t = w_t - \frac{\alpha}{\sqrt{V_{t}}} \cdot  m_t\end{aligned}\end{equation}\]</span></p><h1 id="nadam">Nadam</h1><p>Nadam是Nesterov和Adam的结合， Nadam=Nesterov + Adam <span class="math display">\[\begin{equation}\begin{aligned}g_t &amp;= \nabla f(w_t + \cfrac{\alpha}{\sqrt{V_{t-1}}} \cdot m_{t-1})  \\m_1 &amp;= g_1,V_1 = g_1^2 \\m_t &amp;= \beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t \\V_t &amp;= \beta_2 \cdot V_{t-1} + (1-\beta_2) \cdot g_t^2\\w_{t+1} &amp;= w_t - \eta_t = w_t - \frac{\alpha}{\sqrt{V_{t}}} \cdot  m_t\end{aligned}\end{equation}\]</span></p><h1 id="优化算法的常用tricks">优化算法的常用tricks</h1><p>最后，分享一些在优化算法的选择和使用方面的一些tricks。</p><ol type="1"><li><strong>首先，各大算法孰优孰劣并无定论</strong>。如果是刚入门，<strong>优先考虑 SGD+Nesterov Momentum 或者 Adam</strong>.（<a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">Standford 231n</a> : The two recommended updates to use are either SGD+Nesterov Momentum or Adam）</li><li><strong>选择你熟悉的算法</strong>——这样你可以更加熟练地利用你的经验进行调参</li><li><strong>充分了解你的数据</strong>——如果数据是非常稀疏的，那么优先考虑自适应学习率的算法。</li><li><strong>根据你的需求来选择</strong>——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。</li><li><strong>先用小数据集进行实验</strong>。论文 <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf" target="_blank" rel="noopener">Stochastic Gradient Descent Tricks</a> 指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。</li><li><strong>考虑不同算法的组合</strong>。先用Adam进行快速下降，而后再换到 SGD 进行充分的调优。切换策略可以参考本文介绍的方法。</li><li><strong>数据集一定要充分的打散（shuffle）</strong>。这样在使用自适应学习率算法的时候，可以<strong>避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题</strong>。</li><li>训练过程中<strong>持续监控训练数据和验证数据</strong>上的目标函数值以及精度或者 AUC 等指标的变化情况。<strong>对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合</strong>。</li><li><strong>制定一个合适的学习率衰减策略</strong>。可以使用定期衰减策略，比如每过多少个 epoch 就衰减一次；或者利用精度或者 AUC 等性能指标来监控，<strong>当测试集上的指标不变或者下跌时，就降低学习率。</strong></li></ol><h1 id="reference">reference</h1><p>https://zhuanlan.zhihu.com/p/32230623</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;学习优化方法的基本框架&quot;&gt;学习优化方法的基本框架&lt;/h1&gt;
&lt;p&gt;首先定义：待优化参数：&lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt;,目标函数：&lt;span class=&quot;math inline&quot;&gt;\(f(w)\)&lt;/span&gt;,学习率&lt;span class=&quot;math inline&quot;&gt;\(\alpha\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;然后进行迭代优化，每个epoch &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt; 的操作如下:&lt;/p&gt;
&lt;p&gt;1.目标函数与当前参数的梯度：&lt;span class=&quot;math inline&quot;&gt;\(g_t=\nabla f(w_t)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;2.由历史梯度按特定方法计算得出的一阶动量：&lt;span class=&quot;math inline&quot;&gt;\(m_t = \phi(g_1, g_2, \cdots, g_t)\)&lt;/span&gt;和二阶动量:&lt;span class=&quot;math inline&quot;&gt;\(V_t = \psi(g_1, g_2, \cdots, g_t)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;3.计算当前参数的下降梯度：&lt;span class=&quot;math inline&quot;&gt;\(\eta_t = \alpha \cdot \frac{m_t }{ \sqrt{V_t}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;4.对当前参数进行更新：&lt;span class=&quot;math inline&quot;&gt;\(w_{t+1} = w_t - \eta_t\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;关键在于第2步。第二步的变化，产生了不同的算法&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Review of Classes Neural Networks</title>
    <link href="//Rocky1ee.github.io/2019/12/02/Review%20of%20Classes%20Neural%20Networks/"/>
    <id>//Rocky1ee.github.io/2019/12/02/Review of Classes Neural Networks/</id>
    <published>2019-12-02T08:56:36.000Z</published>
    <updated>2020-04-18T07:40:05.759Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lenet5">LeNet5</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\LeNet_5.jpeg"></p><a id="more"></a><h1 id="alexnet">AlexNet</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\AlexNet.jpg"></p><h1 id="vgg">VGG</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\VGG.png"></p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\VGG%20(1).PNG"></p><h1 id="google-net">GoogLe-Net</h1><h2 id="inception-v1">Inception-v1</h2><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Googlenet.png"></p><h2 id="inception-v2">Inception-v2</h2><p>在v1的基础上加入Batch Normalization，同时将5x5的卷积替换成2个3x3的卷积，增加网络的深度和减少参数。</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Googlenet_V2.png"></p><h2 id="inception-v3">Inception-v3</h2><p>核心思想是将卷积核分解成更小的卷积，如将7×7分解成1×7和7×1两个卷积核（Module B）。</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Inception-v3.png"></p><h3 id="inception-module-a">Inception Module A</h3><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Inception%20Module%20A.png"></p><h3 id="inception-module-b">Inception Module B</h3><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Inception%20Module%20B.png"></p><h3 id="inception-module-c">Inception Module C</h3><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Inception%20Module%20C.png"></p><h3 id="auxiliary-classifiers">Auxiliary Classifiers</h3><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Auxiliary%20Classifiers%20.png"></p><h3 id="efficient-grid-size-reduction">Efficient Grid Size Reduction</h3><figure><img src="E:\Rocky1ee.github.io\source\Assets\Img\Efficient%20Grid%20Size%20Reduction.png" alt><figcaption><strong>Conventional downsizing (Top Left), Efficient Grid Size Reduction (Bottom Left), Detailed Architecture of Efficient Grid Size Reduction (Right)</strong></figcaption></figure><h2 id="inception-v4">Inception V4</h2><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Inception-v4.png"></p><h2 id="inception-resnet-v1">Inception-ResNet-v1</h2><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Inception-ResNet-v1.png"></p><h2 id="inception-resnet-v2">Inception-ResNet-v2</h2><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Inception-ResNet-v2.png"></p><h1 id="inception-x">Inception-X</h1><p>正常的Convolution：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Convolution.png"></p><p>Depthwise separable Convolution：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Depthwise%20separable%20convolution.png"></p><p>结果是一样的，计算量对比：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\DSP%20conv%20campare%20conv.png"></p><p>Inception-X的操作与DSP相反，先进行1x1的pointwise conv，再进行Depthwise conv:</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Xception-building-block.ppm.png"></p><p>Inception-X的整体结构如下：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Xception_structure.png"></p><h1 id="mobilenet">MobileNet</h1><h2 id="mobilenet-v1">MobileNet-v1</h2><p>采用Depthwise separable Convolution：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\DSP.png"></p><p>MobileNet-v1采用的结构以及其中的BN+ReLU：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\MobileNet_structure.png"></p><h2 id="mobilenet-v2">MobileNet-V2</h2><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\MobileNet_V2.png" style="zoom:50%;"></p><p>我们将上图MobileNetV2左侧的结构称为bottleneck Residual layer简称bottleneck（bottleneck + residual），其结构如下：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Bottleneck_residual.png" style="zoom:60%;"></p><p>V2的具体结构如下：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\MobileNetV2 Overall Architecture.png" style="zoom:67%;"></p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\MobileNetV2 detail.png" style="zoom:50%;"></p><p>t：bottleneck使用t个1x1xk个卷积核对input进行卷积。c：bottleneck 输出feature map的channel大小。</p><p>n：bottleneck重复操作次数。s：stride</p><h2 id="mobilenet-v3">MobileNet-V3</h2><p>MobileNet-v3属于自动优化生成的模型，采用NAS策略构建网络的模块，用NetAdapt机制调整网络的层级形成主体框架，并加上手动调整的last stage。然后在bottleneck中引入注意力机制的SE模块和h-swish激活函数。</p><p>Last stage的结构如下：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\MobileNet-v3_last_stage.jpeg"></p><p>bottleneck + SE</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\M3_SE.jpg" style="zoom:80%;"></p><figure><img src="E:\Rocky1ee.github.io\source\Assets\Img\SE-pipeline.jpg" alt><figcaption>SE—Net</figcaption></figure><p>h-swish激活函数： <span class="math display">\[\text{h-swish}(x) = x \cdot \frac{ReLU6(x+3)}{6}\]</span> <img src="E:\Rocky1ee.github.io\source\Assets\Img\h_swish.png"></p><p>MV3的整体结构如下，左边为large版本，右边为small版本：</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\Mobile-V3_structure.jpeg" style="zoom:80%;"></p><h1 id="resnet">ResNet</h1><p>用non-linear的stacked NN使F(x)=x,要比residual中是F(x)=0要难。</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\ResNet.png" style="zoom:80%;"></p><p>比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。</p><p>Residual block的2种结构，左边为Basic block，右边为Bottleneck。</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\basic_botten.png"></p><h1 id="resnext">ResNeXt</h1><p>基于ResNet和Inception的split+transform+concate结合。但效果却比ResNet、Inception、Inception-ResNet效果都要好。可以使用group convolution。</p><p>一般来说增加网络表达能力的途径有三种：1.增加网络深度，如从AlexNet到ResNet，但是实验结果表明由网络深度带来的提升越来越小；2.增加网络模块的宽度，但是宽度的增加必然带来指数级的参数规模提升，也非主流CNN设计；3.改善CNN网络结构设计，如Inception系列和ResNeXt等。且实验发现增加Cardinatity即一个block中所具有的相同分支的数目可以更好的提升模型表达能力。</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\ResNeXt.png" style="zoom:50%;"></p><h1 id="densenet">DenseNet</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\DenseNet_silde.png"></p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\DenseNet_structure.png"></p><h1 id="senet">SENet</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\SE-pipeline.jpg"></p><h1 id="sknet">SKNet</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\SKNet.png"></p><h1 id="references">References</h1><p>https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc</p><p>MobileNet V2:https://www.zybuluo.com/Team/note/1350161</p><p>MobileNet V3：https://www.cnblogs.com/monologuesmw/p/12272243.html</p><p>Neural Architecture Search：https://blog.csdn.net/jinzhuojun/article/details/84698471</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;lenet5&quot;&gt;LeNet5&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;E:\Rocky1ee.github.io\source\Assets\Img\LeNet_5.jpeg&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Structure" scheme="//Rocky1ee.github.io/tags/Structure/"/>
    
  </entry>
  
  <entry>
    <title>Joint rain detection and removal from a single image with contextualized deep networks</title>
    <link href="//Rocky1ee.github.io/2019/11/30/Joint%20rain%20detection%20and%20removal%20from%20a%20single%20image%20with%20contextualized%20deep%20networks/"/>
    <id>//Rocky1ee.github.io/2019/11/30/Joint rain detection and removal from a single image with contextualized deep networks/</id>
    <published>2019-11-30T08:56:36.000Z</published>
    <updated>2020-04-18T07:39:41.108Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>模型采用multi-task DNN结构学习：binary rain-streak map，rain streak layers和clean background。使用binary rain map更细致标识rain-streaks。引入contextual dilated network利用局部contextual信息，生成对于rain steaks具有不变性的特征。针对overlapping rain streaks，采取recurrent process策略逐步去除。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>首先，加入 rain streak binary map （1：对应pixel被雨覆盖，0：对应pixel 未被雨覆盖）。其次，NN自动检测rain streak 区域，为rain removal 提供有效信息。再次，用contextual dilated network扩大receptive filed和获取context 信息。最后采用recurrent 进行rain 检测和removal。</p><p>较JORDER的变化：修复图像亮度，采用residual task 学习，contextual dilated network 采用coarse-to-fine multi-scale loss。通常用<span class="math inline">\(L_1\)</span>loss 替代MSE loss。</p><h1 id="region-dependent-rain-image-model">REGION-DEPENDENT RAIN IMAGE MODEL</h1><p>一般模型 <span class="math display">\[\mathbf{O} = \mathbf{B}+\mathbf{\widetilde{S}} \quad(1)\]</span> O:有rain streaks的图像，B:clean background，<span class="math inline">\(\widetilde_{s}\)</span>:rain streaks layer.</p><p>加入区域的indicate map，式（1）变为（2）： <span class="math display">\[\mathbf{O} = \mathbf{B}+ \mathbf{S} ◦ \mathbf{R}\quad(2)\]</span> R为indicate map，对应pixel取1代表rain regions，取0代表non-rain regions。</p><p>考虑到雨的聚集以及不同方向雨的叠加，式（2）变为（3）： <span class="math display">\[\mathbf{O} = α(\mathbf{B}+ \sum_{t_r=1}^s \mathbf{\widetilde{S}}_{t_r} ◦ \mathbf{R})+ (1 − α)\mathbf{A}\quad(3)\]</span> s种不同类型rain layer，（为什么s种不同rain layer用一个indicate map？），A：全局环境光，α越大环境光转换越小。</p><h1 id="joint-rain-streak-detection-and-removal">JOINT RAIN STREAK DETECTION AND REMOVAL</h1><h2 id="multi-task-networks-for-jorder">Multi-Task Networks for JORDER</h2><p>为了评估B,S,R采用MAP： <span class="math display">\[arg \underset{B,S,R}{min}||O−B−S ◦ R||_2^2 +P_b(B)+P_s(S)+P_r(R), (4)\]</span> 采取递进方式应用估计结果进行下一步的估计。逐步评估B，S，R（分别表示rain streak detection，estimation 和removal）</p><p><img src="/Assets/Img/JORDAR.png"></p><h2 id="contextualized-dilated-networks">Contextualized Dilated Networks</h2><p>CDN使用多分支结构，采用recurrent和dilated conv策略。</p><p><img src="/Assets/Img/JORDER_CDN.png" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;模型采用multi-task DNN结构学习：binary rain-streak map，rain streak layers和clean background。使用binary rain map更细致标识rain-streaks。引入contextual dilated network利用局部contextual信息，生成对于rain steaks具有不变性的特征。针对overlapping rain streaks，采取recurrent process策略逐步去除。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Progressive Image Deraining Networks A Better and Simpler Baseline</title>
    <link href="//Rocky1ee.github.io/2019/11/30/Progressive%20Image%20Deraining%20Networks%20A%20Better%20and%20Simpler%20Baseline/"/>
    <id>//Rocky1ee.github.io/2019/11/30/Progressive Image Deraining Networks A Better and Simpler Baseline/</id>
    <published>2019-11-30T08:56:36.000Z</published>
    <updated>2020-04-18T07:10:45.644Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>提出一个baseline deraining network，progressive ResNet（PRN）进行recursive 计算。提出Progressive Recurrent network（PReNet）包含recurrent layer发掘各stages 中deep feature间的关联。同时在PRN和PReNet中采用intra-stage recursieve computation的ResNet。将stage-wise result和原始的雨图输入每个ResNet最后得到residual image。</p><h1 id="introduction">Introduction</h1><p>随着de-rainy模型的复杂化，难以评估各种模型及其组合。提出一种简单，超过常见de-rainy模型以及易于改装的baseline network。</p><p>结构：首先是basic shallow ResNet 包含 5个 residual blocks，然后有同等参数量recursively unfold的ResNet组成的PRN，最后是多个具有recurrent layer的PReNet。</p><p>输入：stage-wise result和 original rainy image。输出：rain streak layer or clean background</p><p>Loss Function：single negative SSIM or MSE loss.</p><a id="more"></a><h1 id="progressive-image-deraining-networks">Progressive Image Deraining Networks</h1><p><img src="/Assets/Img/PRN_PReNet.png"></p><h2 id="progressive-networks">Progressive Networks</h2><p>PRN采用multiple stages和inter-stage recursive计算策略，使multiple stages使用共同的参数,减少训练量。</p><p>单一stage进行的操作如下： <span class="math display">\[\mathrm{x}^{t−0.5} = f_{in}(\mathrm{x}^{t−1}, \mathrm{y}), \\ \mathrm{x}_t = f_{out}(f_{res}(\mathrm{x}^{t−0.5}))\]</span> 在PRN中引入recurren network变成PReNet，单一stage进行的操作如下： <span class="math display">\[\mathrm{x}^{t−0.5} = f_{in}(\mathrm{x}^{t−1}, \mathrm{y}), \\ s^t = f_{recurrent}(s^{t−1}, \mathrm{x}^{t−0.5}) \\\mathrm{x}_t = f_{out}(f_{res}(s^t))\]</span> 其中<span class="math inline">\(s^t\)</span>为stage t的recurrent state。<span class="math inline">\(f_{recurrent}\)</span>可以是LSTM或GRU。</p><h2 id="network-architectures">Network Architectures</h2><p><img src="/Assets/Img/PRN_PReNet_ResBlock_structure.png" style="zoom:50%;"></p><p><span class="math inline">\(f_recurrent\)</span>可以采用两种结构，实线框的ResBlocks自身recursive一次，而虚线框的ResBlocks自身recursive五次。</p><p><img src="/Assets/Img/PRN_PReNet_ResBlock.png"></p><h2 id="learning-objective">Learning Objective</h2><p>PRN或PReNet都采用MSE loss 或 negative SSIM loss。model 共T个stages，第i个stage输出为<span class="math inline">\(\mathrm x^i\)</span>，<span class="math inline">\(\mathrm x^T\)</span>为最后的输出。</p><p>MSE Loss: <span class="math display">\[\mathcal{L}=||\mathrm x^T-\mathrm x^{gt}||^2\]</span> SSIM Loss： <span class="math display">\[\mathcal{L}=-SSIM(\mathrm x^T-\mathrm x^{gt})\]</span> 同时也可以采用多stage loss，其对最终效果并没有什么提升，但在early stages可以产生较好的视觉效果。 <span class="math display">\[\mathcal{L} = -\sum_{t=1}^T\lambda_tSSIM(\mathrm x^t,\mathrm x^{gt})\]</span></p><h1 id="experimental-results">Experimental Results</h1><p>patch size： 100 x 100，batch size：18，Aada 初始lr=0.001，训练100 epochs，decayed lr：30,50,80，每次乘20%。</p><h2 id="loss-functions">Loss Functions</h2><p>Negative SSIM比MSE更好，资源充足采用single loss in final stage。</p><h2 id="network-architecture">Network Architecture</h2><p>PReNet效果最好，各种变体的区别在：Concat（x = torch.cat((input, x), 1)），Intra-stage Recursion(单一conv 循环，减少参数)，Recurrent Layer（LSTM，GRU），residual（x = input + x）</p><table><thead><tr class="header"><th>PRN + LSTM + residual = PReNet</th></tr></thead><tbody><tr class="odd"><td>PRN + LSTM = PReNet-LSTM</td></tr><tr class="even"><td>PRN + GRU = PReNet-LSTM</td></tr><tr class="odd"><td>采用intra-stage Recursion = PReNet_r,PNet_r</td></tr><tr class="even"><td>PReNet-residual-concat = PReNet_x</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;提出一个baseline deraining network，progressive ResNet（PRN）进行recursive 计算。提出Progressive Recurrent network（PReNet）包含recurrent layer发掘各stages 中deep feature间的关联。同时在PRN和PReNet中采用intra-stage recursieve computation的ResNet。将stage-wise result和原始的雨图输入每个ResNet最后得到residual image。&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;随着de-rainy模型的复杂化，难以评估各种模型及其组合。提出一种简单，超过常见de-rainy模型以及易于改装的baseline network。&lt;/p&gt;
&lt;p&gt;结构：首先是basic shallow ResNet 包含 5个 residual blocks，然后有同等参数量recursively unfold的ResNet组成的PRN，最后是多个具有recurrent layer的PReNet。&lt;/p&gt;
&lt;p&gt;输入：stage-wise result和 original rainy image。输出：rain streak layer or clean background&lt;/p&gt;
&lt;p&gt;Loss Function：single negative SSIM or MSE loss.&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Squeeze-and-Excitation Networks</title>
    <link href="//Rocky1ee.github.io/2019/11/30/Squeeze-and-Excitation%20Networks/"/>
    <id>//Rocky1ee.github.io/2019/11/30/Squeeze-and-Excitation Networks/</id>
    <published>2019-11-30T08:56:36.000Z</published>
    <updated>2020-04-18T07:39:52.018Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>探索channel间的关系提出新的结构单元：Squeezeand-Excitation” (SE) block。通过channel间的关联，调整对每个channel的feature进行调整。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>feature recalibration：使用全局信息进行重要特征的强化和非重要特征的抑制。结构上是使用一个支路去学习如何评估通道间的关联，然后作用到原feature map上去，实现对输入的校准。</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\SE-pipeline.jpg"></p><h1 id="squeeze-and-excitation-blocks">Squeeze-and-Excitation Blocks</h1><p><span class="math inline">\(F_{tr} : X → U, X ∈ R^{H′×W′×C′},U ∈ R^{H×W×C}.\)</span>, <span class="math display">\[V = [v_1, v_2, . . . , v_C],U = [u_1, u_2, . . . , u_C]\]</span> V为C个卷积核（对应输入的channel），U为X经过每个卷积核得到的feature map,每个卷积核对应的卷积操作如下： <span class="math display">\[u_c = v_c ∗ X =\sum ^{C′} _{s=1} v^s_c ∗ x^s\]</span></p><h2 id="squeeze-global-information-embedding">Squeeze: Global Information Embedding</h2><p><span class="math inline">\(F_{sq}\)</span>进行的操作如下： <span class="math display">\[z_c = F_{sq}(u_c) =\frac {1} {H × W}\sum ^H _{i=1}\sum ^W _{j=1}u_{c(i, j)}\\z = [z_1,z_2,...,z_c]\]</span> 相当于对feature map的每个channel进行global average Pooling。</p><h2 id="excitation-adaptive-recalibration">Excitation: Adaptive Recalibration</h2><p><span class="math inline">\(F_{ex}\)</span>进行的操作如下： <span class="math display">\[s = F_{ex}(z,W) = σ(g(z,W)) = σ(W_2δ(W_1z))\]</span> <span class="math inline">\(σ\)</span>:ReLU,上式表示2次FC+ReLU。</p><p><span class="math inline">\(F_{scale}\)</span>进行的操作如下： <span class="math display">\[\widetilde x_{ec} = F_{scale}(u_c, s_c) = s_c · u_c,\]</span> 表示feature map的每个channel乘以对应的权重。</p><h2 id="exemplars-se-inception-and-se-resnet">Exemplars: SE-Inception and SE-ResNet</h2><p><img src="/Assets/Img/SE_Inception_and_SE_ResNet.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;探索channel间的关系提出新的结构单元：Squeezeand-Excitation” (SE) block。通过channel间的关联，调整对每个channel的feature进行调整。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="attention" scheme="//Rocky1ee.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining</title>
    <link href="//Rocky1ee.github.io/2019/11/30/Uncertainty%20Guided%20Multi-Scale%20Residual%20Learning-using%20a%20Cycle%20Spinning%20CNN%20for%20Single%20Image%20De-Raining/"/>
    <id>//Rocky1ee.github.io/2019/11/30/Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining/</id>
    <published>2019-11-30T08:56:36.000Z</published>
    <updated>2020-04-18T07:40:49.816Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>UEML通过学习不同尺度的rain content进行去雨，针对雨线的位置信息提出residual 的confidence measure引导网络训练的方法。最后用cycle spinning提升最后的derain效果。</p><a id="more"></a><h1 id="proposed-method">Proposed Method</h1><h2 id="umrl-network">UMRL Network</h2><p>给出residual的confidence，将residual和confidence一起输入后面的layer作为雨线位置的表示。在3个不同的尺度（1,0.5,0.25)上求residual。</p><p><img src="C:\Users\My\AppData\Roaming\Typora\typora-user-images\image-20200331091456255.png" alt="image-20200331091456255" style="zoom:67%;"></p><p>a 为 ConvBlock，b为Residual Network（RN）用于提取residual，c为Confidence map Network（CN）用于提取confidence。</p><p>confidence用于标识residual每个pixel residual value的正确性，有助于生成高质量的residual。</p><h2 id="loss-for-umrl">Loss for UMRL</h2><p>采用loss function 如下 <span class="math display">\[\begin{equation}\begin{aligned}\mathcal{L_1} &amp;= \sum_{i\in {(\times1,\times2,\times4)} }||(c_i \bigodot \hat x_i)-(c_i \bigodot x_i)||_1 \\\mathcal{L_c} &amp;=  \sum_{i\in {(\times1,\times2,\times4)} }(\sum_j\sum_klog(c_{i_{jk}}))\\\mathcal{L_u}&amp;=\mathcal{L_1}-\lambda_1\mathcal{L_c}\\\mathcal{L_p}&amp;=\frac{1}{NHW}\sum_i\sum_j\sum_k||F(\hat x_1)^{i,j,k}-F(x_1)^{i,j,k}||_2^2\\\mathcal{L}&amp;=\mathcal{L_l}-\lambda_1\mathcal{L_c}+\lambda_2\mathcal{L_p}\end{aligned}\end{equation}\]</span> c表示confidence，x表示去雨图，为了防止c全为0而加入<span class="math inline">\(L_c\)</span>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;UEML通过学习不同尺度的rain content进行去雨，针对雨线的位置信息提出residual 的confidence measure引导网络训练的方法。最后用cycle spinning提升最后的derain效果。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Non-local Neural Networks</title>
    <link href="//Rocky1ee.github.io/2019/11/28/Non-local%20Neural%20Networks/"/>
    <id>//Rocky1ee.github.io/2019/11/28/Non-local Neural Networks/</id>
    <published>2019-11-28T08:56:36.000Z</published>
    <updated>2020-03-06T02:10:53.316Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>提出non-local operations 加入到NN中用于获取long-range dependencies（间断时间画面之间的联系）。如图所示，在<span class="math inline">\(X_i\)</span>点的response为所有箭头指向特征的加权和。与<span class="math inline">\(X_i\)</span>相关性越强的特征，权重越大。</p><p><img src="/Assets/Img/Non_Local_NN.png"></p><h1 id="introduction">Introduction</h1><p>常用的Conv和recurrent操作都是基于local neighborhood获取long-range dependencies，其缺点为计算低效，optimization困难，难以利用多个远距离block间的联系信息。因此提出non-local 操作加入到NN中，</p><p>NL操作不考虑2个位置间的距离，直接计算两者间的关联以获取long-range dependence。</p><a id="more"></a><h1 id="related-work">Related Work</h1><p>self attention</p><h1 id="non-local-neural-networks">Non-local Neural Networks</h1><h2 id="formulation">Formulation</h2><p>DNN中non-local操作定义如下： <span class="math display">\[\textbf y_i = \frac{1}{\mathcal C(\textbf x)} \sum_{\forall j} f(\textbf x_i, \textbf x_j) g(\textbf x_j). \tag{1}\]</span> <span class="math inline">\(i\)</span>：输出位置的index，<span class="math inline">\(j\)</span>：所有可能位置的index。<span class="math inline">\(\textbf x\)</span>:输入信号，<span class="math inline">\(\textbf y\)</span>：与<span class="math inline">\(\textbf x\)</span>同size的输出信号，<span class="math inline">\(f()\)</span>：关系描述函数，<span class="math inline">\(g()\)</span>:输入信号在j位置的特征，<span class="math inline">\(\mathcal C(\textbf x)\)</span>:归一化系数。</p><p>式（1）表示所有位置在一个operation中都会考虑到，只是赋予的权重不同，所有叫non-local。而conv只关注相邻，而recurrent只关注之前的time step。</p><h2 id="instantiations">Instantiations</h2><p>举一些<span class="math inline">\(g(),f()\)</span>采用函数的例子，<span class="math inline">\(g()\)</span>可以是linear embedding：<span class="math inline">\(g(\textbf x_j)=W_g \textbf x_j\)</span>。</p><p><strong>Gaussian</strong> <span class="math display">\[f(\textbf x_i, \textbf x_j) = e^{\textbf x_i^T \textbf x_j}. \tag{2}\]</span> 其中：<span class="math inline">\(\textbf x_i^T \textbf x_j\)</span>为点积相似度，对应<span class="math inline">\(\mathcal C(\textbf x) = \sum_{\forall j} f(\textbf x_i, \textbf x_j)\)</span></p><p><strong>Embeded Guassian</strong> <span class="math display">\[f(\textbf x_i, \textbf x_j) = e^{\theta(\textbf x_i)^T \phi(\textbf x_j)}. \tag{3}\]</span> 其中： <span class="math display">\[\theta(\textbf x_i) = W_\theta \textbf x_i,\phi(\textbf x_j) = W_\phi \textbf x_j,\mathcal C(\textbf x) = \sum_{\forall j} f(\textbf x_i, \textbf x_j)\]</span> <span class="math inline">\(\frac{1}{\mathcal C(x)} f(\textbf x_i, \textbf x_j)\)</span>：<span class="math inline">\(\textbf x_i对应 \textbf x_j\)</span>的softmax即<span class="math inline">\(y = softmax(x^TW^T_θ W__φx)g(x)\)</span>。</p><p><strong>Dot product</strong> <span class="math display">\[f(\textbf x_i, \textbf x_j) = \theta(\textbf x_i)^T \phi(\textbf x_j). \tag{4}\]</span> 其中： <span class="math display">\[\theta(\textbf x_i) = W_\theta \textbf x_i，\phi(\textbf x_j) = W_\phi \textbf x_j，\mathcal C(\textbf x) = N\]</span> N为X中positions 的总数。</p><p><strong>Concatenation</strong> <span class="math display">\[f(\textbf x_i, \textbf x_j) = \text{ReLU} (\textbf w_f^T[\theta(\textbf x_i), \phi(\textbf x_j)]). \tag{5}\]</span> 其中<span class="math inline">\([\cdot, \cdot]\)</span>为concatenation。</p><h2 id="non-local-block"><strong>Non-local Block</strong></h2><p>将式（1）的non-local operation 加入到non-local block，block的定义如下： <span class="math display">\[\textbf z_i = W_z \textbf y_i + \textbf x_i, \tag{6}\]</span> 其中<span class="math inline">\(\textbf y_i,\textbf x_i\)</span>采用residual connection，这样做可以使non-local block插入到pre-train的NN中。具体操作如下：</p><p><img src="/Assets/Img/Non_local_DNN.png"></p><p><img src="/Assets/Img/Non_local_DNN_structure.png"></p><h1 id="reference">reference</h1><p>https://walkccc.github.io/blog/2018/10/27/Papers/nonlocal-nn/</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;提出non-local operations 加入到NN中用于获取long-range dependencies（间断时间画面之间的联系）。如图所示，在&lt;span class=&quot;math inline&quot;&gt;\(X_i\)&lt;/span&gt;点的response为所有箭头指向特征的加权和。与&lt;span class=&quot;math inline&quot;&gt;\(X_i\)&lt;/span&gt;相关性越强的特征，权重越大。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/Assets/Img/Non_Local_NN.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;常用的Conv和recurrent操作都是基于local neighborhood获取long-range dependencies，其缺点为计算低效，optimization困难，难以利用多个远距离block间的联系信息。因此提出non-local 操作加入到NN中，&lt;/p&gt;
&lt;p&gt;NL操作不考虑2个位置间的距离，直接计算两者间的关联以获取long-range dependence。&lt;/p&gt;
    
    </summary>
    
      <category term="网络结构" scheme="//Rocky1ee.github.io/categories/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="网络结构" scheme="//Rocky1ee.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Learning dual convolutional neural networks for low-level vision</title>
    <link href="//Rocky1ee.github.io/2019/11/25/Learning%20dual%20convolutional%20neural%20networks%20for%20low-level%20vision/"/>
    <id>//Rocky1ee.github.io/2019/11/25/Learning dual convolutional neural networks for low-level vision/</id>
    <published>2019-11-25T08:56:36.000Z</published>
    <updated>2020-04-18T07:41:10.850Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>Dual CNN：由2个平行分支构成，分别恢复图像的结构和细节信息。</p><h1 id="introduction">Introduction</h1><p>Residual learning的方法难以复原low-requency（图像的主要结构）。为解决此问题提出Dual CNN同时复原details和structure。Dual由2个分支组成，分支1为简单的网络用于估计structure，分支2为深度的网络用于估计details。</p><a id="more"></a><h1 id="related-work">related work</h1><p>Super-resolution：SRCNN，residual learning，VDSR。</p><p>Noise/artifacts removal:plain network难以复原detils，residual network难以正确恢复structural。</p><p>Edge-preserving filtering。</p><p>Image dehazing：构建一个transmission map，并加入atmospheric light。</p><h1 id="proposed-algorithm">Proposed Algorithm</h1><p>Dual CNN由Net-S（structure）和Net-D（Detail）构成，如下图</p><p><img src="/Assets/Img/Dual_CNN.png"></p><p>Loss Function 如下： <span class="math display">\[\mathcal{L}_x(S,D)=||\phi (S)+\varphi(D)-X||_2^2\]</span> X:ground truth, S:Net-S的输出，D：Net-D的输出，<span class="math inline">\(\phi(),\varphi\)</span>:特定任务对应的物理模型</p><h2 id="regularization-of-the-dualcnn-model">Regularization of the DualCNN Model</h2><p>为了保证Dual CNN训练的稳定性Net-S与Net-D采用不同的loss <span class="math display">\[\mathcal{L}_s(S)=||S-S_{gt}||_2^2 \\\mathcal{L}_d(D)=||D-D_{gt}||_2^2\]</span> <span class="math inline">\(S_{gt},D__{gt}\)</span>:structure 和 detail的ground truth。两者的GT是怎么得到的？？</p><p>Dual CNN的总体loss为： <span class="math display">\[\mathcal{L}=\alpha\mathcal{L}_x+ \lambda\mathcal{L}_s+\gamma\mathcal{L}_d\]</span> 在测试阶段有 <span class="math display">\[X_{test} = \phi (S)+\varphi(D)\]</span></p><h2 id="generalization">Generalization</h2><p>将上述模型具体应用在Image dehazing任务，按照Image dehazing的物理模型有 <span class="math display">\[I = JD+S(1-D)\]</span> <span class="math inline">\(I\)</span>:hazy image, <span class="math inline">\(J\)</span>:haze-free image,<span class="math inline">\(S\)</span>:atmospheric light,<span class="math inline">\(D\)</span>:transmission map.</p><p>按照上述模型则有： <span class="math display">\[\phi (S)=S(1-D)，\varphi(D)=JD \\\mathcal{L}_x(S,D)=||JD+S(1-D)-I||_2^2\]</span> 在测试的阶段有： <span class="math display">\[J_{test}=\frac{I-S}{max{D,d_0}} + S\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Dual CNN：由2个平行分支构成，分别恢复图像的结构和细节信息。&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Residual learning的方法难以复原low-requency（图像的主要结构）。为解决此问题提出Dual CNN同时复原details和structure。Dual由2个分支组成，分支1为简单的网络用于估计structure，分支2为深度的网络用于估计details。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Puppy money</title>
    <link href="//Rocky1ee.github.io/2019/11/25/Puppy%20Money/"/>
    <id>//Rocky1ee.github.io/2019/11/25/Puppy Money/</id>
    <published>2019-11-25T08:56:36.000Z</published>
    <updated>2020-02-09T07:17:15.200Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cp1">CP1</h1><p>明确目标，不断进行自我提醒和自我激励。</p><h1 id="cp2">CP2</h1><p>先去做，对于复杂的问题，完全搞懂or计划好再行动是不可取的。在行动中思考，再以此调整行动。</p><h1 id="cp3">CP3</h1><p>去发现和利用周围环境中的一切可利用条件，发现机遇，把注意力集中在自己知道，能做和拥有的东西上。</p><h1 id="cp4">CP4</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;cp1&quot;&gt;CP1&lt;/h1&gt;
&lt;p&gt;明确目标，不断进行自我提醒和自我激励。&lt;/p&gt;
&lt;h1 id=&quot;cp2&quot;&gt;CP2&lt;/h1&gt;
&lt;p&gt;先去做，对于复杂的问题，完全搞懂or计划好再行动是不可取的。在行动中思考，再以此调整行动。&lt;/p&gt;
&lt;h1 id=&quot;cp3&quot;
      
    
    </summary>
    
      <category term="理财" scheme="//Rocky1ee.github.io/categories/%E7%90%86%E8%B4%A2/"/>
    
    
      <category term="理财" scheme="//Rocky1ee.github.io/tags/%E7%90%86%E8%B4%A2/"/>
    
      <category term="读书" scheme="//Rocky1ee.github.io/tags/%E8%AF%BB%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image DerainingDeraining</title>
    <link href="//Rocky1ee.github.io/2019/11/20/Recurrent%20Squeeze-and-Excitation%20Context%20Aggregation%20Net%20for%20Single%20Image%20Deraining/"/>
    <id>//Rocky1ee.github.io/2019/11/20/Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining/</id>
    <published>2019-11-20T08:56:36.000Z</published>
    <updated>2020-04-18T07:40:29.396Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>提出新颖的卷积，循环神经网络。首先用dilated convolution扩大receptive filed，其次用squeeze-and-excitation block根据雨的密度和趋势，为各种rain streak标记不同alpha-values。rain streak layers 有重叠难以一次清除，所以采用多stages处理。采用循环神经网络保存每个stages的有效信息，以便逐步removing rainy。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>问题：</p><p>1.基于image patches的去雨方法，会忽视大片区域的contextual information，而该信息有助于去雨。</p><p>2.现存方法忽略了每步处理间的信息关联。</p><p>REcurrent SE Context Aggregation Net (RESCAN)：根据以上问题提出多stages的处理的方法，每个stages使用带有多 FC layer的context aggregation network 进行去雨。并用SE模块根据每个卷积层的关联给与每个channel不同alpha-value，同时使用dilation conv获取更多的contextual information。为了更好利用previous stages信息，later stages采用包含3种recurrent units的RNN结构。</p><h1 id="rain-models">Rain Models</h1><p>基本模型为 <span class="math display">\[\mathbf{O}=\mathbf{B}+\mathbf{R} \quad(1)\]</span> 因为rain streak layers 种类各异，于是将（1）变为（2） <span class="math display">\[\mathbf{O} = \mathbf{B} + \sum_{i=1}^n\mathbf{R}^i \quad(2)\]</span> n：n种不同rain streak layers</p><p>考虑到光的散射，在模型中引入环境光，rain streak layers 对B的影响因子,模型如下： <span class="math display">\[\mathbf{O} = (1-\sum_{i=1}^n\alpha_i)\mathbf{B} +\alpha_0\mathbf{A}+ \sum_{i=1}^n \alpha_i \mathbf{R}^i , s.t. α_i ≥ 0,\sum_{i=0}^n α_i ≤ 1 \quad(3)\]</span> <span class="math inline">\(\alpha_i\)</span>越大表示雨点越亮，对图像的影响越大，<span class="math inline">\(\alpha_0\)</span>越大表示整体环境光对图像的影响越大。</p><h1 id="deraining-method">Deraining Method</h1><p>学习O到R间的mapping，每个stages用含SE block的 context aggregation 网络，网络中的每个feature map对应一种类型rain streak，同时SE block根据feature maps之间的相互依赖度分配不同alpha-values。previous stages所提取的信息可以指导later stages的学习。 ## SE Context Aggregation Net</p><p><img src="/Assets/Img/SE_Context_Aggregation_Net.png"></p><p><span class="math inline">\(L_0\)</span>为decoder，<span class="math inline">\(L_1,L_2,L_3\)</span>为dilation conv得到的feature map，<span class="math inline">\(L_4,L_5\)</span>为decoder。具体结构如下表所示：</p><p><img src="/Assets/Img/architecture_of_scan.png"></p><p>feature map的每个channel代表一种类型的rain streak，用SE模块为每个channel分配相应的alpha-value。SCAN不使用BN，因为BN会将特征图的标准化，从而减少了channel间的差异，不利于SE操作。</p><h2 id="recurrent-se-context-aggregation-net">Recurrent SE Context Aggregation Net</h2><p>Recurrent基本结构如下： <span class="math display">\[\begin{equation}\begin{aligned}\mathbf{O}_1 &amp;= \mathbf{O}\\\mathbf{R}_s &amp;= f_{CNN} (\mathbf{O}_s) , 1 ≤ s ≤ S \quad (5)\\ \mathbf{O}_{s+1} &amp;= \mathbf{O}_s −\mathbf{R}_s, 1 ≤ s ≤ S,\\\mathbf{R} &amp;= \sum_{s=1}^S\mathbf{R}_s\end{aligned}\end{equation}\]</span> s为stages的编号，<span class="math inline">\(R_s\)</span>为第s 个stages的输出，<span class="math inline">\(O_{s+1}\)</span>为第s个stages 得到的去雨图，也是第s+1个stages的输入。（O为一开始输入的合成雨图）。</p><p>为了更好的利用每个stage之间的信息，采用了类似RNN中的记忆单元以便于深层stage将浅层的stage信息利用起来，于是将式（5）变为（8）： <span class="math display">\[\mathbf{R}_s = \sum_{i=1}^n\alpha_i \bar{\mathbf{R}}_s^i =f_{CNN+RNN}(\mathbf{O}_s,x_{s-1}), 1 ≤ s ≤ S \quad (5)\]</span></p><p><span class="math inline">\(\bar{\mathbf{R}}_s^i\)</span>:第s个stage，第i个channel的rain streak layer，<span class="math inline">\(x__{s-1}\)</span>第s-1个stage的hidden state，<span class="math inline">\(R_s\)</span>：第s个stage 各个channel配置不同alpha-value的总和。</p><p>recurrent unit 有3种备选方法：ConvRNN，ConvGRU，ConvLSTM，下面以ConvGRU为例</p><p><img src="/Assets/Img/RSECAN_GRU.png"></p><p><img src="/Assets/Img/GRU.png"></p><h2 id="recurrent-frameworks">Recurrent Frameworks</h2><p>Additive prediction框架:将上个stage的predictions（<span class="math inline">\(O_s\)</span>)和feature maps(<span class="math inline">\(x_{s-1}\)</span>)作为输入，去预测前面stages的predictions的总和与ground truth间的residual。 <span class="math display">\[\mathbf{R}_s = f(\mathbf{O}_s,x_{s-1})\\\mathbf{O}_{s+1}=\mathbf{O}-\sum_{j=1}^s\mathbf{R}_j=\mathbf{O}_s-\mathbf{R}_s\]</span> 其loss为： <span class="math display">\[L(Θ)=\sum_{s=1}^S||\sum_{j=1}^s\mathbf{R_j}-\mathbf{R}||_F^2\]</span> 这个loss 需要讨论下？？？</p><p>Full Prediction框架：预测最终的full rain streaks <span class="math display">\[\hat{\mathbf{R}_s} = f(\hat{\mathbf{O}_s},x_{s-1})\\\hat{\mathbf{O}_{s+1}}=\mathbf{O}-\hat{\mathbf{R}_s}\]</span> 其loss为： <span class="math display">\[L(Θ)=\sum_{s=1}^S||\hat{\mathbf{R}}_s-\mathbf{R}||_F^2\]</span> 这两个loss没看懂，看看代码</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;提出新颖的卷积，循环神经网络。首先用dilated convolution扩大receptive filed，其次用squeeze-and-excitation block根据雨的密度和趋势，为各种rain streak标记不同alpha-values。rain streak layers 有重叠难以一次清除，所以采用多stages处理。采用循环神经网络保存每个stages的有效信息，以便逐步removing rainy。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Density-aware Single Image De-raining using a Multi-stream Dense Network</title>
    <link href="//Rocky1ee.github.io/2019/11/13/Density-aware%20Single%20Image%20De-raining%20using%20a%20Multi-stream%20Dense%20Network/"/>
    <id>//Rocky1ee.github.io/2019/11/13/Density-aware Single Image De-raining using a Multi-stream Dense Network/</id>
    <published>2019-11-13T08:56:36.000Z</published>
    <updated>2020-04-18T07:41:27.198Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>DID-MDN：Density-aware Image De-raining method using a Multistream Dense Network 由雨密度评rain-density classification（heavy,medium,light)和de-raining 2部分组成。网络先评估图像雨痕的密度标签，然后根据标签信息+多分支网络利用多尺度特征高效去雨。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>DID-MDN由residual-aware rain-density classification （对rainy image的residual component 进行分类）和 rain streak removal （multi-stream densely connected network：不同尺度的densely-connected block结合）组成。</p><p>提出一个12000张由heavy，medium，light 3种水平synthesized rain image组成的训练集。1200张包含3种水平synthesized rain image组成的测试集。</p><h1 id="proposed-method">Proposed Method</h1><p><img src="/Assets/Img/DID-MDN.png"></p><h2 id="residual-aware-rain-density-classifier">Residual-aware Rain-density Classifier</h2><p>单一网络难以学习不同密度的雨，为解决此问题。引入RaRdC CNN classifier对rainy images的rain-density进行分类（light，medium，high），并以此label辅助de-raining 过程。</p><p>由于使用fine-turn的pre-defined model 如VGG-16，这些deep model更关注high-level features（discriminative object),所以相对细节的rain-streaks容易被忽略，因此采用对residual进行分类： <span class="math display">\[r = y -x \]</span> r:residual(rain-streak), y:rainy image, x: clean background image。</p><p>RaRdC由residual part feature extraction sub-network和classification sub-network构成。Loss function如下： <span class="math display">\[\mathfrak{L} = \mathfrak{L}_{E,r}+\mathfrak{L}_C\]</span> 其中<span class="math inline">\(\mathfrak{L}_{E,r}\)</span>:每个pixel间的欧式距离用于获取residual component，<span class="math inline">\(\mathfrak{L}__C\)</span>:交叉熵loss 用于rain-density分类</p><h2 id="multi-stream-dense-network">Multi-stream Dense Network</h2><p>不同程度的rain-streaks需要用不同尺度的receptive fields获取（longer rain-streaks 需要 larger receptive）。因此采用包含不同尺度的kernel的dense-block提取特征。然后将3个不同尺度Dense提取的特征进行modified connectivity与RaRdC 生成 label map（将label map 进行up-sampling）合并用于生成residual（<span class="math inline">\(\hat{r}\)</span>)。rainy image -（<span class="math inline">\(\hat{r}\)</span>)=coarse de-rained image，最后将coarse de-rained image输入refinement（conv+ReLU）进行精修。</p><p><img src="/Assets/Img/DID-MDN_Dense_Block.png"></p><p>MsDN中每个Dense包含6个Dense Block，将每个Dense Block输出的特征Up-sampling到同一尺寸后合并起来（6合1）。</p><p>整个De-raining Network的loss function如下： <span class="math display">\[\begin{equation}\begin{aligned}\mathfrak{L} &amp;= \mathfrak{L}_{E,r}+\mathfrak{L}_{E,d}+\lambda_F\mathfrak{L}_{F}\\\mathfrak{L}_{F} &amp;= \frac{1}{CWH}||F(\hat x)^{c,w,h}-F(x)^{c,w,h}||^2_2\end{aligned}\end{equation}\]</span> 其中：$ _____{E,r}<span class="math inline">\(为residual的loss，\)</span> _____{E,d}<span class="math inline">\(为 de-rain image的loss，\)</span>_{F}$为refinement的loss。</p><p>这几个loss还需要看看源码！！！！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;DID-MDN：Density-aware Image De-raining method using a Multistream Dense Network 由雨密度评rain-density classification（heavy,medium,light)和de-raining 2部分组成。网络先评估图像雨痕的密度标签，然后根据标签信息+多分支网络利用多尺度特征高效去雨。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Removing rain from single images via a deep detail network</title>
    <link href="//Rocky1ee.github.io/2019/11/04/Removing%20rain%20from%20single%20images%20via%20a%20deep%20detail%20network/"/>
    <id>//Rocky1ee.github.io/2019/11/04/Removing rain from single images via a deep detail network/</id>
    <published>2019-11-04T07:56:36.000Z</published>
    <updated>2020-04-18T07:39:12.824Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>利用ResNet的思想提出Deep detail network减少从输入到输出的mapping range，同时用hig frequency detail进行训练。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>negative residual mapping:清晰图和有雨图之间的差别<span class="math inline">\(Y-X\)</span>,因为雨线接近白色pixel值较高，所以<span class="math inline">\(Y-X\)</span>中的大部分元素为negative。同时这样可以减少mapping的空间，更易于network学习。目标函数由（1）变成（2） <span class="math display">\[\mathcal{L}=\sum_i||h(\boldsymbol{X_i})-\boldsymbol{Y_i}||^2_F \quad(1)\]</span></p><p><span class="math display">\[\mathcal{L}=\sum_i||h(\boldsymbol{X_i})+\boldsymbol{X_i}-\boldsymbol{Y_i}||^2_F \quad(2)\]</span> 先用 low pass filter将有雨图分为detail layer+base layer，再将detail layer作为network的输入 <span class="math display">\[X = \boldsymbol{X}_{detail} +\boldsymbol{X}_{base}\]</span> 目标函数由（2）变成（3） <span class="math display">\[\mathcal{L}=\sum_{i=1}^N||f(\boldsymbol{X}_{i,detail},\boldsymbol{W},b)+\boldsymbol{X_i}-\boldsymbol{Y_i}||^2_F \quad(2)\]</span> 由于<span class="math inline">\(X_base \approx Y_base\)</span>,所以<span class="math inline">\(X_base - Y_base\)</span>等价于<span class="math inline">\(X_{detail}-Y__{detail}\)</span>.</p><p>采用的网络结构如下：</p><p><img src="/Assets/BlogImg/negative_mapping_structure.png"> $$ <span class="math display">\[\begin{equation}\begin{aligned}\boldsymbol{X}^0_{detail}&amp;=\boldsymbol{X}-\boldsymbol{X}_{base}\\\boldsymbol{X}^1_{detail}&amp;=\sigma(BN(\boldsymbol{W}^1*\boldsymbol{X}^0_{detail}+b^1))\\\boldsymbol{X}^{2l}_{detail}&amp;=\sigma(BN(\boldsymbol{W}^{2l}*\boldsymbol{X}^{2l-1}_{detail}+b^{2l}))\\\boldsymbol{X}^{2l+1}_{detail}&amp;=\sigma(BN(\boldsymbol{W}^{2l+1}*\boldsymbol{X}^{2l}_{detail}+b^{2l+1}))+\boldsymbol{X}^{2l-1}_{detail}\\\boldsymbol{Y}_{approx}&amp;=BN(\boldsymbol{W}^{L}*\boldsymbol{X}^{L-1}_{detail}+b^L)+\boldsymbol{X}\end{aligned}\end{equation}\]</span> $$ 网络相当于学会了一个生成负雨线的mapping，将其结果与有雨图像相加就可以把有雨图像上的雨线去掉。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;利用ResNet的思想提出Deep detail network减少从输入到输出的mapping range，同时用hig frequency detail进行训练。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="单幅图像去雨" scheme="//Rocky1ee.github.io/tags/%E5%8D%95%E5%B9%85%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/"/>
    
      <category term="深度学习算法" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Deep Joint Rain Detection and Removal from a Single Image</title>
    <link href="//Rocky1ee.github.io/2019/10/31/Deep%20Joint%20Rain%20Detection%20and%20Removal%20from%20a%20Single%20Image/"/>
    <id>//Rocky1ee.github.io/2019/10/31/Deep Joint Rain Detection and Removal from a Single Image/</id>
    <published>2019-10-31T08:56:36.000Z</published>
    <updated>2020-04-18T07:39:28.074Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>新的模型和新的深度学习结构，binary map（rain streak layer + back ground layer）对雨痕定位,来处理haven rain情况（mist，overlapping rain streaks），提出含有contextualized dilated 结构</p><p>recurrent rain detection and removal network清除haven rain。</p><a id="more"></a><h1 id="introduction">Introduction</h1><p>问题：</p><p>1.雨痕与背景纹理的重叠，导致纹理细节被去除，某些区域被over-smoothing。</p><p>2.感受野的大小有限，无法获取spatial contextual information in larger ragions</p><p>贡献：</p><p>1.region-dependent rain models：binary map，pixel=1表示该为rain streak，pixel=0表示该点为background。</p><p>2.检测和去雨一体化的deep network：对rain和non-rain regions 采取不同操作</p><p>3.contextualized dilated network：空洞卷积扩大感受野，采用多路平行，卷积核各异的结构</p><p>4.recurrent rain detection and removal network。</p><h1 id="region-dependent-rain-image-model">Region-Dependent Rain Image Model</h1><p><span class="math inline">\(widetilde{S}\)</span>无法用常用分布的描述 <span class="math display">\[\boldsymbol{O=B+\widetilde{S}} \quad(1)\]</span> 为了给出<span class="math inline">\(widetilde{S}\)</span>的位置和密度信息，将（1）变为（2） <span class="math display">\[\boldsymbol{O=B+SR} \quad(2)\]</span> <span class="math inline">\(boldsymbol{R}\)</span>：二值掩码，0=无雨区域，1=有雨区域。 <span class="math display">\[\boldsymbol{O}=\alpha (\boldsymbol{B+\sum_{t=1}^s\widetilde{S}_tR}) +(1-\alpha)\boldsymbol{A} \quad(3)\]</span> <span class="math inline">\(\widetilde{S}_t\)</span>代表第t类雨型，<span class="math inline">\(\boldsymbol{A}\)</span>为光照。s越大，叠加各种类型的雨越多，雨也就越厚重。</p><h1 id="joint-rain-streak-detection-and-removal">Joint Rain Streak Detection and Removal</h1><p>缺乏细节，如何对 F 进行conv得到R？以及后续的F，R conv得到S？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;新的模型和新的深度学习结构，binary map（rain streak layer + back ground layer）对雨痕定位,来处理haven rain情况（mist，overlapping rain streaks），提出含有contextualized dilated 结构&lt;/p&gt;
&lt;p&gt;recurrent rain detection and removal network清除haven rain。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>PSNR的原理和代码</title>
    <link href="//Rocky1ee.github.io/2019/10/29/PSNR%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/"/>
    <id>//Rocky1ee.github.io/2019/10/29/PSNR的原理和代码/</id>
    <published>2019-10-29T08:56:36.000Z</published>
    <updated>2020-04-18T07:37:31.625Z</updated>
    
    <content type="html"><![CDATA[<h1 id="原理">原理</h1><p>峰值信噪比(Peak Signal to Noise Ratio) PSNR, 表示信号可能取得的最大功率和影响其精度的噪声功率的比值，用来衡量图像画质的损失程度。PSNR越大表示画质损失越小，其表达式如下：<span class="math inline">\(v_max\)</span>表示pixel取值范围内的最大值，例如取值范围为<span class="math inline">\([0,255]\)</span>，则<span class="math inline">\(v_max=255\)</span>。 <span class="math display">\[\begin{equation}\begin{aligned}PSNR &amp;= 10 log_{10} \frac{v_{max}^2}{MSE}\\MSE &amp;= \frac{\sum_{y=0}^{H-1}\sum_{x=0}^{W-1}[I_1(x,y)-I_2(x,y)]^2}{HW}\end{aligned}\end{equation}\]</span></p><a id="more"></a><h1 id="代码">代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">psnr</span><span class="params">(target,ref,scale)</span>:</span></span><br><span class="line">  <span class="comment">#目标图像，对比图像，size</span></span><br><span class="line"></span><br><span class="line">  target_data = np.array(target)</span><br><span class="line">  target_data = target_data[scale:-scale,scale:-scale]</span><br><span class="line"></span><br><span class="line">  ref_data = np.array(ref)</span><br><span class="line">  ref_data = ref_data[scale:-scale,scale:-scale]</span><br><span class="line"></span><br><span class="line">  diff = ref_data - target_data</span><br><span class="line">  diff = diff.flatten(<span class="string">'C'</span>)</span><br><span class="line">  rmse = math.sqrt(np.mean(diff ** <span class="number">2</span>))</span><br><span class="line">  <span class="keyword">return</span> <span class="number">20</span>*math.log10(<span class="number">10</span>/rmse)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;原理&quot;&gt;原理&lt;/h1&gt;
&lt;p&gt;峰值信噪比(Peak Signal to Noise Ratio) PSNR, 表示信号可能取得的最大功率和影响其精度的噪声功率的比值，用来衡量图像画质的损失程度。PSNR越大表示画质损失越小，其表达式如下：&lt;span class=&quot;math inline&quot;&gt;\(v_max\)&lt;/span&gt;表示pixel取值范围内的最大值，例如取值范围为&lt;span class=&quot;math inline&quot;&gt;\([0,255]\)&lt;/span&gt;，则&lt;span class=&quot;math inline&quot;&gt;\(v_max=255\)&lt;/span&gt;。 &lt;span class=&quot;math display&quot;&gt;\[
\begin{equation}
\begin{aligned}
PSNR &amp;amp;= 10 log_{10} \frac{v_{max}^2}{MSE}\\
MSE &amp;amp;= \frac{\sum_{y=0}^{H-1}\sum_{x=0}^{W-1}[I_1(x,y)-I_2(x,y)]^2}{HW}
\end{aligned}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>SSIM的原理和代码</title>
    <link href="//Rocky1ee.github.io/2019/10/29/SSIM%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/"/>
    <id>//Rocky1ee.github.io/2019/10/29/SSIM的原理和代码/</id>
    <published>2019-10-29T08:56:36.000Z</published>
    <updated>2020-04-18T07:37:10.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="原理">原理</h1><p>Structural Similarity（SSIM）结构相似度，用于衡量原图和重构图像的相似性。因为MSE只是计算图像间像素值的差异，SSIM中加入对比度和结构的衡量，更能反映人类视觉系统。</p><p>两幅图像x，y的SSIM由三个维度组成：亮度（luminance），对比度（contrast）和结构（structure）。其函数表达式为： <span class="math display">\[S(x,y)=f(l(x,y),c(x,y),s(x,y))\]</span> 亮度<span class="math inline">\(l(x,y)\)</span>：图像x有N个pixel，每个pixel的值为<span class="math inline">\(x_i\)</span>，x的亮度为</p><a id="more"></a><p><span class="math display">\[\mu_x=\frac{1}{N}\sum_{i=1}^Nx_i\]</span> x和y的亮度对比为：<span class="math inline">\(C_1\)</span>为常数防止分母为0 <span class="math display">\[l(x,y)=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}\]</span> 对比度<span class="math inline">\(c(x,y)\)</span>:图像明暗的变化程度，用pixel值的标准差衡量 <span class="math display">\[\sigma_x = (\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_x)^2)^{\frac{1}{2}}\]</span> x和y的对比度衡量为：<span class="math inline">\(C_2\)</span>为常数防止分母为0 <span class="math display">\[c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}\]</span> 结构<span class="math inline">\(s(x,y)\)</span>:由图像中所有pixel组成的向量表示，同时在研究结构时应该排除亮度和对比度的影响，因此将向量进行归一化后，采用余弦相似度策略 <span class="math display">\[\begin{equation}\begin{aligned}s(x,y)&amp;=(\frac{1}{\sqrt{N-1}}\frac{x-\mu_x}{\sigma_x})(\frac{1}{\sqrt{N-1}}\frac{y-\mu_y}{\sigma_y})\\&amp;=\frac{1}{\sigma_x\sigma_y}(\frac{1}{\sqrt{N-1}}\sum_{i=1}^N(x_i-\mu_x)(y_i-\mu_y))\\&amp;=\frac{\sigma_{xy}}{\sigma_x\sigma_y}\end{aligned}\end{equation}\]</span> 加入常数<span class="math inline">\(C_3\)</span>防止分母为0 <span class="math display">\[s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3}\]</span> 为化简，令<span class="math inline">\(C_3=\frac{C_2}{2}\)</span>,将<span class="math inline">\(l,c,s\)</span>相乘，最终SSIM为： <span class="math display">\[SSIM(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}\]</span> 类比人眼的注意力特征，视觉只聚焦于局部，两幅图的SSIM计算采用了sliding window求平均策略</p><p>单幅图像分为M个patch，MSSIM的计算为： <span class="math display">\[MSSIM(X,Y)=\frac{1}{M}\sum_{j=1}^MSSIM(x_j,y_j)\]</span></p><h1 id="代码">代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">import numpy</span><br><span class="line">from scipy.ndimage import uniform_filter,gaussian_filter</span><br><span class="line"></span><br><span class="line">from skimage.util.dtype import dtype_range</span><br><span class="line">from skimage.util.arraycrop import crop</span><br><span class="line"></span><br><span class="line">def compare_ssim(X,Y,win_size=None,dynamic_range=None,gaussian_weights=False,full=False</span><br><span class="line">                 ,**kwargs):</span><br><span class="line"> </span><br><span class="line">  #默认参数设置</span><br><span class="line">  K1 = 0.01</span><br><span class="line">  K2 = 0.03</span><br><span class="line">  sigma = 1.5</span><br><span class="line"></span><br><span class="line">  use_sample_covariance = True</span><br><span class="line"></span><br><span class="line">  #window大小的选择</span><br><span class="line">  if win_size is None:</span><br><span class="line">    if gaussian_weights:</span><br><span class="line">      win_size = 11</span><br><span class="line">    else:</span><br><span class="line">      win_size = 7</span><br><span class="line">  </span><br><span class="line">  if not (win_size % 2 == 1):</span><br><span class="line">    raise ValueError(&apos;Window size must be odd&apos;)</span><br><span class="line"></span><br><span class="line">  #确定图像数据类型的范围用于亮度，对比度中的常数设置</span><br><span class="line">  if dynamic_range is None:</span><br><span class="line">    dmin,dmax = dtype_range[X.dtype.type]</span><br><span class="line">    dynamic_range = dmax - dmin</span><br><span class="line"></span><br><span class="line">  ndim = X.ndim</span><br><span class="line">  </span><br><span class="line">  #sliding window内滤波方式选择</span><br><span class="line">  if gaussian_weights:</span><br><span class="line">    filter_func = gaussian_filter</span><br><span class="line">    filter_args = &#123;&apos;sigma&apos;:sigma&#125;</span><br><span class="line"></span><br><span class="line">  else:</span><br><span class="line">    filter_func = uniform_filter</span><br><span class="line">    filter_args = &#123;&apos;size&apos;:win_size&#125;</span><br><span class="line">  </span><br><span class="line">  X = X.astype(np.float64)</span><br><span class="line">  Y = Y.astype(np.float64)</span><br><span class="line"></span><br><span class="line">  #window内pixel个数</span><br><span class="line">  NP = win_size ** ndim</span><br><span class="line">  </span><br><span class="line">  #是否采用无偏估计</span><br><span class="line">  if use_sample_covariance:</span><br><span class="line">    conv_norm = NP / (NP - 1)</span><br><span class="line">  else:</span><br><span class="line">    conv_norm = 1.0</span><br><span class="line"></span><br><span class="line">  #计算window内均值</span><br><span class="line">  ux = filter_func(X,**filter_args)</span><br><span class="line">  uy = filter_func(Y,**filter_args)</span><br><span class="line"></span><br><span class="line">  #计算window内均值</span><br><span class="line">  uxx = filter_func(X * X,**filter_args)</span><br><span class="line">  uyy = filter_func(Y * Y,**filter_args)</span><br><span class="line"></span><br><span class="line">  uxy = filter_func(X * Y,**filter_args)</span><br><span class="line"></span><br><span class="line">  #计算window内方差，协方差</span><br><span class="line">  vx = conv_norm * (uxx - ux * ux)</span><br><span class="line">  vy = conv_norm * (uyy - uy * uy)</span><br><span class="line">  vxy = conv_norm * (uxy - ux * uy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  R = dynamic_range</span><br><span class="line"></span><br><span class="line">  C1 = (K1 * R) ** 2</span><br><span class="line">  C2 = (K1 * R) ** 2</span><br><span class="line"></span><br><span class="line">  #按公式计算</span><br><span class="line">  A1,A2,B1,B2 = ((</span><br><span class="line">      2 * ux * uy + C1,2 * uxy + C2,</span><br><span class="line">      ux ** 2 + uy ** 2 + C1, vx + vy + C2</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  D = B1 * B2</span><br><span class="line">  S = (A1 * A2) / D</span><br><span class="line"></span><br><span class="line">  #边缘部分卷积时的处理</span><br><span class="line">  pad = (win_size - 1) // 2</span><br><span class="line"></span><br><span class="line">  #多pitch计算SSIM的均值</span><br><span class="line">  mssim = crop(S,pad).mean()</span><br><span class="line"></span><br><span class="line">  if full:</span><br><span class="line">    return mssim,S</span><br><span class="line">  else:</span><br><span class="line">    return mssim</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;原理&quot;&gt;原理&lt;/h1&gt;
&lt;p&gt;Structural Similarity（SSIM）结构相似度，用于衡量原图和重构图像的相似性。因为MSE只是计算图像间像素值的差异，SSIM中加入对比度和结构的衡量，更能反映人类视觉系统。&lt;/p&gt;
&lt;p&gt;两幅图像x，y的SSIM由三个维度组成：亮度（luminance），对比度（contrast）和结构（structure）。其函数表达式为： &lt;span class=&quot;math display&quot;&gt;\[
S(x,y)=f(l(x,y),c(x,y),s(x,y))
\]&lt;/span&gt; 亮度&lt;span class=&quot;math inline&quot;&gt;\(l(x,y)\)&lt;/span&gt;：图像x有N个pixel，每个pixel的值为&lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt;，x的亮度为&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
  <entry>
    <title>Guided Filter 导向滤波</title>
    <link href="//Rocky1ee.github.io/2019/10/05/Guided_Filter_%E5%AF%BC%E5%90%91%E6%BB%A4%E6%B3%A2/"/>
    <id>//Rocky1ee.github.io/2019/10/05/Guided_Filter_导向滤波/</id>
    <published>2019-10-05T07:56:36.000Z</published>
    <updated>2020-01-14T07:53:44.732Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/Assets/Img/Guaded_filter2.jpeg"></p><a id="more"></a><h1 id="数学推导">数学推导</h1><p>输入图像为P，引导图像为G,输出图像为Q。导向滤波目标一：尽可能使P和Q相同，即噪声越小越好，数学表达为： <span class="math display">\[Min(Q-P)^2 \qquad*\]</span> 导向滤波目标二：保存edge，尽量使Q与G的纹理相似，数学表达为： <span class="math display">\[\nabla{Q}=a\nabla{G}\]</span> 积分可得： <span class="math display">\[Q=aG+b\]</span> 假设在一个小窗口<span class="math inline">\(W_k\)</span>内，a,b为常数。则Q与G在<span class="math inline">\(W_k\)</span>内的pixel满足： <span class="math display">\[q_i=a_kg_i+b_k\qquad i\in W_k \qquad(1)\]</span> 将（1）带入*式，同时对<span class="math inline">\(a_k\)</span>进行正则化： <span class="math display">\[E(a_k,b_k)=\sum_{i \in W_k}((a_kg_i+b_k-p_i)^2+\varepsilon a_k^2) \qquad(2)\]</span> 使（2）最小化需满足： $$ <span class="math display">\[\begin{equation}\begin{aligned}\frac{\partial E}{\partial a_k}&amp;=\sum_{i \in W_k}(2g_i(g_ia_k+b_k-p_i)+2\varepsilon a_k)=0\\\frac{\partial E}{\partial b_k}&amp;=\sum_{i \in W_k}(2(g_ia_k+b_k-p_i))=0\end{aligned}\end{equation}\]</span> <span class="math display">\[化简得：\]</span> <span class="math display">\[\begin{equation}\begin{aligned}(\sum_{i \in W_k}g_i^2+\varepsilon |W|)a_k+\sum_{i \in W_k}g_ib_k-\sum_{i \in W_k}p_ig_i&amp;=0\\\sum_{i \in W_k}g_ia_k+|W|b_k-\sum_{i \in W_k}p_i&amp;=0\end{aligned}\end{equation}\]</span> <span class="math display">\[其中|W|为$W_k$内pixel总数。解得：\]</span> <span class="math display">\[\begin{equation}\begin{aligned}a_k&amp;=\frac{|W|\sum_{i \in W_k}p_ig_i-\sum_{i \in W_k}p_i\sum_{i \in W_k}g_i}{|W|\sum_{i \in W_k}g_i^2+\varepsilon|W|^2-(\sum_{i \in W_k}g_i)^2}\quad (3)\\b_k&amp;=\frac{\sum_{i \in W_k}p_i-a_k\sum_{i \in W_k}g_i}{|W|}\quad(4)\end{aligned}\end{equation}\]</span> <span class="math display">\[化简得：\]</span> <span class="math display">\[\begin{equation}\begin{aligned}a_k&amp;=\frac{\frac{1}{|W|}\sum_{i \in W_k}p_ig_i-\bar{p_k}\mu_k}{\sigma^2_k+\varepsilon}=\frac{Cov(p_k,g_k)}{\sigma^2_k+\varepsilon}\quad(5) \\b_k&amp;=\bar{p_k}-\mu_ka_k\quad(6)\end{aligned}\end{equation}\]</span> $$ 其中$ {p_k}<span class="math inline">\(为P的pixel在\)</span>W_k<span class="math inline">\(内均值，\)</span>_k和<span class="math inline">\(为G的pixel在\)</span>W_k$内均值和方差。</p><p>覆盖pixel i 的窗口有|W|个，对应不同的窗口<span class="math inline">\(W_k\)</span>，根据<span class="math inline">\(a_k,b_k\)</span>可以求出不同的<span class="math inline">\(q_i\)</span>,因此我们对|W|个窗口对应的<span class="math inline">\(q_i\)</span>求均值: <span class="math display">\[\begin{equation}\begin{aligned}q_i&amp;=\frac{1}{|W|}\sum_{k=1,i \in W_k}^{k=|W|}(a_kg_i+b_k)\quad(7)\\q_i&amp;=\bar a_i g_i+\bar b_i \quad(8)\end{aligned}\end{equation}\]</span></p><h1 id="原图作为导向图">原图作为导向图</h1><p>当G=P，也就是把原图作为导向图则有： <span class="math display">\[Cov(p_k,g_k)=\sigma^2_k,\bar{p_k}=\mu_k\]</span> 带入（5）(6)可得： <span class="math display">\[a_k=\frac{\sigma^2_k}{\sigma_k^2+\varepsilon},b_k=\mu_k(1-a_k)\]</span> 如果<span class="math inline">\(\varepsilon =0时，a_k=1,b_k=0\)</span>，即Q=P。</p><p>如果<span class="math inline">\(\varepsilon &gt;0,且\sigma_k^2&gt;&gt;\varepsilon\)</span>时,即P在窗口<span class="math inline">\(W_k\)</span>内高方差（edge很多）。则有<span class="math inline">\(a_k \approx 1,b_k \approx 0\)</span>，即纹理被保留下来，进行很小的平滑。</p><p>如果<span class="math inline">\(\varepsilon &gt;0,且\sigma_k^2&lt;&lt;\varepsilon\)</span>时,即P在窗口<span class="math inline">\(W_k\)</span>内很平坦。则有<span class="math inline">\(a_k \approx 0,b_k \approx \mu_k\)</span>，即在平坦块中进行均值滤波。</p><p>这样，当一个像素在高方差的窗口中时，它的输出值是不变的。在平坦区域中，它的输出值变成周围窗口像素的平均值。</p><p>具体的，高方差和平坦的标准是由正则系数ε控制的。如果窗口的方差比ε小的多则被平滑，如果方差比ε大得多的则被保留。而窗口的半径r决定参考周围多大范围内的pixel来计算方差和均值。</p><p><img src="/Assets/Img/guided_filter.jpeg"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/Assets/Img/Guaded_filter2.jpeg&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="图像处理" scheme="//Rocky1ee.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="//Rocky1ee.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="单幅图像去雨" scheme="//Rocky1ee.github.io/tags/%E5%8D%95%E5%B9%85%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>autoencoder</title>
    <link href="//Rocky1ee.github.io/2019/09/20/autoencoder/"/>
    <id>//Rocky1ee.github.io/2019/09/20/autoencoder/</id>
    <published>2019-09-20T05:56:36.000Z</published>
    <updated>2020-01-04T00:12:22.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>用encoder将输入压缩成一个低维的code（输入的总结），然后用decoder将code重建成输入。</p><p><img src="/Assets/BlogImg/autoencode.png"></p><p>autoencoder的特点是：</p><p>Data-specific：只能压缩训练过的或类似的数据。</p><p>Lossy：输入和输出并不完全一致，不适合无损压缩问题。</p><p>Unsupervised：self-supervised 其label为输入数据自身。</p><a id="more"></a><h1 id="architecture">Architecture</h1><p>标准的encoder和decoder都是由fully-connected neural network 构成。decoder与encoder的结构一般具有镜面对称性，输入和输出尺寸一致。</p><p><img src="/Assets/BlogImg/ANN%20structure%20of%20autoencode.png"></p><p>训练时需要设置的超参数：</p><p>Code size：code的尺寸。</p><p>Number of layers: encoder 和decoder的层数，如上为2。</p><p>Number of nodes per layer：encoder中的notes逐层减少，decoder中的notes逐层增加。</p><p>Loss function：mse or binary crossentropy 等</p><h1 id="implementation">Implementation</h1><p>用keras在MNIST数据集上建立一个一层的autoencode</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">input_size = <span class="number">784</span></span><br><span class="line">hidden_size = <span class="number">128</span></span><br><span class="line">code_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">input_img = Input(shape=(input_size,))</span><br><span class="line">hidden_1 = Dense(hidden_size,activation=<span class="string">'relu'</span>)(input_img)</span><br><span class="line">code = Dense(code_size,activation=<span class="string">'relu'</span>)(hidden_1)</span><br><span class="line">hidden_2 = Dense(hidden_size,activation=<span class="string">'relu'</span>)(code)</span><br><span class="line">output_img = Dense(input_size,activatoin=<span class="string">'sigmoid'</span>)(hidden_2)</span><br><span class="line"></span><br><span class="line">autoencoder = Model(input_img,output_img)</span><br><span class="line">autoencoder.complie(optimizer=<span class="string">'adam'</span>,loss=<span class="string">'binary_crossentropy'</span>)</span><br><span class="line">autoencoder.fit(x_train,x_train,epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h1 id="visualization">Visualization</h1><p><img src="/Assets/BlogImg/autoencode%20result.png"></p><p>我们可以通过增加hyperparameters数量去增强autoencode的能力。如果autoencod太强过拟合了，就只能copy训练过的input。code的size决定了其被压缩的程度。</p><h1 id="denoising-autoencoders">Denoising Autoencoders</h1><p>用autoencode去除图像上的噪声：</p><p><img src="/Assets/BlogImg/Denoising_auto.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.fit(x_train_noisy,x_train)</span><br></pre></td></tr></table></figure><p><img src="/Assets/BlogImg/denoising_results.png"></p><h1 id="sparse-autoencoders">Sparse Autoencoders</h1><p>只激活部分nodes（激活即nodes输入不为0），在loss function中加入penalty term</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">code = Dense(code_size,activation=<span class="string">'relu'</span>)(input_img)</span><br><span class="line">code = Dense(code_size,activation=<span class="string">'relu'</span>,activity_regularizer=l1)(input_img)</span><br></pre></td></tr></table></figure><h1 id="use-cases">Use Cases</h1><p>autoencoders的压缩能力不如当前其他方法，其主要应用在：</p><p>Data denoising</p><p>Dimensionality reduction:用autoencoders高维数据进行降维，让后用t-SNE将降维数据视觉化。</p><p>Variational Autoencoders:VAE学习input的概率分布，用随机分布的数据生成input。</p><h1 id="tensorflow-code">Tensorflow Code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入MNIST数据</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"/tmp/data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#超参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_steps = <span class="number">30000</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line">display_step = <span class="number">1000</span></span><br><span class="line">examples_to_show = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#网络结构参数设置</span></span><br><span class="line">num_hidden_1 = <span class="number">256</span> </span><br><span class="line">num_hidden_2 = <span class="number">128</span> </span><br><span class="line">num_input = <span class="number">784</span> <span class="comment">#MNIST 28*28</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tf Graph 输入</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>, [<span class="literal">None</span>, num_input])</span><br><span class="line"></span><br><span class="line"><span class="comment">#网络参数</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'encoder_h1'</span>: tf.Variable(tf.random_normal([num_input, num_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_h2'</span>: tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_h1'</span>: tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_h2'</span>: tf.Variable(tf.random_normal([num_hidden_1, num_input])),</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'encoder_b1'</span>: tf.Variable(tf.random_normal([num_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_b2'</span>: tf.Variable(tf.random_normal([num_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_b1'</span>: tf.Variable(tf.random_normal([num_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_b2'</span>: tf.Variable(tf.random_normal([num_input])),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立encoder，采用sigmoid激活</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 第一层</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'encoder_h1'</span>]),</span><br><span class="line">                                   biases[<span class="string">'encoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># 第二层</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'encoder_h2'</span>]),</span><br><span class="line">                                   biases[<span class="string">'encoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立decoder，采用sigmoid激活</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 第一层 </span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'decoder_h1'</span>]),</span><br><span class="line">                                   biases[<span class="string">'decoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># 第二层</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'decoder_h2'</span>]),</span><br><span class="line">                                   biases[<span class="string">'decoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建auto模型</span></span><br><span class="line">encoder_op = encoder(X)</span><br><span class="line">decoder_op = decoder(encoder_op)</span><br><span class="line"></span><br><span class="line"><span class="comment">#预测</span></span><br><span class="line">y_pred = decoder_op</span><br><span class="line"><span class="comment"># 将输入数据作为标签</span></span><br><span class="line">y_true = X</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义loss和optimization</span></span><br><span class="line">loss = tf.reduce_mean(tf.pow(y_true - y_pred, <span class="number">2</span>))</span><br><span class="line">optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 载入批次数据 256张28*28的图像</span></span><br><span class="line">        batch_x, _ = mnist.train.next_batch(batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用loss指导optimizer</span></span><br><span class="line">        _, l = sess.run([optimizer, loss], feed_dict=&#123;X: batch_x&#125;)</span><br><span class="line">        <span class="comment"># 显示批次loss</span></span><br><span class="line">        <span class="keyword">if</span> i % display_step == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">            print(<span class="string">'Step %i: Minibatch Loss: %f'</span> % (i, l))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Testing</span></span><br><span class="line">    <span class="comment"># 用4张图来做测试</span></span><br><span class="line">    n = <span class="number">4</span></span><br><span class="line">    canvas_orig = np.empty((<span class="number">28</span> * n, <span class="number">28</span> * n))</span><br><span class="line">    canvas_recon = np.empty((<span class="number">28</span> * n, <span class="number">28</span> * n))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># MNIST 测试集</span></span><br><span class="line">        batch_x, _ = mnist.test.next_batch(n)</span><br><span class="line">        <span class="comment"># 将图像输入auto进行encode和decode</span></span><br><span class="line">        g = sess.run(decoder_op, feed_dict=&#123;X: batch_x&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 图像可视化</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 原始4张图像</span></span><br><span class="line">            canvas_orig[i * <span class="number">28</span>:(i + <span class="number">1</span>) * <span class="number">28</span>, j * <span class="number">28</span>:(j + <span class="number">1</span>) * <span class="number">28</span>] = \</span><br><span class="line">                batch_x[j].reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 重建后的4张图像</span></span><br><span class="line">            canvas_recon[i * <span class="number">28</span>:(i + <span class="number">1</span>) * <span class="number">28</span>, j * <span class="number">28</span>:(j + <span class="number">1</span>) * <span class="number">28</span>] = \</span><br><span class="line">                g[j].reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Original Images"</span>)</span><br><span class="line">    plt.figure(figsize=(n, n))</span><br><span class="line">    plt.imshow(canvas_orig, origin=<span class="string">"upper"</span>, cmap=<span class="string">"gray"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Reconstructed Images"</span>)</span><br><span class="line">    plt.figure(figsize=(n, n))</span><br><span class="line">    plt.imshow(canvas_recon, origin=<span class="string">"upper"</span>, cmap=<span class="string">"gray"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h1 id="references">references</h1><p>https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;用encoder将输入压缩成一个低维的code（输入的总结），然后用decoder将code重建成输入。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/Assets/BlogImg/autoencode.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;autoencoder的特点是：&lt;/p&gt;
&lt;p&gt;Data-specific：只能压缩训练过的或类似的数据。&lt;/p&gt;
&lt;p&gt;Lossy：输入和输出并不完全一致，不适合无损压缩问题。&lt;/p&gt;
&lt;p&gt;Unsupervised：self-supervised 其label为输入数据自身。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="autoencoder，算法" scheme="//Rocky1ee.github.io/tags/autoencoder%EF%BC%8C%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>差分进化算法</title>
    <link href="//Rocky1ee.github.io/2019/09/18/%E5%B7%AE%E5%88%86%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95DE/"/>
    <id>//Rocky1ee.github.io/2019/09/18/差分进化算法DE/</id>
    <published>2019-09-18T13:56:36.000Z</published>
    <updated>2019-12-29T12:39:16.790Z</updated>
    
    <content type="html"><![CDATA[<h1 id="references">References</h1><p>python DE：https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/</p><p>http://www.omegaxyz.com/2018/04/24/differential_evolution_intro/</p><p>https://vlight.me/2018/04/17/differential-evolution/</p><p>https://blog.csdn.net/hehainan_86/article/details/38685231</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;python DE：https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/&lt;
      
    
    </summary>
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Attention GAN Raindrop Removal</title>
    <link href="//Rocky1ee.github.io/2019/09/12/Attention%20GAN%20for%20raindrop%20removal/"/>
    <id>//Rocky1ee.github.io/2019/09/12/Attention GAN for raindrop removal/</id>
    <published>2019-09-12T13:56:36.000Z</published>
    <updated>2019-12-29T12:25:00.148Z</updated>
    
    <content type="html"><![CDATA[<p>Attentive Generative Adversarial Network for Raindrop Removal from A Single Image</p><h1 id="abstract">Abstract</h1><p>问题：1.雨点覆盖区域未给出。2.被覆盖区域下的真实信息缺失。</p><p>解决方法：在G和D中加入visual attention模块，用于学习雨滴区域及其背景。这样的话G可以更多关注于学习到的区域及其背景结构，D可以以该区域为判别真伪的重要依据。</p><h1 id="introduction">Introduction</h1><p>G由attentive-recurrent-network 和 auto-encoder构成：</p><figure><img src="/Assets/Img/attention%20gan%20rain%20drop.png" alt><figcaption>the structure of AGAN</figcaption></figure><p>attentive-recurrent network：由ResNet构造的RNN，LSTM和Conv组成，用于生成attention mapping。</p><p>contextual auto-encoder：在decoder端使用了multi-scale loss，每个loss去评估相应尺寸的feature map和ground truth pairs，用此模块获取更多纹理信息。auto-encoder用perceptual loss评估最后结果与ground truth的整体相似度。</p><a id="more"></a><h1 id="raindrop-image-formation">Raindrop Image Formation</h1><p><span class="math display">\[I=(1-M)\bigodot B+R\]</span></p><p><span class="math inline">\(I\)</span>:输入的有雨点图，<span class="math inline">\(B\)</span>:无雨点图，<span class="math inline">\(R\)</span>:被雨点影响区域 <span class="math display">\[M(x)=\begin{cases}1 \qquad if \quad 被雨点覆盖\\0 \qquad if \quad 未雨点覆盖\end{cases}\]</span></p><h1 id="raindrop-removal-using-attentive-gan">Raindrop Removal using Attentive GAN</h1><p><span class="math display">\[\min_G\max_D \Epsilon_\R \sim_{P_{clean}}[log(D(\R))]+\Epsilon_{\Iota\sim{P_{raindrop}}}[log(1-D(G(\Iota)))]\]</span></p><p><span class="math inline">\(\Iota\)</span>:输入雨图，<span class="math inline">\(\R\)</span>：输入的真实无雨图</p><h2 id="generative-network">Generative Network</h2><h3 id="attentive-recurrent-network">Attentive-Recurrent Network</h3><p>用于寻找输入图像中需要关注的区域（雨点区域及其周边信息），结构如下：</p><figure><img src="/Assets/Img/agan2.png" alt><figcaption>construction of ARN</figcaption></figure><p>每个block（time step）由5个ResNet（提取特征），LSTM和Conv（生成attention mapping）组成。</p><p>attention mapping是一个matrix，<span class="math inline">\(x in matrix,x \ in [0,1]\)</span>, 数值越大代表关注越强，从无雨点区域到有雨点区域取值不断变大。</p><p>LSTM部分？？？</p><p>产生attention mapping所采用的loss： <span class="math display">\[\mathcal{L}_{ATT}(\{A\}，M)=\sum_{t=1}^N\theta^{N-t}\mathcal{L}_{MSE}(A_t,M)\]</span> <span class="math inline">\(A_t\)</span>:attention map,其成分如下： <span class="math display">\[A_t=ATT_t(F_{t-1},H_{t-1},C_{t-1}),\\F_{t-1}:输入图像和上个time step产生attention map的组合.\\H_{t-1}:LSTM 上个time step产生的feature.\\C_{t-1}:Conv 上个time step产生的feature.\]</span> M:binary mask 由<span class="math inline">\(I-B\)</span>,然后进行Threshold得到。N:timestep总和，<span class="math inline">\(\theta\)</span>：超参数。</p><h3 id="contextual-autoencoder">Contextual Autoencoder</h3><p><img src="/Assets/Img/agan3.png" style="zoom:70%;"></p><p>Contextual Autoencoder 中包含multi-scale loss 和 perceptual loss。其中multi-scale loss是为了获取不同尺度的contextual information，其定义如下： <span class="math display">\[\mathcal{L}(\{S\},\{T\})=\sum_{i=1}^{M}\lambda_i\mathcal{L}_{MSE}(S_i,T_i)\]</span> <span class="math inline">\(S_i\)</span>：不同尺度的decoder端获取的feature map，<span class="math inline">\(T_i\)</span>是不同尺度feature map对应的ground truth，<span class="math inline">\(\lambda_i\)</span>为各尺度MSE权重。</p><p>perceptual loss 是为了获取autoencoder 的结果和相应的ground truth的全局差异。 <span class="math display">\[\mathcal{L}_P(O,T)=\mathcal{L}_{MSE}(VGG(O),VGG(T))\]</span> <span class="math inline">\(O=G(I)\)</span>：autoencoder的输出，T：ground truth</p><p>整个generative的loss为： <span class="math display">\[\mathcal{L}_G=10^{-2}\mathcal{L}_{GAN}(O)+\mathcal{L}_{ATT}({A},M)+\mathcal{L}_M(\{S\},\{T\}+\mathcal{L}_p(O,T))\\\mathcal{L}_{GAN}(O)=log(1-D(O))\]</span></p><h2 id="discriminative-network">Discriminative Network</h2><p><img src="/Assets/Img/agan4.png" alt="structure of discriminator" style="zoom:80%;"></p><p>特点是将D前面层的特征图a输入到CNN中得到结果b，将a和b相乘输入到下一层。基于b和attentive-recurrent network产生的attention map的loss定义如下： <span class="math display">\[\mathcal{L}_D(O,R,A_N)=-log(D(R))-log(1-D(O))+\gamma\mathcal{L}_{map}(O,R,A_N)\]</span> <span class="math inline">\(O\)</span>:G的输出的raindrop removal image，<span class="math inline">\(R\)</span>：ground truth image，<span class="math inline">\(A_N\)</span>:attentive-recurrent network 产生的attention map. <span class="math display">\[\mathcal{L}_{map}(O,R,A_N)=\mathcal{L}_{MSE}(D_{map}(O),A_N)+\mathcal{L}_{MSE}(D_{map}(R),0)\]</span> <span class="math inline">\(D_{map}()\)</span>:D前层提取的特征图，公式的意思是对于输入<span class="math inline">\(O\)</span>我们更关注attention 区域的 loss，对于<span class="math inline">\(R\)</span>我们没有需要特别关注的区域。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Attentive Generative Adversarial Network for Raindrop Removal from A Single Image&lt;/p&gt;
&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;问题：1.雨点覆盖区域未给出。2.被覆盖区域下的真实信息缺失。&lt;/p&gt;
&lt;p&gt;解决方法：在G和D中加入visual attention模块，用于学习雨滴区域及其背景。这样的话G可以更多关注于学习到的区域及其背景结构，D可以以该区域为判别真伪的重要依据。&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;G由attentive-recurrent-network 和 auto-encoder构成：&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/Img/attention%20gan%20rain%20drop.png&quot; alt&gt;&lt;figcaption&gt;the structure of AGAN&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;attentive-recurrent network：由ResNet构造的RNN，LSTM和Conv组成，用于生成attention mapping。&lt;/p&gt;
&lt;p&gt;contextual auto-encoder：在decoder端使用了multi-scale loss，每个loss去评估相应尺寸的feature map和ground truth pairs，用此模块获取更多纹理信息。auto-encoder用perceptual loss评估最后结果与ground truth的整体相似度。&lt;/p&gt;
    
    </summary>
    
      <category term="去雨方法" scheme="//Rocky1ee.github.io/categories/%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="Raindrop Removal" scheme="//Rocky1ee.github.io/tags/Raindrop-Removal/"/>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>蒙特卡罗算法</title>
    <link href="//Rocky1ee.github.io/2019/09/12/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E7%AE%97%E6%B3%95/"/>
    <id>//Rocky1ee.github.io/2019/09/12/蒙特卡罗算法/</id>
    <published>2019-09-12T13:56:36.000Z</published>
    <updated>2019-12-29T12:38:16.304Z</updated>
    
    <content type="html"><![CDATA[<p>蒙特卡罗算法：采样越多，越<strong>近似</strong>最优解；用有限随机数去计算估计值.</p><p>举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——<strong>尽量找好的，但不保证是最好的</strong>。</p><figure><img src="/Assets/BlogImg/圆周率估计.gif" alt><figcaption>Monte Carlo Method</figcaption></figure><p>使用蒙特卡洛方法估算π值. 放置30000个随机点后,π的估算值与真实值相差0.07%.</p><p>这张图其实很简单, 就像我们玩飞镖一样, 随机地在一个方形平面上投掷30000个飞镖, 事先我们并不知道圆周率π的值究竟是多少, 但是我们知道这里有1/4的圆, 于是我们把红色面积上的点数<span class="math inline">\(m\)</span>, 和蓝色面积上的点数<span class="math inline">\(n\)</span>, 以及圆周率<span class="math inline">\(π\)</span>的关系, 可以写出一个约等于的式子: <span class="math display">\[π ≈ 4m/(n+m)\]</span> 随着<span class="math inline">\(m+n\)</span>的投射点的逐渐增加, <span class="math inline">\(π\)</span>值的计算也越来越精确, 最后我们就估计出一个不错的比较精确的<span class="math inline">\(π\)</span>值啦</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;蒙特卡罗算法：采样越多，越&lt;strong&gt;近似&lt;/strong&gt;最优解；用有限随机数去计算估计值.&lt;/p&gt;
&lt;p&gt;举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少
      
    
    </summary>
    
      <category term="算法" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>RNN，LSTM</title>
    <link href="//Rocky1ee.github.io/2019/09/10/RNN/"/>
    <id>//Rocky1ee.github.io/2019/09/10/RNN/</id>
    <published>2019-09-10T11:56:36.000Z</published>
    <updated>2020-01-07T00:50:31.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rnn">RNN</h1><p>Recurrent Neural Networks:将神经网络中前层获得的信息传递到后层</p><figure><img src="/Assets/BlogImg/RNN.png" alt><figcaption>RNN structure</figcaption></figure><figure><img src="/Assets/BlogImg/standard_RNN.png" alt><figcaption>standard RNN</figcaption></figure><p>RNN的问题：随着层数的增长对于有用信息<span class="math inline">\(x_0,x_1\)</span>的记忆越弱</p><figure><img src="/Assets/BlogImg/RNN_problem.png" alt><figcaption>drawback of RNN</figcaption></figure><h1 id="lstm">LSTM</h1><h2 id="介绍">介绍</h2><p>Long Short Term Memory networks :可以记忆久远的的有用信息</p><p><img src="/Assets/BlogImg/structure_of_LSTM.png" alt="structure of LSTM"> <span class="math display">\[\begin{aligned}f_t&amp;=\sigma(W_f[h_{t-1},x_t]+b_f)\\i_t&amp;=\sigma(W_i[h_{t-1},x_t]+b_i)\\\widetilde{C_{t}}&amp;=tanh(W_c[h_{t-1},x_t]+b_c)\\C_t&amp;=f_t*C_{t-1}+i_t*\widetilde{C}\\o_t&amp;=\sigma(W_o[h_{t-1},x_t]+b_o)\\h_t&amp;=o_t*tanh(C_t)\end{aligned}\]</span> 主线信息贯穿整个网络</p><figure><img src="/Assets/BlogImg/main_line.png" alt><figcaption>cell state</figcaption></figure><a id="more"></a><h2 id="逐步分析">逐步分析</h2><p>forget gate layer：决定<span class="math inline">\(C_{t-1}\)</span>中的那些信息保留下来，那些信息丢掉。对输入<span class="math inline">\(x_t\)</span>和<span class="math inline">\(h_{t-1}\)</span>的组合进行sigmoid mapping为<span class="math inline">\(C_{t-1}\)</span>中的每个元素产生对应的output，output<span class="math inline">\(\in [0,1]\)</span>，0指丢弃当前信息，1指保留当前信息。</p><figure><img src="/Assets/BlogImg/forget_gate.png" alt><figcaption>forget_gate</figcaption></figure><p>input gate layer: 决定将<span class="math inline">\(\widetilde{C_t}\)</span>中的哪些信息加入到主线信息。</p><figure><img src="/Assets/BlogImg/input_gate.png" alt><figcaption>input gate</figcaption></figure><p>主线信息的更新：用<span class="math inline">\(f_t\)</span>决定主线中哪些信息保留，哪些信息丢弃。用<span class="math inline">\(i_t\)</span>决定添加哪些新信息到主线。</p><figure><img src="/Assets/BlogImg/update_cell_state.png" alt><figcaption>update cell state</figcaption></figure><p>output gate layer：决定<span class="math inline">\(C_t\)</span>主线中输出的信息。</p><figure><img src="/Assets/BlogImg/output_gate_layer.png" alt><figcaption>output gate layer</figcaption></figure><h2 id="变体">变体</h2><p>将主线信息加入到门判断中</p><figure><img src="/Assets/BlogImg/cell_state_add_to_%20all_gate.png" alt><figcaption>adding cell state to all gate</figcaption></figure><p>用coupled forget 即一个门<span class="math inline">\(f_t\)</span>,<span class="math inline">\(1-f_t\)</span>判断主线信息的更新</p><figure><img src="/Assets/BlogImg/coupled_forget.png" alt><figcaption>coupled forget</figcaption></figure><p>Gated Recurrent Unit (GRU)</p><figure><img src="/Assets/BlogImg/GRU.png" alt><figcaption>structure of GRU</figcaption></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;rnn&quot;&gt;RNN&lt;/h1&gt;
&lt;p&gt;Recurrent Neural Networks:将神经网络中前层获得的信息传递到后层&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/RNN.png&quot; alt&gt;&lt;figcaption&gt;RNN structure&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/standard_RNN.png&quot; alt&gt;&lt;figcaption&gt;standard RNN&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;RNN的问题：随着层数的增长对于有用信息&lt;span class=&quot;math inline&quot;&gt;\(x_0,x_1\)&lt;/span&gt;的记忆越弱&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/RNN_problem.png&quot; alt&gt;&lt;figcaption&gt;drawback of RNN&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&quot;lstm&quot;&gt;LSTM&lt;/h1&gt;
&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;Long Short Term Memory networks :可以记忆久远的的有用信息&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/Assets/BlogImg/structure_of_LSTM.png&quot; alt=&quot;structure of LSTM&quot;&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{aligned}
f_t&amp;amp;=\sigma(W_f[h_{t-1},x_t]+b_f)\\
i_t&amp;amp;=\sigma(W_i[h_{t-1},x_t]+b_i)\\
\widetilde{C_{t}}&amp;amp;=tanh(W_c[h_{t-1},x_t]+b_c)\\
C_t&amp;amp;=f_t*C_{t-1}+i_t*\widetilde{C}\\
o_t&amp;amp;=\sigma(W_o[h_{t-1},x_t]+b_o)\\
h_t&amp;amp;=o_t*tanh(C_t)
\end{aligned}
\]&lt;/span&gt; 主线信息贯穿整个网络&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/main_line.png&quot; alt&gt;&lt;figcaption&gt;cell state&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
      <category term="神经网络算法学习" scheme="//Rocky1ee.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="//Rocky1ee.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="//Rocky1ee.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>反卷积(Transposed Conv)和空洞卷积(Dilated conv)</title>
    <link href="//Rocky1ee.github.io/2019/09/10/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E5%92%8C%E8%86%A8%E8%83%80%E5%8D%B7%E7%A7%AF/"/>
    <id>//Rocky1ee.github.io/2019/09/10/转置卷积和膨胀卷积/</id>
    <published>2019-09-10T08:56:36.000Z</published>
    <updated>2020-01-14T08:04:15.475Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积">卷积</h1><p>假设输入图像<span class="math inline">\(input,size = 4 x 4\)</span>,表示如下： $$ <span class="math display">\[\begin{equation}input = \begin{bmatrix} x_1&amp; x_2 &amp; x_3 &amp;x_4 \\  x_5&amp; x_6 &amp; x_7 &amp;x_8 \\  x_9&amp; x_{10} &amp; x_{11} &amp; x_{12}\\  x_{13}&amp; x_{14} &amp; x_{15} &amp; x_{16}\end{bmatrix}\end{equation}\]</span> <span class="math display">\[$kernel, size =3 x 3$,表示如下：\]</span> \begin{bmatrix} w_{0,0}&amp; w_{0,1} &amp; w_{0,2} \ w_{1,0}&amp; w_{1,1} &amp; w_{1,2} \ w_{2,0}&amp; w_{2,1} &amp; w_{2,2} \end{bmatrix} $$ 按照卷积公式<span class="math inline">\(o=\frac{i+2p-k}{s}+1\)</span>得出输出特征图<span class="math inline">\(output size = 2 x 2\)</span></p><p>卷积过程用矩阵乘法描述为<span class="math inline">\(Y=CX\)</span> <span class="math display">\[X = [x_1,x_2,x_3,x_4, x_5,x_6,x_7,x_8,  x_9,x_{10},x_{11},x_{12}, x_{13},x_{14},x_{15},x_{16}]^T\]</span></p><p><span class="math display">\[C = \begin{bmatrix} w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}  &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0 \\ 0 &amp;w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}  &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\ 0 &amp;0 &amp;0 &amp;0 &amp;w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}  &amp; 0   \\  0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;w_{0,0}&amp;w_{0,1}  &amp;w_{0,2}  &amp;0  &amp;w_{1,0}  &amp;w_{1,1}  &amp;w_{1,2}  &amp;0  &amp;w_{2,0}  &amp;w_{2,1}  &amp;w_{2,2}   \end{bmatrix}\]</span></p><figure><img src="/Assets/BlogImg/conv.gif" alt><figcaption>convelution</figcaption></figure><a id="more"></a><h1 id="反卷积">反卷积</h1><p>反卷积的操作就是要对这个矩阵运算过程进行逆运算，即通过 <span class="math inline">\(C\)</span>和<span class="math inline">\(Y\)</span>得到<span class="math inline">\(X\)</span>,反卷积操作可以表示为：</p><p>$X=C^TY&amp; <span class="math display">\[Y=\begin{bmatrix}y_1\\ y_2\\ y_3\\ y_4\end{bmatrix}\]</span></p><p><span class="math display">\[C^T=\begin{bmatrix} w_{0,0}&amp;0  &amp; 0 &amp; 0\\  w_{0,1}&amp;w_{0,0}  &amp;0  &amp; 0\\  w_{0,2}&amp;w_{0,1}  &amp; 0 &amp; 0\\  0&amp; w_{0,2} &amp; 0 &amp; 0\\  w_{1,0}&amp; 0 &amp;w_{0,0}  &amp; 0\\  w_{1,1}&amp; w_{1,0} &amp;w_{0,1}  &amp; w_{0,0}\\  w_{1,2}&amp;w_{1,1}  &amp; w_{0,2} &amp;w_{0,0} \\  0&amp;w_{1,2}  &amp;0  &amp;w_{2,1} \\  w_{2,0}&amp; 0 &amp; w_{1,0} &amp; 0\\  w_{2,1}&amp; w_{2,0} &amp;w_{1,1}  &amp;w_{1,0} \\  w_{2,2}&amp; w_{2,1} &amp; w_{1,2} &amp;w_{1,1} \\ 0&amp; w_{2,2} &amp; 0 &amp; w_{1,2}\\  0&amp; 0 &amp; w_{2,0} &amp; 0\\  0&amp;0  &amp;w_{2,1}  &amp; w_{2,0}\\  0&amp;0  &amp;w_{2,2}  &amp;w_{2,1} \\   0&amp; 0 &amp;0  &amp; w_{2,2}\end{bmatrix}\]</span></p>反卷积公式为： $$ o=<span class="math display">\[\begin{cases}s(i-1)-2p+k,\quad &amp;if \quad(o+2p-k)\%s=0 \\s(i-1)-2p+k+(o+2p-k)\%s,\quad &amp;if \quad (o+2p-k)\%s\neq0\end{cases}\]</span><p><span class="math display">\[代码中的stride的计算方法为：\]</span> \begin{cases} stride=output_{size}(input_{size}-1),&amp;if padding为SAME \ stride=（output_{size}-kernel + 1）(input_{size}-1),&amp;if padding为VALID</p><p>\end{cases} $$ <img src="/Assets/BlogImg/Tconv.gif" alt="Transform Convelution"></p><h1 id="空洞卷积">空洞卷积</h1><figure><img src="/Assets/BlogImg/dilated.gif" alt><figcaption>dilated conv</figcaption></figure><figure><img src="/Assets/BlogImg/dilated%20conv.jpg" alt><figcaption>论文实例</figcaption></figure><p>(a):3x3,1-dilated conv,其效果和普通卷积一样。</p><p>(b):3x3,2dilated conv,其效果为非红点pixel取0的7x7普通卷积。</p><p>(c):3x3,4dilated conv,其效果为非红点pixel取0的15x15普通卷积</p><p>优点：在不做pooling损失信息和相同的计算条件下的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。空洞卷积经常用在实时图像分割中。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑空洞卷积。</p><h1 id="references">references</h1><p>https://www.cnblogs.com/hellcat/p/9687624.html</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;卷积&quot;&gt;卷积&lt;/h1&gt;
&lt;p&gt;假设输入图像&lt;span class=&quot;math inline&quot;&gt;\(input,size = 4 x 4\)&lt;/span&gt;,表示如下： $$ &lt;span class=&quot;math display&quot;&gt;\[\begin{equation}
input = 
\begin{bmatrix}
 x_1&amp;amp; x_2 &amp;amp; x_3 &amp;amp;x_4 \\ 
 x_5&amp;amp; x_6 &amp;amp; x_7 &amp;amp;x_8 \\ 
 x_9&amp;amp; x_{10} &amp;amp; x_{11} &amp;amp; x_{12}\\ 
 x_{13}&amp;amp; x_{14} &amp;amp; x_{15} &amp;amp; x_{16}
\end{bmatrix}

\end{equation}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
$kernel, size =3 x 3$,表示如下：
\]&lt;/span&gt; \begin{bmatrix} w_{0,0}&amp;amp; w_{0,1} &amp;amp; w_{0,2} \ w_{1,0}&amp;amp; w_{1,1} &amp;amp; w_{1,2} \ w_{2,0}&amp;amp; w_{2,1} &amp;amp; w_{2,2} \end{bmatrix} $$ 按照卷积公式&lt;span class=&quot;math inline&quot;&gt;\(o=\frac{i+2p-k}{s}+1\)&lt;/span&gt;得出输出特征图&lt;span class=&quot;math inline&quot;&gt;\(output size = 2 x 2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;卷积过程用矩阵乘法描述为&lt;span class=&quot;math inline&quot;&gt;\(Y=CX\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
X = [x_1,x_2,x_3,x_4,
 x_5,x_6,x_7,x_8, 
 x_9,x_{10},x_{11},x_{12},
 x_{13},x_{14},x_{15},x_{16}]^T
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
C = \begin{bmatrix}
 w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp;0 \\ 
0 &amp;amp;w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  \\ 
0 &amp;amp;0 &amp;amp;0 &amp;amp;0 &amp;amp;w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}  &amp;amp; 0   \\ 
 0 &amp;amp;0 &amp;amp;0 &amp;amp;0 &amp;amp;0 &amp;amp;w_{0,0}&amp;amp;w_{0,1}  &amp;amp;w_{0,2}  &amp;amp;0  &amp;amp;w_{1,0}  &amp;amp;w_{1,1}  &amp;amp;w_{1,2}  &amp;amp;0  &amp;amp;w_{2,0}  &amp;amp;w_{2,1}  &amp;amp;w_{2,2}   
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/conv.gif&quot; alt&gt;&lt;figcaption&gt;convelution&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="upsampling，卷积" scheme="//Rocky1ee.github.io/tags/upsampling%EF%BC%8C%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>模拟退火算法</title>
    <link href="//Rocky1ee.github.io/2019/09/06/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95SA/"/>
    <id>//Rocky1ee.github.io/2019/09/06/模拟退火算法SA/</id>
    <published>2019-09-06T11:56:36.000Z</published>
    <updated>2019-12-10T07:43:48.033Z</updated>
    
    <content type="html"><![CDATA[<p>模拟退火算法(Simulated Annealing)：采用类似于物理退火的过程，先在一个高温状态下（相当于算法随机搜索），然后逐渐退火，在每个温度下（相当于算法的每一次状态转移）徐徐冷却（相当于算法局部搜索），最终达到物理基态（相当于算法找到最优解）。</p><h1 id="算法思想">算法思想</h1><h2 id="爬山算法">爬山算法</h2><p><strong>爬山算法</strong>是一种简单的<strong>贪心搜索算法</strong>，它每次都<strong>鼠目寸光</strong>地从当前解临近的解空间中选择一个更优解代替当前解，直到得到一个局部最优解。</p><p>如图1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解后就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。因此，爬山算法只能搜索到局部最优值，而不一定能搜索到全局最优解。</p><figure><img src="/Assets/BlogImg/SA.jpg" alt><figcaption>图 1</figcaption></figure><h2 id="模拟退火算法">模拟退火算法</h2><p>模拟退火算法其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法<strong>以一定的概率</strong>来接受一个比当前解要差的解，因此<strong>有可能</strong>会跳出这个局部的最优解，达到全局的最优解。</p><p>以图1为例，模拟退火算法在搜索到局部最优解A后，会<strong>以一定的概率</strong>向E的移动。可能经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。模拟退火算法是一种随机算法，并不一定能找到全局的最优解，但可以比较快的找到问题的近似最优解。</p><a id="more"></a><h1 id="算法流程">算法流程</h1><p><img src="/Assets/BlogImg/SA2.png" alt="The process of SA" style="zoom:80%;"></p><h2 id="初始化">初始化</h2><p>模拟退火时需设定三个参数：一个比较大初始温度<span class="math inline">\(T_0\)</span> ，降温系数<span class="math inline">\(d\)</span> 非常接近但小于<span class="math inline">\(1\)</span>，终止温度 <span class="math inline">\(T_k\)</span>是一个接近<span class="math inline">\(0\)</span>的正数。</p><h2 id="metropolis准则">Metropolis准则</h2><p>解的替换按照Metropolis准则进行，假设前一状态为 <span class="math inline">\(f(x_n)\)</span>，系统受到一定扰动，状态变为 <span class="math inline">\(f(x_{n+1})\)</span>，相应地，系统能量由 <span class="math inline">\(E(x_n)\)</span> 变为 <span class="math inline">\(E(x_{n+1})\)</span>。 定义系统由 <span class="math inline">\(f(x_n)\)</span> 变为$f(x_{n+1}) $的接收概率为 <span class="math inline">\(p\)</span>（probability of acceptance）： $$ <span class="math display">\[\begin{equation}p=\begin{cases}1, \quad &amp;if \quad E(x_{n+1})&lt;E(x_n)\\e^{-\frac{E(x_{n+1})-E(x_n)}{T}} \quad &amp;if \quad E(x_{n+1})\geq E(x_n)\end{cases}\end{equation}\]</span> $$</p><p>通俗解释为：</p><ol type="1"><li>在高温下可接受与当前状态能量差较大的新状态；</li><li>在低温下基本只接受与当前能量差较小的新状态；</li><li>当温度趋于零时，就不能接受比当前状态能量高的新状态。</li></ol><p>随着温度的降低，对随机性的接受度降低。</p><h2 id="迭代更新">迭代更新</h2><p>首先让温度 <span class="math inline">\(T=T_0\)</span>，然后按照上述Metroplis准则对解进行更新，再让<span class="math inline">\(T=d \times T\)</span>。当<span class="math inline">\(T&lt;T_k\)</span>时模拟退火过程结束，当前最优解即为最终的最优解。</p><p>直观表示如下图：随着温度的降低，跳跃越来越不随机，最优解也越来越稳定。</p><figure><img src="/Assets/BlogImg/SA3.gif" alt><figcaption>退火算法求解过程</figcaption></figure><h1 id="算法实例">算法实例</h1><h1 id="references">references</h1><p>OI Wiki 习题https://oi-wiki.org/misc/simulated-annealing/</p><p>科学网 http://blog.sciencenet.cn/blog-1813407-893984.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;模拟退火算法(Simulated Annealing)：采用类似于物理退火的过程，先在一个高温状态下（相当于算法随机搜索），然后逐渐退火，在每个温度下（相当于算法的每一次状态转移）徐徐冷却（相当于算法局部搜索），最终达到物理基态（相当于算法找到最优解）。&lt;/p&gt;
&lt;h1 id=&quot;算法思想&quot;&gt;算法思想&lt;/h1&gt;
&lt;h2 id=&quot;爬山算法&quot;&gt;爬山算法&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;爬山算法&lt;/strong&gt;是一种简单的&lt;strong&gt;贪心搜索算法&lt;/strong&gt;，它每次都&lt;strong&gt;鼠目寸光&lt;/strong&gt;地从当前解临近的解空间中选择一个更优解代替当前解，直到得到一个局部最优解。&lt;/p&gt;
&lt;p&gt;如图1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解后就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。因此，爬山算法只能搜索到局部最优值，而不一定能搜索到全局最优解。&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/SA.jpg&quot; alt&gt;&lt;figcaption&gt;图 1&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;模拟退火算法&quot;&gt;模拟退火算法&lt;/h2&gt;
&lt;p&gt;模拟退火算法其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法&lt;strong&gt;以一定的概率&lt;/strong&gt;来接受一个比当前解要差的解，因此&lt;strong&gt;有可能&lt;/strong&gt;会跳出这个局部的最优解，达到全局的最优解。&lt;/p&gt;
&lt;p&gt;以图1为例，模拟退火算法在搜索到局部最优解A后，会&lt;strong&gt;以一定的概率&lt;/strong&gt;向E的移动。可能经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。模拟退火算法是一种随机算法，并不一定能找到全局的最优解，但可以比较快的找到问题的近似最优解。&lt;/p&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="模拟退火算法" scheme="//Rocky1ee.github.io/tags/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>遗传算法</title>
    <link href="//Rocky1ee.github.io/2019/09/04/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95GA/"/>
    <id>//Rocky1ee.github.io/2019/09/04/遗传算法GA/</id>
    <published>2019-09-04T09:56:36.000Z</published>
    <updated>2019-12-10T07:45:30.478Z</updated>
    
    <content type="html"><![CDATA[<p>遗传算法（Genetic Algorithm）</p><h1 id="算法背景">算法背景</h1><p><strong>种群(Population)</strong>：生物的进化以群体的形式进行，这样的一个群体称为种群。</p><p><strong>个体</strong>：组成种群的单个生物。</p><p><strong>染色体 ( Chromosome )</strong> ：包含一组的基因。</p><p><strong>基因 ( Gene )</strong> ：一个遗传因子。</p><p><strong>生存竞争，适者生存</strong>：对环境适应度高的、优良的个体参与繁殖的机会比较多，后代就会越来越多。适应度低的个体参与繁殖的机会比较少，后代就会越来越少。</p><p><strong>选择</strong>：根据相应策略选择父母染色体对进行繁殖。</p><p><strong>交叉</strong>：将父母双方各一部分的基因进行组合形成子代染色体。</p><p><strong>变异</strong>：子代染色体有一定的概率发生基因变异。</p><p>简单说来就是：选择优良父代进行繁殖，期间会发生基因交叉( Crossover ) ，基因突变 ( Mutation ) ，适应度( Fitness )低的个体会被逐步淘汰，而适应度高的个体会越来越多。那么经过N代的自然选择后，保存下来的个体都是适应度很高的，其中很可能包含史上产生的适应度最高的个体。</p><h1 id="算法流程">算法流程</h1><h2 id="总体流程">总体流程</h2><figure><img src="/Assets/BlogImg/GA1.jpg" alt><figcaption>The process of GA</figcaption></figure><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">begin</span><br><span class="line">       initialize P(0);      </span><br><span class="line">       t = 0;             //t是进化的代数，一代、二代、三代...</span><br><span class="line"></span><br><span class="line">       while(t &lt;= T) do</span><br><span class="line">              for i = 1 to M  do     //M是初始种群的个体数</span><br><span class="line">                     Evaluate fitness of P(t);  //计算P（t）中各个个体的适应度</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M  do</span><br><span class="line">                     Select operation to P(t);  //将选择算子作用于群体</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M/2  do</span><br><span class="line">                     Crossover operation to P(t); //将交叉算子作用于群体</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M  do</span><br><span class="line">                     Mutation operation to P(t);  //将变异算子作用于群体</span><br><span class="line">              end for</span><br><span class="line"></span><br><span class="line">              for i = 1 to M  do</span><br><span class="line">                     P(t+1) = P(t);      //得到下一代群体P（t + 1）</span><br><span class="line">              end for</span><br><span class="line">              </span><br><span class="line">              t = t + 1;      //终止条件判断  t≦T：t← t+1 转到步骤2</span><br><span class="line"></span><br><span class="line">       end while</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="编码">编码</h2><p>遗传算法需要将问题的解(个体)编码成字符串的形式。最简单的一种编码方式是二进制编码，</p><p>即<span class="math inline">\(24=11000\)</span>。</p><h2 id="适应度计算">适应度计算</h2><p>适应度函数 ( Fitness Function )：用于评价染色体的适应度，用<span class="math inline">\(f(x)\)</span>表示。<strong>适应度函数</strong>与<strong>目标函数</strong>是正相关的，可对目标函数作一些变形来得到适应度函数。有时需要区分染色体的适应度函数与问题的目标函数。例如：0-1背包问题的目标函数是所取得物品价值，但将物品价值作为染色体的适应度函数可能并不一定适合。</p><h2 id="选择">选择</h2><p>使用选择运算子对个体进行优胜劣汰操作。适应度高的个体被遗传到下一代群体中的概率大；适应度低的个体，被遗传到下一代群体中的概率小。目标就是从父代群体中选取优个体，遗传到下一代群体。</p><p>常用算法为轮盘赌法： <span class="math display">\[P(x_i)=\frac{f(x_i)}{\sum\limits_{j=1}^Nf(x_j)}\qquad(1)\]</span> <span class="math inline">\(x_i\)</span>:个体<span class="math inline">\(i，i \in[1,N]\)</span>,<span class="math inline">\(f()\)</span>:适应度函数,<span class="math inline">\(P(x_i)\)</span>:选择个体<span class="math inline">\(i\)</span>的概率。 <span class="math display">\[q_i = \sum\limits_{j=1}^iP(x_j)\qquad (2)\]</span> <span class="math inline">\(q_i\)</span>为累积概率。</p><figure><img src="/Assets/BlogImg/轮盘赌选择法.jpeg" alt><figcaption>轮盘赌法</figcaption></figure><p>生成随机数<span class="math inline">\(r_k \in[0,1],k =1,2,3...N\)</span>,<span class="math inline">\(N\)</span>为选择个体总数。假设<span class="math inline">\(r_1=0.4\)</span>,则选择<span class="math inline">\(个体3\)</span>,即选择<span class="math inline">\(q_i \geq r_1\)</span>的第首个个体。</p><h2 id="交叉">交叉</h2><p>染色体交叉是以一定的概率发生的，这个概率记为$P_c $。交叉的方法有很多种，这里用的是单点交叉</p><p>交叉前：</p><p>00000|<strong>011100000000</strong>|10000</p><p>11100|<strong>000001111110</strong>|00101</p><p>交叉后：</p><p>00000|<strong>000001111110</strong>|10000</p><p>11100|<strong>011100000000</strong>|00101</p><h2 id="变异">变异</h2><p>在繁殖过程，新产生的染色体中的基因会以一定的概率出错，称为变异。变异发生的概率记为&amp;P_m&amp;</p><p>变异前：</p><p>000001110000<strong>0</strong>00010000</p><p>变异后：</p><p>000001110000<strong>1</strong>00010000</p><h1 id="算法实例">算法实例</h1><p>已知<span class="math inline">\(x\)</span>为整数，利用遗传算法求函数<span class="math inline">\(y=x^2\)</span>区间<span class="math inline">\([0, 31]\)</span>上的的最大值。</p><h2 id="初始化">初始化</h2><p>将种群规模设定为<span class="math inline">\(M=4\)</span>；用5位二进制数编码染色体；取下列个体组成初始种群<span class="math inline">\(S_1\)</span> <span class="math display">\[s_1= 13 (01101),  s_2= 24 (11000)\\s_3= 8 (01000),    s_4= 19 (10011）\]</span></p><h2 id="适应度计算-1">适应度计算</h2><p>适应度函数定义为<span class="math inline">\(f(x)=x^2\)</span>,由此计算各染色体的适应度： <span class="math display">\[\begin{split}&amp;f (s_1) = f(13) = 13^2 = 169\\&amp;f (s_2) = f(24) = 24^2 = 576\\&amp;f (s_3) = f(8) = 8^2 = 64\\&amp;f (s_4) = f(19) = 19^2 = 361\end{split}\]</span> ## 选择</p><p>再由公式(1)计算<span class="math inline">\(S_1\)</span>中各个体的选择概率： <span class="math display">\[\begin{split}&amp;P(s_1) = P(13) = 0.14\\&amp;P(s_2) = P(24) = 0.49\\&amp;P(s_3) = P(8) = 0.06\\&amp;P(s_4) = P(19) = 0.31\end{split}\]</span> 再由公式(2)计算<span class="math inline">\(S_1\)</span>中各个体的累计概率： <span class="math display">\[\begin{split}&amp;q(s_1) = q(13) = 0.14\\&amp;q(s_2) = q(24) = 0.63\\&amp;q(s_3) = q(8) = 0.69\\&amp;q(s_4) = q(19) = 1\end{split}\]</span> 假设从<span class="math inline">\([0,1]\)</span>中产生的<span class="math inline">\(N=4\)</span>个随机数如下： <span class="math display">\[\begin{split}&amp;r1 = 0.450126,\\     &amp;r2 = 0.110347\\&amp;r3 = 0.572496, \\    &amp;r4 = 0.98503\end{split}\]</span> 选择结果为：</p><table><thead><tr class="header"><th>个体</th><th>染色体</th><th>适应度</th><th>选择概率</th><th>累积概率</th><th>选择次数</th></tr></thead><tbody><tr class="odd"><td>13</td><td><span class="math inline">\(s_1=01101\)</span></td><td>169</td><td>0.14</td><td>0.14</td><td>1</td></tr><tr class="even"><td>24</td><td><span class="math inline">\(s_2=11000\)</span></td><td>576</td><td>0.49</td><td>0.63</td><td>2</td></tr><tr class="odd"><td>8</td><td><span class="math inline">\(s_3=01000\)</span></td><td>64</td><td>0.06</td><td>0.69</td><td>0</td></tr><tr class="even"><td>19</td><td><span class="math inline">\(s_4=10011\)</span></td><td>361</td><td>0.31</td><td>1</td><td>1</td></tr></tbody></table><h2 id="交叉-1">交叉</h2><p>设交叉率<span class="math inline">\(p_c\)</span>=100%，即<span class="math inline">\(S_1\)</span>中的全体染色体都参加交叉运算,交叉点位: | 。 设<span class="math inline">\(s_1’与s_2’\)</span>配对，<span class="math inline">\(s_3’与s_4’\)</span>配对。 <span class="math display">\[s_1’ =110|00（24）,  s_2’ =011|01（13）s_3’ =110|00（24）,  s_4’ =100|11（19）\]</span> 分别交换后两位基因，得新染色体： <span class="math display">\[s_1’’=11001（25）,  s_2’’=01100（12）s_3’’=11011（27）,  s_4’’=10000（16）\]</span></p><h2 id="变异-1">变异</h2><p>设变异率<span class="math inline">\(p_m=0.001\)</span>。这样，群体<span class="math inline">\(S1\)</span>中共有<span class="math inline">\(5×4×0.001=0.02\)</span>位基因可以变异。<span class="math inline">\(0.02\)</span>位显然不足1位，所以本轮遗传操作不做变异。</p><p>于是，得到第二代种群S2： <span class="math display">\[s_1=11001（25）,  s_2=01100（12）s_3=11011（27）,  s_4=10000（16）\]</span></p><h2 id="迭代更替">迭代更替</h2><table><thead><tr class="header"><th>个体</th><th>染色体</th><th>适应度</th><th>选择概率</th><th>累积概率</th><th>选择次数</th></tr></thead><tbody><tr class="odd"><td>25</td><td><span class="math inline">\(s_1=11001\)</span></td><td>625</td><td>0.36</td><td>0.36</td><td>1</td></tr><tr class="even"><td>12</td><td><span class="math inline">\(s_2=01100\)</span></td><td>144</td><td>0.07</td><td>0.44</td><td>0</td></tr><tr class="odd"><td>27</td><td><span class="math inline">\(s_3=11011\)</span></td><td>729</td><td>0.41</td><td>0.85</td><td>2</td></tr><tr class="even"><td>256</td><td><span class="math inline">\(s_4=10000\)</span></td><td>256</td><td>0.15</td><td>1.00</td><td>1</td></tr></tbody></table><p>假设这一轮遗传操作中，种群S2中的4个染色体都被选中，则得到群体： <span class="math display">\[ s_1’=11|001（25）,  s_2’=11|011（27）， s_3’=11|011（27）,  s_4’= 10|000（16）\]</span> 做交叉运算，让s1’与s2’，s3’与s4’ 分别交叉，得 <span class="math display">\[s1’’=11011（27）,    s2’’ = 11001（25）,  s3’’ =11000（24）,   s4’’ = 10011（19）\]</span> 这一轮仍然不会发生变异。 于是，得第三代种群S3： <span class="math display">\[s1=11011（27）,    s2 = 11001（25）,  s3 =11000（24）,   s4= 10011（19）\]</span> 迭代到第5代可以得到最优解<span class="math inline">\(s^*=11111(31),f(s^*)=961\)</span>。</p><h2 id="结束">结束</h2><p>按照以上流程不断迭代更替直到迭代次数超出预设值或目标函数的最优值不再变化。</p><h1 id="references">references</h1><p>JULY大神 https://blog.csdn.net/v_JULY_v/article/details/6132775</p><p>Michael翔 https://michael728.github.io/2015/12/24/algorithm-GA-basic/</p><!--more-->]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遗传算法（Genetic Algorithm）&lt;/p&gt;
&lt;h1 id=&quot;算法背景&quot;&gt;算法背景&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;种群(Population)&lt;/strong&gt;：生物的进化以群体的形式进行，这样的一个群体称为种群。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;个体&lt;/strong&gt;：组成种群的单个生物。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;染色体 ( Chromosome )&lt;/strong&gt; ：包含一组的基因。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基因 ( Gene )&lt;/strong&gt; ：一个遗传因子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;生存竞争，适者生存&lt;/strong&gt;：对环境适应度高的、优良的个体参与繁殖的机会比较多，后代就会越来越多。适应度低的个体参与繁殖的机会比较少，后代就会越来越少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;选择&lt;/strong&gt;：根据相应策略选择父母染色体对进行繁殖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;交叉&lt;/strong&gt;：将父母双方各一部分的基因进行组合形成子代染色体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;变异&lt;/strong&gt;：子代染色体有一定的概率发生基因变异。&lt;/p&gt;
&lt;p&gt;简单说来就是：选择优良父代进行繁殖，期间会发生基因交叉( Crossover ) ，基因突变 ( Mutation ) ，适应度( Fitness )低的个体会被逐步淘汰，而适应度高的个体会越来越多。那么经过N代的自然选择后，保存下来的个体都是适应度很高的，其中很可能包含史上产生的适应度最高的个体。&lt;/p&gt;
&lt;h1 id=&quot;算法流程&quot;&gt;算法流程&lt;/h1&gt;
&lt;h2 id=&quot;总体流程&quot;&gt;总体流程&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/GA1.jpg&quot; alt&gt;&lt;figcaption&gt;The process of GA&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="进化算法" scheme="//Rocky1ee.github.io/tags/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>简化粒子群</title>
    <link href="//Rocky1ee.github.io/2019/09/03/%E7%AE%80%E5%8C%96%E7%BE%A4%E4%BC%98%E5%8C%96SSO/"/>
    <id>//Rocky1ee.github.io/2019/09/03/简化群优化SSO/</id>
    <published>2019-09-03T07:56:36.000Z</published>
    <updated>2019-12-10T05:54:19.335Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>Swarm-based optimization tend to suffer from premature convergence in the high dimensional problem space. thus，proposing simplified swarm optimization (SSO) algorithm to overcome the above convergence problem by incorporating it with the new <strong>local search strategy</strong> named ELS(Exchange Local Search).</p><h1 id="introduction">Introduction</h1><p>The ELS strategy is introduced to find a better solution from the neighbourhood of the current solution which is produced by SSO. It allows the particles to better explore the search space, and preserves swarm diversity which is important in preventing premature convergence of the particles.</p><a id="more"></a><h1 id="particle-swarm-optimization-and-simplified-swarm-optimization.">Particle Swarm Optimization and Simplified Swarm Optimization.</h1><h2 id="pso">PSO</h2><p><span class="math display">\[v_{id}^t = \omega\cdot v_{id}^{t-1}+c_1\cdot rand_1\cdot (p_{id}-x_{id}^{t-1})+c_2\cdot rand_2\cdot(p_{gd}-x_{id}^{t-1}) \quad (1)\]</span></p><p><span class="math display">\[x_{id}^t = x_{id}^{t-1}+v_{id}^t \quad (2)\]</span></p><p>The algorithm of the standard PSO is presented as follows:</p><ol type="1"><li>Initialize a population of particles with random positions and velocities.</li><li>Evaluate the fitness value of each particle in the population.</li><li>Get the <span class="math inline">\(pbest\)</span> value. If the fitness value of the particle <span class="math inline">\(i\)</span> is better than its <span class="math inline">\(pbest\)</span> fitness value, and then set the fitness value as a new <span class="math inline">\(pbest\)</span> of particle <span class="math inline">\(i\)</span>.</li><li>Get the <span class="math inline">\(gbest\)</span> value. If any <span class="math inline">\(pbest\)</span> is updated and it is better than the current <span class="math inline">\(gbest\)</span>, and then set <span class="math inline">\(gbest\)</span> to the current <span class="math inline">\(pbest\)</span> value of particle <span class="math inline">\(i\)</span>.</li><li>Update particle’s velocity and position according to (1) and (2).</li><li>Stop iteration if the best fitness value or the maximum generation is met; otherwise go back to step 2.</li></ol><h2 id="sso">SSO</h2><p>In every generation, the particle’s position value in each dimension will be kept or be updated by its <span class="math inline">\(pbest\)</span> value or by the <span class="math inline">\(gbest\)</span> value or be replaced by new random value according to this procedure. <span class="math display">\[x_{id} = \begin{cases}x_{id}^{t-1},\quad if\quad rand() \in[0,C_\omega]&amp;\\p_{id}^{t-1},\quad if\quad rand() \in[C_\omega,C_p]&amp;\\g_{id}^{t-1},\quad if\quad rand() \in[C_p,C_g]&amp;\\x ,\qquad if\quad rand()\in[C_g,1]&amp;\\\end{cases}\quad (3)\]</span> <span class="math inline">\(C_w\)</span> , <span class="math inline">\(C_p\)</span> and <span class="math inline">\(C_g\)</span> are three predetermined positive constants with <span class="math inline">\(C_w &lt; C_p &lt; C_g\)</span> .</p><figure><img src="/Assets/BlogImg/SSO1.png" alt><figcaption>Flowchart of SSO algorithm</figcaption></figure><p><strong>根据随机概率对粒子的位置进行更新。从而增强模型的随机性，防止提前收敛。</strong></p><h1 id="the-proposed-sso-els-data-mining-algorithm">The Proposed SSO-ELS Data Mining Algorithm</h1><p>SSO-ELS:Simplified Swarm Optimization (SSO) with Exchange Local Search scheme.</p><h2 id="the-rule-mining-encoding">The rule mining encoding</h2><p><span class="math display">\[LowerBound=x-rand()*(max(X_i)-min(X_i))\quad (4)\]</span></p><p><span class="math display">\[UpperBound=x+rand()*(max(X_i)-min(X_i))\quad (5)\]</span></p><p><strong>$ (max(X_i ) − min(X_i ))$ is the range value of the data source in each attribute.???</strong></p><h2 id="rule-evaluation">Rule evaluation</h2><p>P,N: What’s your judgement about the sample?</p><p>T,F: Is your judgement right(True) or not(False)?</p><figure><img src="/Assets/BlogImg/ROC1.png" alt><figcaption>Confusion matrix</figcaption></figure><p>Evaluation index:</p><p><img src="/Assets/BlogImg/ROC2.png" alt="evaluation index-w60"> <span class="math display">\[The \quad rule\quad quality = sensitivety\times specifigity=\frac{TP}{TP+FN}\times \frac{TN}{TN+FP}\]</span></p><h2 id="rule-pruning">Rule pruning</h2><p><span class="math display">\[Prediction\quad value +=a*rule\quad quality +\beta*percentage\quad of\quad the \quad rule \quad covered\]</span></p><h2 id="the-proposed-exchange-local-search-strategy">The proposed exchange local search strategy</h2><p>The principle of the ELS applied in SSO and PSO rule mining is to exchange the lower bound and upper bound value for one selected attribute from the neighbour particle, re-evaluate the fitness value of the target particle, and then try to find out the new <span class="math inline">\(pbest\)</span> of the target particle or new <span class="math inline">\(gbest\)</span> in the swarm.</p><p>Step 1 Pre-determine local search time <span class="math inline">\((T)\)</span> which will only be used for <span class="math inline">\(gbest\)</span>.</p><p>Step 2 Choose a target particle$ (P_t )$. In this phase, <span class="math inline">\(gbest\)</span> will be the first target particle to be run in <span class="math inline">\(T\)</span> times’ of local search. Later, the other <span class="math inline">\(pbest\)</span> will be sequentially selected as target particles and they will only be run once in local search.</p><p>Step 3 Randomly select one attribute from the rule set in the dataset. This process is called exchangeAttribute.</p><p>Step 4 Randomly select two different neighbour particles, <span class="math inline">\(P_x\)</span> and <span class="math inline">\(P_y\)</span> from the population.</p><p>Step 5 Get a <span class="math inline">\(LowerBound(x)\)</span> of the selected attribute (selected from Step 3) of $P_x $, and get an <span class="math inline">\(UpperBound(y)\)</span> of the selected attribute of <span class="math inline">\(P_y\)</span> .</p><p>Step 6 Temporarily replace the corresponding <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> of the tar- get particle with <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p><p>Step 7 Re-evaluate the fitness value of the target particle.</p><p>Step 8 Check whether the fitness value is better than the current <span class="math inline">\(pbest\)</span> of the target particle or better than <span class="math inline">\(gbest\)</span>. If it is better, <span class="math inline">\(pbest\)</span> and <span class="math inline">\(gbest\)</span> will be updated,and the exchange value for the target particle will be kept; otherwise, the original <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> values will be positioned back to the target particle.</p><figure><img src="/Assets/BlogImg/ELS.png" alt><figcaption>The process of the ELS</figcaption></figure><figure><img src="/Assets/BlogImg/exchange%20Attribute.png" alt><figcaption>The UpperBound and LowerBound exchange strategy for one selected attribute in each generation (note: UB = UpperBound, LB = LowerBound)</figcaption></figure><h1 id="conclusions">Conclusions</h1><p>The main idea of exchange local search is to improve and refine the searching process by exchanging the <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> for one selected attribute into the <span class="math inline">\(LowerBound\)</span> and <span class="math inline">\(UpperBound\)</span> of the corresponding target particle.</p><p>reference</p><p>precision and recall,ROC,AUC</p><p>https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c</p><p>A new simplified swarm optimization (SSO) using exchange local search scheme</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Swarm-based optimization tend to suffer from premature convergence in the high dimensional problem space. thus，proposing simplified swarm optimization (SSO) algorithm to overcome the above convergence problem by incorporating it with the new &lt;strong&gt;local search strategy&lt;/strong&gt; named ELS(Exchange Local Search).&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The ELS strategy is introduced to find a better solution from the neighbourhood of the current solution which is produced by SSO. It allows the particles to better explore the search space, and preserves swarm diversity which is important in preventing premature convergence of the particles.&lt;/p&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="粒子群算法" scheme="//Rocky1ee.github.io/tags/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>梯度消失与梯度爆炸</title>
    <link href="//Rocky1ee.github.io/2019/09/01/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"/>
    <id>//Rocky1ee.github.io/2019/09/01/梯度消失与梯度爆炸/</id>
    <published>2019-09-01T08:56:36.000Z</published>
    <updated>2020-04-18T07:22:09.736Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度消失">梯度消失</h1><p>以全连接神经网络为例子：</p><p><img src="/Assets/Img/Gradient_vanishi.png"></p><p>当激活函数<span class="math inline">\(\sigma()\)</span>为sigmoid函数，且weight初始化在（-1,1）间,sigmoid函数导数的值域如下：</p><p><img src="/Assets/Img/derivative_of_sigmoid.png" style="zoom:50%;"></p><p>因此<span class="math inline">\(|w_i \sigma{a_i}&#39;|&lt;0.25\)</span>当网络很深时累乘会导致<span class="math inline">\(\frac{dL}{dw_1}\)</span>非常小，前层网络的参数在训练中变化很小或没有变化，这就是梯度消失。</p><h1 id="梯度爆炸">梯度爆炸</h1><p><img src="/Assets/Img/gradient_explosion.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;梯度消失&quot;&gt;梯度消失&lt;/h1&gt;
&lt;p&gt;以全连接神经网络为例子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/Assets/Img/Gradient_vanishi.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;当激活函数&lt;span class=&quot;math inline&quot;&gt;\(\sigm
      
    
    </summary>
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>粒子群算法</title>
    <link href="//Rocky1ee.github.io/2019/09/01/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95PSO/"/>
    <id>//Rocky1ee.github.io/2019/09/01/粒子群算法PSO/</id>
    <published>2019-09-01T08:56:36.000Z</published>
    <updated>2019-12-10T07:45:55.605Z</updated>
    
    <content type="html"><![CDATA[<h1 id="算法思路">算法思路</h1><h2 id="基本思想">基本思想</h2><p>粒子群算法-particle swarm optimization。粒子群优化算法的基本思想是：通过群体中个体之间的协作和信息共享来寻找最优解。</p><p>我们可以设想这样的一个场景，一群鸟在随机搜寻食物。这个区域里只有一块食物。所有的鸟都不知道食物再哪里，但他们知道目前距离食物还有多远，那么找到食物的最佳策略是什么？</p><p><strong>1.找寻距离食物最近的鸟之周围区域</strong></p><p><strong>2.根据自己本身飞行的经验判断食物的所在</strong></p><p>在PSO中，每个优化问题的潜在解都可以想象成<span class="math inline">\(N\)</span>维搜索空间上的一个点，我们称之为“粒子”（Particle），由目标函数决定每个粒子的适应值，每个粒子还有一个速度决定他们运动的方向和距离，然后粒子们就追随当前的最优粒子在解空间中搜索。</p><h2 id="数学定义">数学定义</h2><p>粒子<span class="math inline">\(i\)</span>在<span class="math inline">\(N\)</span>维空间的位置为： <span class="math display">\[X_i = (x_1,x_2,...,x_N)\]</span> 运动速度表示为： <span class="math display">\[V_i=(v_1,v_2,...,v_N)\]</span> 每个粒子都有一个由目标函数决定的<strong>适应值</strong>，并且知道自己到目前为止的最好位置<span class="math inline">\((pbest_i)\)</span>和现在所在的位置<span class="math inline">\(X_i\)</span>．这个可以看作是粒子自己的运动经验。同时，每个粒子还知道到目前整个群体中位置最好的粒子<span class="math inline">\((gbest) (gbest是pbest_i中的最好值)\)</span>．这个可以看作是粒子同伴的经验。粒子就是通过自己的经验和同伴中最好的经验来决定下一步的运动。</p><a id="more"></a><h1 id="算法基本内容">算法基本内容</h1><h2 id="初始化">初始化</h2><p>对于每个粒子的位置<span class="math inline">\(X_i\)</span>和运动速度<span class="math inline">\(V_i\)</span>进行随机初始化</p><h2 id="更新">更新</h2><p>粒子速度的更新： <span class="math display">\[V_i = V_i+c_1\times rand()\times(pbest_i-X_i) + c_2\times rand()\times(gbest-X_i)\qquad公式(1)\]</span> 位置的更新： <span class="math display">\[X_i = X_i+V_i\qquad 公式(2)\]</span> 假设种群中粒子的总数为<span class="math inline">\(M\)</span>。<span class="math inline">\(i \in [1,M]\)</span>。</p><p>随机数<span class="math inline">\(rand() \in (0,1)\)</span>。</p><p>学习因子：<span class="math inline">\(c_1和c_2\)</span></p><p>假设空间的维度为<span class="math inline">\(N\)</span>,在每一维度上，<span class="math inline">\(v_d的上界为v_{dmax},d \in [1,N]\)</span></p><p>公式（1）的第一部分称为<strong>记忆项</strong>，表示上次速度大小和方向的影响；</p><p>第二部分称为<strong>自身认知项</strong>，是从当前点指向粒子自身最好点的一个矢量，粒子自身经验所决定的运动；</p><p>第三部分称为<strong>群体认知项</strong>，是从当前点指向种群最好点的矢量，反映了粒子间的协同合作。</p><p>粒子通过自身的经验和同伴中最好的经验来决定下一步的运动。</p><figure><img src="/Assets/BlogImg/粒子群算法1.png" alt><figcaption>粒子群算法1</figcaption></figure><h2 id="引入惯性权重因子">引入惯性权重因子</h2><p>为了完善算法引入惯性权重因子:<span class="math inline">\(\omega \geq 0\)</span>,公式(2)变为： <span class="math display">\[V_i = \omega \times V_i+c_1\times rand()\times(pbest_i-X_i) + c_2\times rand()\times(gbest-X_i)\qquad公式(3)\]</span> 当<span class="math inline">\(\omega\)</span>越大，越倾向于寻找全局最优。当<span class="math inline">\(\omega\)</span>越小越倾向于寻找局部最优。</p><h2 id="动态权重因子">动态权重因子</h2><p>试验发现动态权重因子比固定权重因子更有效。目前经常采用线性递减权值策略（linearly decreasing weight，LDW） <span class="math display">\[\omega=(\omega_{ini}-\omega_{end})(G_k-g)/G_k+\omega_{end}\]</span> <span class="math display">\[\omega_{ini}:初始惯性权值， \omega_{end}:迭代至最大数时惯性权值，G_k:最大迭代数， g:当前迭代数，\]</span></p><p>公式(3)被称为标准PSO。</p><h2 id="全局最优法">全局最优法</h2><p>当<span class="math inline">\(c_1=0\)</span>时，粒子没有了认知能力，变为<strong>只有社会的模型(social-only)</strong>： <span class="math display">\[V_i = \omega \times V_i + c_2\times rand()\times(gbest-X_i)\qquad公式(4)\]</span> 被称为<strong>全局PSO算法</strong>。粒子有扩展搜索空间能力，具有较快收敛速度，由于缺少局部搜索，对于复杂问题比标准PSO 更易陷入局部最优。</p><h2 id="局部最优法">局部最优法</h2><p>当<span class="math inline">\(c_2=0\)</span>时，则粒子之间没有社会信息，模型变为<strong>只有自我(cognition-only)模型</strong>： <span class="math display">\[V_i = \omega \times V_i+c_1\times rand()\times(pbest_i-X_i) \qquad公式(5)\]</span> 被称为<strong>局部PSO算法</strong>。由于个体之间没有信息的交流，整个群体相当于多个粒子进行盲目的随机搜索，收敛速度慢，因而得到最优解的可能性小。</p><h2 id="算法流程">算法流程</h2><figure><img src="/Assets/BlogImg/粒子群算法2.png" alt><figcaption>粒子群算法2</figcaption></figure><h1 id="实例应用">实例应用</h1><p>用粒子群算法求解<span class="math inline">\(y=f(x_1,x_2)=x_1^2+x_2^2,-10&lt;x_1,x_2&lt;10\)</span>的最小值。已知当<span class="math inline">\(x_1=x_2=0\)</span>时<span class="math inline">\(y\)</span>取最小值。现在用粒子群算法求解。</p><h2 id="初始化-1">初始化</h2><p>设种群大小<span class="math inline">\(M=3\)</span>;惯性权重<span class="math inline">\(\omega=0.5\)</span>;<span class="math inline">\(c_1=c_2=2\)</span>;<span class="math inline">\(r_1,r_2是[0,1]\)</span>内随机数。</p><p>假设个体的初始位置和速度分别为： <span class="math display">\[p_1=\begin{cases}v_1=(3,2)&amp;\\x_1=(8,-5)\end{cases}p_2=\begin{cases}v_1=(-3,-2)&amp;\\x_1=(-5,9)\end{cases}p_3=\begin{cases}v_1=(5,2)&amp;\\x_1=(-7,-8)\end{cases}\]</span> 计算适应函数值，并且得到粒子的历史最优位置和群体的全局最优位置 <span class="math display">\[\begin{cases}f_1=8^2+(-5)^2=89&amp;\\pbest=x_1=(8,-5)\end{cases}\begin{cases}f_2=(-5)^2+9^2=106&amp;\\pbest=x_2=(-5,9)\end{cases}\begin{cases}f_3=(-7)^2+(-8)^2=113&amp;\\pbest=x_3=(-7,-8)\end{cases}\]</span></p><p><span class="math display">\[gbest=pbest=(8,-5)\]</span></p><h2 id="粒子速度和位置更新">粒子速度和位置更新</h2><p><strong>根据自身的历史最优位置和全局最优位置，更新每个粒子的速度和位置：</strong> <span class="math display">\[p1=\begin{cases}V_1 = \omega \times V_1+c_1\times r_1\times(pbest_1-X_1) + c_2\times r_2\times(gbest-X_1)&amp;\\\Rightarrow v_1=\begin{cases}0.5\times3+0+0=1.5 &amp;\\0.5\times2+0+0=1\end{cases}&amp;\\x_1=x_1+v_1=(8,-5)+(1.5,1)=(9.5,-4)\end{cases}\]</span></p><p><span class="math display">\[p2=\begin{cases}V_2 = \omega \times V_2+c_1\times r_1\times(pbest_2-X_2) + c_2\times r_2\times(gbest-X_2)&amp;\\\Rightarrow v_2=\begin{cases}0.5\times-3+0+2\times0.3\times(8-(-5))=6.1 &amp;\\0.5\times2+0+2\times0.3\times(-5-9)=1.8\end{cases}&amp;\\x_2=x_2+v_2=(-5,9)+(6.1,1.8)=(1.1,10)\end{cases}\]</span></p><p><span class="math display">\[p3=\begin{cases}V_3 = \omega \times V_3+c_1\times r_1\times(pbest_3-X_3) + c_2\times r_2\times(gbest-X_3)&amp;\\\Rightarrow v_2=\begin{cases}0.5\times5+0+2\times0.3\times(8-(-7))=11.5 &amp;\\0.5\times3+0+2\times0.3\times(-5-(-8))=3.3\end{cases}&amp;\\x_3=x_3+v_3=(-7,-8)+(11.5,3.3)=(4.5,-4.7)\end{cases}\]</span></p><h2 id="更新粒子的历史最优位置和全局最优位置">更新粒子的历史最优位置和全局最优位置：</h2><p><span class="math display">\[f_1^*=9.5^2+(-4)^2=106.25&gt;f_1=89\\\begin{cases}f_1=89&amp;\\pbest_1=(8,-5)\end{cases}\]</span></p><p><span class="math display">\[f_2^*=1.1^2+10^2=101.21&lt;f_2=106\\\begin{cases}f_2=f_2^*=101.32&amp;\\pbest_2=(1.1,10)\end{cases}\]</span></p><p><span class="math display">\[f_3^*=4.5^2+(-4.7)^2=42.34&lt;f_3=113\\\begin{cases}f_3=f_3^*=42.34&amp;\\pbest_3=(4.5,-4.7)\end{cases}\]</span></p><p><span class="math display">\[gbest=pbest_3=(4.5,-4.7)\]</span></p><h2 id="迭代">迭代</h2><p>如果满足结束条件，则输出全局最优结果并结束程序，否则，转向第二步继续执行。</p><!-- more -->]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;算法思路&quot;&gt;算法思路&lt;/h1&gt;
&lt;h2 id=&quot;基本思想&quot;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;粒子群算法-particle swarm optimization。粒子群优化算法的基本思想是：通过群体中个体之间的协作和信息共享来寻找最优解。&lt;/p&gt;
&lt;p&gt;我们可以设想这样的一个场景，一群鸟在随机搜寻食物。这个区域里只有一块食物。所有的鸟都不知道食物再哪里，但他们知道目前距离食物还有多远，那么找到食物的最佳策略是什么？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.找寻距离食物最近的鸟之周围区域&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.根据自己本身飞行的经验判断食物的所在&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在PSO中，每个优化问题的潜在解都可以想象成&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;维搜索空间上的一个点，我们称之为“粒子”（Particle），由目标函数决定每个粒子的适应值，每个粒子还有一个速度决定他们运动的方向和距离，然后粒子们就追随当前的最优粒子在解空间中搜索。&lt;/p&gt;
&lt;h2 id=&quot;数学定义&quot;&gt;数学定义&lt;/h2&gt;
&lt;p&gt;粒子&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;在&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;维空间的位置为： &lt;span class=&quot;math display&quot;&gt;\[
X_i = (x_1,x_2,...,x_N)
\]&lt;/span&gt; 运动速度表示为： &lt;span class=&quot;math display&quot;&gt;\[
V_i=(v_1,v_2,...,v_N)
\]&lt;/span&gt; 每个粒子都有一个由目标函数决定的&lt;strong&gt;适应值&lt;/strong&gt;，并且知道自己到目前为止的最好位置&lt;span class=&quot;math inline&quot;&gt;\((pbest_i)\)&lt;/span&gt;和现在所在的位置&lt;span class=&quot;math inline&quot;&gt;\(X_i\)&lt;/span&gt;．这个可以看作是粒子自己的运动经验。同时，每个粒子还知道到目前整个群体中位置最好的粒子&lt;span class=&quot;math inline&quot;&gt;\((gbest) (gbest是pbest_i中的最好值)\)&lt;/span&gt;．这个可以看作是粒子同伴的经验。粒子就是通过自己的经验和同伴中最好的经验来决定下一步的运动。&lt;/p&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="粒子群算法" scheme="//Rocky1ee.github.io/tags/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>蚁群算法</title>
    <link href="//Rocky1ee.github.io/2019/08/31/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95(ACO)/"/>
    <id>//Rocky1ee.github.io/2019/08/31/蚁群算法(ACO)/</id>
    <published>2019-08-31T08:56:36.000Z</published>
    <updated>2019-12-10T07:45:43.146Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本原理">基本原理</h1><figure><img src="/Assets/BlogImg/蚁群算法.png" alt><figcaption>蚁群算法1</figcaption></figure><figure><img src="/Assets/BlogImg/蚁群算法1.png" alt><figcaption>蚁群算法2</figcaption></figure><figure><img src="/Assets/BlogImg/蚁群算法2.png" alt><figcaption>蚁群算法3</figcaption></figure><a id="more"></a><h1 id="基本流程">基本流程</h1><h2 id="路径构建">路径构建</h2><p><span class="math display">\[\begin{equation}    P_{ij}(t) =   \begin{cases}   \frac{[\tau_{ij}(t)]^\alpha \times [\eta_{ij}(t)]^\beta}{\sum\limits_{k\in allowed_k }[\tau_{ik}(t)]^\alpha \times [\eta_{ik}(t)]^\beta}&amp;   {if \quad j \in allowed_k}\\   \qquad 0&amp;   others   \end{cases}  \end{equation}\]</span></p><hr><ul><li><span class="math inline">\(i,j\)</span>分别为起点和终点</li><li><span class="math inline">\(\eta_{ij} = \frac{1}{d_{ij}}\)</span>为能见度，是两点<span class="math inline">\(i、j\)</span>间的导数</li><li><span class="math inline">\(\tau_{ij}(t)\)</span>为时间<span class="math inline">\(t\)</span>时由<span class="math inline">\(i\)</span>到<span class="math inline">\(j\)</span>的信息素强度</li><li><span class="math inline">\(allowed_k\)</span>为从<span class="math inline">\(i\)</span>出发可访问到节点的集合</li><li><span class="math inline">\(\alpha,\beta\)</span>为两常数，分别是信息素和能见度的加权值</li></ul><p>结果表示<strong>当前点到每个可能的下个节点的概率</strong></p><h2 id="信息素更新">信息素更新</h2><p>蚂蚁的信息素释放量<span class="math inline">\(C(0)\)</span>,如果太小则容易导致局部最优。为什么？</p><p>如果太大，则对搜索方向的导向作用降低。一般用贪婪算法获取一个路径值<span class="math inline">\(Cnn\)</span>，然后根据蚂蚁个数来计算<span class="math inline">\(C(0) = m/Cnn\)</span>,<span class="math inline">\(m\)</span>为蚂蚁个数。</p><p>信息素更新如下：</p><p><span class="math display">\[\tau(t)=(1-p)\tau_{ij}+\sum_{k=1}^m\Delta\tau_{ij}^k\]</span> <span class="math inline">\(m\)</span>:蚂蚁个数，<span class="math inline">\(0&lt;\rho&lt;=1\)</span>:信息素蒸发率,<span class="math inline">\(\Delta\tau_{ij}^k\)</span>:第<span class="math inline">\(k\)</span>只蚂蚁在路径<span class="math inline">\(i\)</span>到 <span class="math inline">\(j\)</span> 所留下的信息素 <span class="math display">\[\begin{equation}    \Delta \tau_{ij}^k =   \begin{cases}   (c_k)^{-1}&amp;\quad 第k^{th}只蚂蚁经过路径(i，j)\\\\   0&amp;\quad others   \end{cases}  \end{equation}\]</span> 信息素挥发(evaporation):避免算法过快地向局部最优区域集中，有助于搜索区域的扩展。</p><p>信息素增强(reinforcement):指引最优路径的指南。</p><h2 id="迭代与停止">迭代与停止</h2><p>算法每次迭代：<strong>每次迭代的m只蚂蚁都完成了自己的路径过程，回到原点后的整个过程。</strong></p><p>停止：指定迭代次数或达成指定的最优解条件</p><h1 id="蚁群算法的实例">蚁群算法的实例</h1><figure><img src="/Assets/BlogImg/蚁群算法4.png" alt><figcaption>蚁群算法4</figcaption></figure><p>假设共<span class="math inline">\(m=3\)</span>只蚂蚁，参数<span class="math inline">\(\alpha=1,\beta=2,\rho=0.5\)</span></p><h2 id="初始化">初始化</h2><p>首先使用贪婪算法得到路径的<strong>(ACDBA)</strong>, 则<span class="math inline">\(C_{nn}=1+2+4+3=10\)</span>,求得<span class="math inline">\(\tau_0=m\div C_{nn}=0.3\)</span> <span class="math display">\[\tau(0)=\begin{bmatrix}0&amp;0.3&amp;0.3&amp;0.3 \\0.3&amp;0&amp;0.3&amp;0.3 \\0.3&amp;0.3&amp;0&amp;0.3 \\0.3&amp;0.3&amp;0.3&amp;0\end{bmatrix}\]</span></p><h2 id="出发地">出发地</h2><p>为每个蚂蚁随机选择出发城市，假设蚂蚁1选择城市A，蚂蚁2选择城市B，蚂蚁3选择城市D。</p><h2 id="访问地">访问地</h2><p>以蚂蚁1为例，当前城市<span class="math inline">\(i=A\)</span>,可访问城市集合<span class="math inline">\(J_1 (i)={B,C,D}\)</span></p><p>计算蚂蚁1访问各个城市的概率 <span class="math display">\[A \Rightarrow \begin{cases}B:\tau_{AB}^a\times\eta_{AB}^\beta=0.3^1\times\frac{1}{3}^2=0.033\\\\C:\tau_{AC}^a\times\eta_{AC}^\beta=0.3^1\times\frac{1}{1}^2=0.300\\\\D:\tau_{AD}^a\times\eta_{AD}^\beta=0.3^1\times\frac{1}{2}^2=0.075\\\\\end{cases}\]</span></p><p><span class="math display">\[P(B)=\frac{0.033}{0.033+0.3+0.075}=0.08 \\\\P(C)=\frac{0.03}{0.033+0.3+0.075}=0.74 \\\\P(D)=\frac{0.075}{0.033+0.3+0.075}=0.1\]</span></p><figure><img src="/Assets/BlogImg/轮盘赌选择法.jpeg" alt><figcaption>轮盘赌选择法</figcaption></figure><p>轮盘赌选择法：选择累积概率超过随机值的第一个个体。</p><p>用轮盘赌法选择下一个访问城市。假设产生的随机值<span class="math inline">\(r=0.05\)</span>，则蚂蚁<strong>1</strong>会选择城市<strong>B</strong> 同样，假设蚂蚁<strong>2</strong>选择城市<strong>D</strong>，蚂蚁<strong>3</strong>选择城市<strong>A</strong>。</p><p>现在蚂蚁<strong>1</strong>已经来到<strong>B</strong>了，路径记忆向量<span class="math inline">\(R^l=(AB)\)</span>,可访问城市集合<span class="math inline">\(J_i(i)={C,D}\)</span></p><p>蚂蚁<strong>1</strong>访问<strong>C，D</strong>城市的概率： <span class="math display">\[B\Rightarrow\begin{cases}C:\tau_{BC}^\alpha\times\eta_{BC}^\beta=0.3^1\times\frac{1}{5}^2=0.012\\\\D:\tau_{BD}^\alpha\times\eta_{BD}^\beta=0.3^1\times\frac{1}{4}^2=0.019\end{cases}\\\\P(C)=\frac{0.012}{0.012+0.019}=0.39\\\\P(D)=\frac{0.019}{0.012+0.019}=0.61\]</span> 用轮盘赌法选择下一个访问城市。假设产生的随机数<span class="math inline">\(r=0.67\)</span>，则蚂蚁<strong>1</strong>会选择城市<strong>D</strong> 同样，假设蚂蚁<strong>2</strong>选择城市<strong>C</strong>，蚂蚁<strong>3</strong>选择城市<strong>D</strong>。</p><p>最终，所有蚂蚁选择的路径为：</p><p>蚂蚁1：ABDCA</p><p>蚂蚁2：BDCAB</p><p>蚂蚁3：DACBD</p><h2 id="信息素更新-1">信息素更新</h2><p>每只蚂蚁走过的路径长度：<span class="math inline">\(C_1=3+4+2+1=10;C_2=4+2+1+3=10;C_3=2+1+5+4=12。\)</span></p><p>每条边信息素的更新如下: <span class="math display">\[\tau_{AB}=（1-\rho)\times\tau_{AB} + \sum_{k=1}^3\Delta\tau_{AB}^k=0.5\times0.3+(\frac{1}{10}+\frac{1}{10})=0.35\\\\\tau_{AC}=（1-\rho)\times\tau_{AC} + \sum_{k=1}^3\Delta\tau_{AC}^k=0.5\times0.3+(\frac{1}{12})=0.16\\\\\tau_{AD},\tau_{BC}...\]</span> 在<span class="math inline">\(\tau_{AB}\)</span>的计算中蚂蚁3并未走过路径<strong>AB</strong>，所以<span class="math inline">\(\tau_{AB}^3=0\)</span>。</p><h2 id="结束">结束</h2><p>如果满足结束条件，则输出全局最优结果并结束程序，否则，则转向步骤2继续执行。</p><p>references</p><p>https://www.cnblogs.com/asxinyu/p/Path_Optimization_Tsp_Problem_Ant_System_CSharp.html#opennewwindow</p><!-- more -->]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;基本原理&quot;&gt;基本原理&lt;/h1&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/蚁群算法.png&quot; alt&gt;&lt;figcaption&gt;蚁群算法1&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/蚁群算法1.png&quot; alt&gt;&lt;figcaption&gt;蚁群算法2&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;/Assets/BlogImg/蚁群算法2.png&quot; alt&gt;&lt;figcaption&gt;蚁群算法3&lt;/figcaption&gt;
&lt;/figure&gt;
    
    </summary>
    
      <category term="算法学习" scheme="//Rocky1ee.github.io/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能算法" scheme="//Rocky1ee.github.io/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"/>
    
      <category term="蚁群算法" scheme="//Rocky1ee.github.io/tags/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset</title>
    <link href="//Rocky1ee.github.io/2019/01/30/Spatial%20Attentive%20Single-Image%20Deraining%20with%20a%20High%20Quality%20Real%20Rain%20Dataset/"/>
    <id>//Rocky1ee.github.io/2019/01/30/Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset/</id>
    <published>2019-01-30T08:56:36.000Z</published>
    <updated>2020-04-18T07:34:37.534Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>合成雨图模式单一，single image de-rainy方向没有benchmark。为解决这些问题制作高质量的real world rain/clean image pairs数据集，提出SPatial Attentive Network (SPANet)以从局部到全局的方式去除rain streak。</p><h1 id="introduction">Introduction</h1><p>基于两个观察：1.雨滴的下落速度很快，同一pixel不可能总是被雨滴覆盖。2.被雨覆盖pixel的intensity在true background radiance 之上波动。</p><a id="more"></a><h1 id="real-rain-image-dataset">Real Rain Image Dataset</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\SPANet_database_generator2.png"></p><p>某个pixel未被雨覆盖时intensity的波动会比较小，被雨覆盖时intensity的波动会比较大。如果假设周围光强不变，可用小波动分布中的众数作为ground truth。</p><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\SPANet_database_generator.png"></p><p>这个<span class="math inline">\(\hat{P}\)</span>是如何使用的？</p><h1 id="proposed-model">Proposed Model</h1><p><img src="E:\Rocky1ee.github.io\source\Assets\Img\SPANet_structure.png"></p><h2 id="spatial-attentive-block">Spatial Attentive Block</h2><p><strong>IRNN</strong>：使用ReLU和identity matrix初始的RNN，以获取long-range varying contextual 信息。采用两个IRNN，四个方向操作组合的结构，第一个用于获取每个位置周围的contexts（local），第二个进一步获取non-local contextual 信息（global）。每个位置的feature与其4个方向的关系如下： <span class="math display">\[h_{i,j} ← max (α_{dir} h_{i,j−1} + h_{i,j}, 0)\]</span> <img src="/Assets/Img/SAM_two_stage.png"></p><p><strong>Spatial attentive module (SAM)</strong>:分为2个分支，分支IRNN 用于获取每个位置周围的信息，分支conv+ReLU用于获取spatial contextual information。SAM模块主要是用于生成attention map。每个SAB中SAM模块的参数是共用的，以此逐步产生更好的attention map。</p><p><strong>spatial attentive residual block (SARB)</strong>：通过学习到的negative residual 去除rain streaks。</p><p><strong>spatial attentive block (SAB)</strong>：由SAM和SARB组成，SAM生成的attention map指导SAB 去除rain streaks。</p><p><strong>Spatial Attentive Network (SPANet)</strong>的结构如下：</p><p><img src="/Assets/Img/SPANet_all_structure.png"></p><h2 id="loss-function">Loss Function</h2><p>SPANet的loss function如下： <span class="math display">\[\mathcal{L}_{total} = \mathcal{L}_1 + \mathcal{L}_{SSIM} + \mathcal{L}_{Att}.\]</span> 其中<span class="math inline">\(\mathcal{L}_1\)</span>为per-pixel reconstruction 准确度。</p><p><span class="math inline">\(\mathcal{L}_{SSIM}\)</span>为结构相似度限制，公式为<span class="math inline">\(1 − SSIM(P, C)\)</span>,P:de-rained image,C: clean image。</p><p>atteion loss为： <span class="math display">\[\mathcal{L}_{Att} = ||\mathcal{A−M}||^2_2\]</span> 其中<span class="math inline">\(\mathcal_A\)</span>指attention map，<span class="math inline">\(\mathcal_M\)</span>指rain streaks的二值图，1表示对应pixel位置被雨覆盖，0则表示未覆盖。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;合成雨图模式单一，single image de-rainy方向没有benchmark。为解决这些问题制作高质量的real world rain/clean image pairs数据集，提出SPatial Attentive Network (SPANet)以从局部到全局的方式去除rain streak。&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;基于两个观察：1.雨滴的下落速度很快，同一pixel不可能总是被雨滴覆盖。2.被雨覆盖pixel的intensity在true background radiance 之上波动。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习去雨方法" scheme="//Rocky1ee.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%BB%E9%9B%A8%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="深度学习" scheme="//Rocky1ee.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="De-rain" scheme="//Rocky1ee.github.io/tags/De-rain/"/>
    
  </entry>
  
</feed>
